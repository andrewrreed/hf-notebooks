{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0147d05e-a409-4f1c-a69e-34360d634d28",
   "metadata": {},
   "source": [
    "# Distributed Data Parallel (DDP) for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ede559-414d-46af-ba6f-36cc6c9b61aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers accelerate optimum einops tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32dbe936-4455-455f-b704-d9dfbf88793a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "from accelerate.utils import release_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098634e9-a010-46f4-bdcb-cd067d63d5a3",
   "metadata": {},
   "source": [
    "### Inference with Pipelines\n",
    "\n",
    "First let's load and run inference with a `pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9f42cc9-e2ec-4683-9ff7-e8d5545cc38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The Falcon model was initialized without `trust_remote_code=True`, and will therefore leverage the transformers library implementation. tiiuae/falcon-7b-instruct's revision is set to a version that doesn't leverage remote code (f8dac3fff96d5debd43edf56fb4e1abcfffbef28).\n",
      "\n",
      "In order to override this, please set a revision manually or set `trust_remote_code=True`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed7d7aba5117411aa8ca7e072c6ca3ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"tiiuae/falcon-7b-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                             torch_dtype=torch.bfloat16, \n",
    "                                             device_map=\"sequential\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69c5d1e-eaf6-40fd-96b0-da0244838a3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name}, dtype: {param.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a9742dd-d8c8-4146-a6b0-7112c75686d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.\n"
     ]
    }
   ],
   "source": [
    "# convert model to bettertransformer to take advantage of Flash Attention\n",
    "model.to_bettertransformer()\n",
    "\n",
    "pipe = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e70909c4-a5e1-4bce-b172-3d1bbfb7fde6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.92 s ± 238 ms per loop (mean ± std. dev. of 5 runs, 2 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 2 -r 5\n",
    "text = \"Hello \" * 1800\n",
    "pipe(text, max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e97ffc7-0205-42fa-8445-a37d78aa2710",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = release_memory(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf381143-b697-4f09-992e-ee9ff34ca600",
   "metadata": {},
   "source": [
    "### Distributed inference with PartialState\n",
    "\n",
    "If our model fits on one GPU, we can take advantage of multiple GPU's by keeping a replica on each and distributing our data across them. See [Distributed Inference with Accelerate](https://huggingface.co/docs/accelerate/main/en/usage_guides/distributed_inference) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0102122-a7b7-4b39-8a7e-298eafb599b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ddp_inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ddp_inference.py\n",
    "\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "from accelerate import PartialState\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "\n",
    "\n",
    "def save_dicts_to_jsonl(filename, data_list):\n",
    "    \"\"\"\n",
    "    Saves a list of dictionaries to a JSONL file, where each dictionary is saved as a separate line.\n",
    "\n",
    "    Args:\n",
    "    - filename (str): Path to the JSONL file.\n",
    "    - data_list (list): List of dictionaries to save.\n",
    "    \"\"\"\n",
    "    with open(filename, 'a') as file:\n",
    "        for data_dict in data_list:\n",
    "            file.write(json.dumps(data_dict) + '\\n')\n",
    "\n",
    "        \n",
    "model_id = \"tiiuae/falcon-7b-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                             torch_dtype=torch.bfloat16)\n",
    "\n",
    "# convert model to bettertransformer to take advantage of Flash Attention\n",
    "model.to_bettertransformer()\n",
    "\n",
    "distributed_state = PartialState()\n",
    "model.to(distributed_state.device)\n",
    "\n",
    "gen_config = GenerationConfig.from_pretrained(model_id, \n",
    "                                              max_new_tokens=50, \n",
    "                                              do_sample=True, \n",
    "                                              pad_token_id=tokenizer.eos_token_id,\n",
    "                                              eos_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "prompts = [\"Why is distributed inference faster?\"]*1000\n",
    "\n",
    "# Set this value to maximize GPU RAM utilization\n",
    "# If OOM reduce value, ideally should be multiple of # GPU's\n",
    "# Note: this isn't batched inference, just batch size to be distributed across GPUs\n",
    "batch_size = 80\n",
    "\n",
    "# Calculate the number of batches (taking the ceiling)\n",
    "n_batches = -(-len(prompts) // batch_size)\n",
    "\n",
    "for i in tqdm(range(0, len(prompts), batch_size), total=n_batches):\n",
    "    with distributed_state.split_between_processes(prompts[i:i + batch_size]) as prompt:\n",
    "\n",
    "        inputs = tokenizer(prompt,\n",
    "                       return_tensors=\"pt\",\n",
    "                       return_token_type_ids=False).to(\"cuda\")\n",
    "\n",
    "        output_ids = model.generate(**inputs, generation_config=gen_config)\n",
    "        result = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "\n",
    "        collect = []\n",
    "        for text in result:\n",
    "            collect.append({\"text\": text, \"device\":distributed_state.process_index})\n",
    "\n",
    "        save_dicts_to_jsonl(\"test_generate.jsonl\", collect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380eb725-49fb-471d-8a44-2028885b1040",
   "metadata": {},
   "source": [
    "#### Run with accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17aece75-f609-489b-bd76-5501dd21bdcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.commands.launch:The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_processes` was set to a value of `4`\n",
      "\t\tMore than one GPU was found, enabling multi-GPU training.\n",
      "\t\tIf this was unintended please pass in `--num_processes=1`.\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "The Falcon model was initialized without `trust_remote_code=True`, and will therefore leverage the transformers library implementation. tiiuae/falcon-7b-instruct's revision is set to a version that doesn't leverage remote code (f8dac3fff96d5debd43edf56fb4e1abcfffbef28).\n",
      "\n",
      "In order to override this, please set a revision manually or set `trust_remote_code=True`.\n",
      "The Falcon model was initialized without `trust_remote_code=True`, and will therefore leverage the transformers library implementation. tiiuae/falcon-7b-instruct's revision is set to a version that doesn't leverage remote code (f8dac3fff96d5debd43edf56fb4e1abcfffbef28).\n",
      "\n",
      "In order to override this, please set a revision manually or set `trust_remote_code=True`.\n",
      "The Falcon model was initialized without `trust_remote_code=True`, and will therefore leverage the transformers library implementation. tiiuae/falcon-7b-instruct's revision is set to a version that doesn't leverage remote code (f8dac3fff96d5debd43edf56fb4e1abcfffbef28).\n",
      "\n",
      "In order to override this, please set a revision manually or set `trust_remote_code=True`.\n",
      "The Falcon model was initialized without `trust_remote_code=True`, and will therefore leverage the transformers library implementation. tiiuae/falcon-7b-instruct's revision is set to a version that doesn't leverage remote code (f8dac3fff96d5debd43edf56fb4e1abcfffbef28).\n",
      "\n",
      "In order to override this, please set a revision manually or set `trust_remote_code=True`.\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:08<00:00,  4.30s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:08<00:00,  4.31s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:08<00:00,  4.33s/it]\n",
      "The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.\n",
      "The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.\n",
      "The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:08<00:00,  4.30s/it]\n",
      "The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.\n",
      "100%|███████████████████████████████████████████| 13/13 [02:08<00:00,  9.90s/it]\n",
      "100%|███████████████████████████████████████████| 13/13 [02:08<00:00,  9.90s/it]\n",
      "100%|███████████████████████████████████████████| 13/13 [02:08<00:00,  9.91s/it]\n",
      "100%|███████████████████████████████████████████| 13/13 [02:09<00:00,  9.93s/it]\n"
     ]
    }
   ],
   "source": [
    "!accelerate launch ddp_inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf8cf91-cbb5-4145-903e-1897221c851d",
   "metadata": {},
   "source": [
    "### Review results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "086d3247-a8fd-43f0-8f0e-3601c24fee94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"test_generate.jsonl\") as file:\n",
    "    data = [json.loads(line) for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65bcbb0f-f8b8-4b16-b33e-939f3e7a921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"test_generate.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c846711a-a57f-4d54-83b9-93f169fd1a4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='device'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.groupby(\"device\").size().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d0a241-73c8-4fe9-a0cd-e96b6b6fdb08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.2-0.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.2-0:m111"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
