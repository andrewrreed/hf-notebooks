{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train SpanMarker to Improve Single Entity NER Model\n",
    "\n",
    "Goals:\n",
    "1. Build custom dataset specifically for \"Person\" entity\n",
    "2. Train SpanMarker NER model (maybe hyperparameter optimization)\n",
    "3. Evaluate performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install span_marker[wandb] names-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Explore and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'fine_ner_tags'],\n",
       "        num_rows: 131767\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'fine_ner_tags'],\n",
       "        num_rows: 18824\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'fine_ner_tags'],\n",
       "        num_rows: 37648\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_id = \"DFKI-SLT/few-nerd\"\n",
    "dataset = load_dataset(dataset_id, \"supervised\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '0', 'tokens': ['Paul', 'International', 'airport', '.'], 'ner_tags': [0, 0, 0, 0], 'fine_ner_tags': [0, 0, 0, 0]}\n",
      "\n",
      "{'id': '1', 'tokens': ['It', 'starred', 'Hicks', \"'s\", 'wife', ',', 'Ellaline', 'Terriss', 'and', 'Edmund', 'Payne', '.'], 'ner_tags': [0, 0, 7, 0, 0, 0, 7, 7, 0, 7, 7, 0], 'fine_ner_tags': [0, 0, 51, 0, 0, 0, 50, 50, 0, 50, 50, 0]}\n",
      "\n",
      "{'id': '2', 'tokens': ['``', 'Time', '``', 'magazine', 'said', 'the', 'film', 'was', '``', 'a', 'multimillion', 'dollar', 'improvisation', 'that', 'does', 'everything', 'but', 'what', 'the', 'title', 'promises', \"''\", 'and', 'suggested', 'that', '``', 'writer', 'George', 'Axelrod', '(', '``', 'The', 'Seven', 'Year', 'Itch', '``', ')', 'and', 'director', 'Richard', 'Quine', 'should', 'have', 'taken', 'a', 'hint', 'from', 'Holden', '[', \"'s\", 'character', 'Richard', 'Benson', ']', ',', 'who', 'writes', 'his', 'movie', ',', 'takes', 'a', 'long', 'sober', 'look', 'at', 'what', 'he', 'has', 'wrought', ',', 'and', 'burns', 'it', '.', \"''\"], 'ner_tags': [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 7, 7, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 7, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'fine_ner_tags': [0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 51, 51, 0, 0, 6, 6, 6, 6, 0, 0, 0, 0, 53, 53, 0, 0, 0, 0, 0, 0, 54, 0, 0, 0, 54, 54, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sample in dataset[\"train\"].select(range(3)):\n",
    "    print(sample)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'art', 'building', 'event', 'location', 'organization', 'other', 'person', 'product']\n"
     ]
    }
   ],
   "source": [
    "# inspect labels\n",
    "labels = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset that consists of examples with `Person` entity only\n",
    "\n",
    "**Note** - Exactly half of the examples in each split contain Person entities, while other half don't have any. This is to help with diversity of training data and model generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import ClassLabel, Features, Sequence, Value\n",
    "\n",
    "\n",
    "FEATURES = Features(\n",
    "    {\n",
    "        \"tokens\": Sequence(feature=Value(dtype=\"string\")),\n",
    "        \"ner_tags\": Sequence(feature=ClassLabel(names=[\"O\", \"B-PER\", \"I-PER\"])),\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "def get_fewnerd(entity_idx):\n",
    "    \"\"\"Loads FewNERD dataset and reformats labels to only include those for the specified entity.\n",
    "\n",
    "    Args:\n",
    "        entity_idx (int): The index of the entity from FewNERD dataset to keep.\n",
    "\n",
    "    Returns:\n",
    "        dataset: The filtered dataset containing `ner_tags` for only the specified entity.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def mapper(sample):\n",
    "        sample[\"ner_tags\"] = [int(tag == entity_idx) for tag in sample[\"ner_tags\"]]\n",
    "        sample[\"ner_tags\"] = [\n",
    "            2 if tag == 1 and idx > 0 and sample[\"ner_tags\"][idx - 1] == 1 else tag\n",
    "            for idx, tag in enumerate(sample[\"ner_tags\"])\n",
    "        ]\n",
    "        return sample\n",
    "\n",
    "    dataset = load_dataset(\"DFKI-SLT/few-nerd\", \"supervised\")\n",
    "    dataset = dataset.map(mapper, remove_columns=[\"id\", \"fine_ner_tags\"])\n",
    "    dataset = dataset.cast(FEATURES)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3579355ba4ff4be8a01039c591e1f440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/131767 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ccd7949deea49d098dce87048c8e274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18824 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c8eb6b104a54bd8b718a72fedda27c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/37648 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa6785442c924064a1893e7e73b0ba8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/131767 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "839fe272815047e2948e51c7c64c2d81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/18824 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d06ef25853d4a80b487aa0c6a29204a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/37648 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 131767\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 18824\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 37648\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fewnerd_dataset = get_fewnerd(7)\n",
    "fewnerd_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from datasets import concatenate_datasets, DatasetDict, Dataset\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "def has_person(sample):\n",
    "    return bool(sum(sample[\"ner_tags\"]))\n",
    "\n",
    "\n",
    "def has_no_person(sample):\n",
    "    return not has_person(sample)\n",
    "\n",
    "\n",
    "def preprocess_raw_dataset(raw_dataset):\n",
    "    \"\"\"\n",
    "    Preprocesses the raw dataset by filtering out examples that do not contain a person entity,\n",
    "    balancing the dataset by randomly selecting and including examples without a person entity,\n",
    "    and concatenating the filtered datasets.\n",
    "\n",
    "    Args:\n",
    "        raw_dataset (Dataset): The raw dataset to be preprocessed.\n",
    "\n",
    "    Returns:\n",
    "        Dataset: The preprocessed dataset.\n",
    "    \"\"\"\n",
    "    dataset_person = raw_dataset.filter(has_person)\n",
    "    dataset_no_person = raw_dataset.filter(has_no_person)\n",
    "    dataset_no_person = dataset_no_person.select(\n",
    "        random.sample(range(len(dataset_no_person)), k=len(dataset_person))\n",
    "    )\n",
    "    dataset = concatenate_datasets([dataset_person, dataset_no_person])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23c9d62092814e9884e10836748585a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/131767 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2034a288d68746e580b6b4e1310fa4fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/131767 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce9288b0014a41879d9dc7c0cfd74c1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/37648 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f3aa4b8a68e40479417070c8998431f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/37648 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f937c797de254cbf87f2045072f10c0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/18824 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93d697119f4f42179ec50515fd25b384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/18824 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 85524\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 12546\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 24422\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare dataset\n",
    "processed_fewnerd_train = preprocess_raw_dataset(fewnerd_dataset[\"train\"])\n",
    "processed_fewnerd_test = preprocess_raw_dataset(fewnerd_dataset[\"test\"])\n",
    "processed_fewnerd_val = preprocess_raw_dataset(fewnerd_dataset[\"validation\"])\n",
    "\n",
    "processed_dataset_dict = DatasetDict(\n",
    "    {\n",
    "        \"train\": processed_fewnerd_train,\n",
    "        \"validation\": processed_fewnerd_val,\n",
    "        \"test\": processed_fewnerd_test,\n",
    "    }\n",
    ")\n",
    "processed_dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['It', 'starred', 'Hicks', \"'s\", 'wife', ',', 'Ellaline', 'Terriss', 'and', 'Edmund', 'Payne', '.'], 'ner_tags': [0, 0, 1, 0, 0, 0, 1, 2, 0, 1, 2, 0]}\n",
      "\n",
      "{'tokens': ['``', 'Time', '``', 'magazine', 'said', 'the', 'film', 'was', '``', 'a', 'multimillion', 'dollar', 'improvisation', 'that', 'does', 'everything', 'but', 'what', 'the', 'title', 'promises', \"''\", 'and', 'suggested', 'that', '``', 'writer', 'George', 'Axelrod', '(', '``', 'The', 'Seven', 'Year', 'Itch', '``', ')', 'and', 'director', 'Richard', 'Quine', 'should', 'have', 'taken', 'a', 'hint', 'from', 'Holden', '[', \"'s\", 'character', 'Richard', 'Benson', ']', ',', 'who', 'writes', 'his', 'movie', ',', 'takes', 'a', 'long', 'sober', 'look', 'at', 'what', 'he', 'has', 'wrought', ',', 'and', 'burns', 'it', '.', \"''\"], 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "\n",
      "{'tokens': ['Imelda', 'de', \"'\", 'Lambertazzi', 'is', 'a', '``', 'melodramma', 'tragico', \"''\", 'or', 'tragic', 'opera', 'in', 'two', 'acts', 'by', 'Gaetano', 'Donizett', 'i', 'from', 'a', 'libretto', 'by', 'Andrea', 'Leone', 'Tottola', ',', 'based', 'on', 'the', 'tragedy', '``', 'Imelda', \"''\", 'by', 'Gabriele', 'Sperduti', '.'], 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "\n",
      "{'tokens': ['In', '1783', 'Campbell', 'married', 'Olympia', 'Elizabeth', '(', 'died', '1794', ')', ',', 'eldest', 'daughter', 'of', 'William', 'Morshead', 'of', 'Cartuther', ',', 'Cornwall', '.'], 'ner_tags': [0, 0, 1, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0]}\n",
      "\n",
      "{'tokens': ['Neville', 'performed', 'a', 'Spinning', 'Back', 'Kick', 'on', 'Tozawa', 'to', 'retain', 'the', 'title', '.'], 'ner_tags': [1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]}\n",
      "\n",
      "{'tokens': ['From', 'her', 'locations', 'Mrs.', 'Strong', 'created', 'papers', 'for', 'the', 'Duke', 'of', 'Windsor', 'and', 'Wallis', ',', 'The', 'Duchess', 'of', 'Windsor', ',', 'Barbara', 'Hutton', ',', 'the', 'Rockefeller', ',', 'Astor', ',', 'Vanderbilt', ',', 'and', 'DuPont', 'families', ',', 'as', 'well', 'as', 'Bette', 'Davis', ',', 'Diana', 'Vreeland', ',', 'Jacqueline', 'Bouvier', 'Kennedy', ',', 'Barbara', 'Paley', ',', 'and', 'other', 'icons', 'of', 'style', '.'], 'ner_tags': [0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 2, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 2, 0, 1, 2, 0, 1, 2, 2, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0]}\n",
      "\n",
      "{'tokens': ['As', 'a', 'result', 'of', 'outdoing', 'the', 'competition', ',', 'Binion', 'received', 'death', 'threats', ',', 'although', 'eventually', 'casinos', 'raised', 'their', 'limits', 'to', 'keep', 'up', 'with', 'him', '.'], 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "\n",
      "{'tokens': ['During', 'medical', 'school', ',', 'Reitman', 'often', 'worked', 'as', 'the', 'house', 'doctor', 'at', 'sporting', 'events', 'at', 'the', 'Boston', 'Garden', '.'], 'ner_tags': [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "\n",
      "{'tokens': ['Formerly', 'known', 'as', 'the', 'Communiplex', ',', 'it', 'is', 'home', 'to', 'the', 'WHL', \"'s\", 'Prince', 'Albert', 'Raiders', 'and', 'the', 'Prince', 'Albert', 'Mintos', '.'], 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 1, 2, 0]}\n",
      "\n",
      "{'tokens': ['The', 'chapel', 'of', 'Stalmine', 'was', 'first', 'mentioned', 'about', '1200', 'and', 'a', 'cemetery', 'was', 'consecrated', 'in', '1230', '.'], 'ner_tags': [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sample in processed_dataset_dict[\"train\"].select(range(10)):\n",
    "    print(sample)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augment the dataset with additional examples\n",
    "\n",
    "Peoples names often occur as standalone strings rather than just being included in a sentence with contextual information. To improve performance on names occuring in this paradigm, let's create some specific examples to augment our dataset with.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from names_dataset import NameDataset\n",
    "\n",
    "\n",
    "def generate_full_names(n, country_codes):\n",
    "    \"\"\"Creates a list of full names from the top `n` first and last names for each country.\n",
    "\n",
    "    Randomly mixes first and last names.\n",
    "\n",
    "    Uses this library: https://github.com/philipperemy/name-dataset\n",
    "\n",
    "    Args:\n",
    "        n (int): The number of first and last names to use for each country.\n",
    "        country_codes (list): A list of country codes to use.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of full names.\n",
    "    \"\"\"\n",
    "    nd = NameDataset()\n",
    "    full_names = []\n",
    "    for code in country_codes:\n",
    "        first_names = nd.get_top_names(n=math.ceil(n / 2), country_alpha2=code)[code]\n",
    "        first_names = first_names[\"M\"] + first_names[\"F\"]\n",
    "\n",
    "        last_names = nd.get_top_names(n=n, country_alpha2=code, use_first_names=False)[\n",
    "            code\n",
    "        ]\n",
    "\n",
    "        random.seed(42)\n",
    "        random.shuffle(first_names)\n",
    "        random.shuffle(last_names)\n",
    "\n",
    "        if len(first_names) != len(last_names):\n",
    "            first_names = first_names[: len(last_names)]\n",
    "\n",
    "        full_names += [\n",
    "            f\"{first} {last}\" for first, last in zip(first_names, last_names)\n",
    "        ]\n",
    "\n",
    "    return full_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_names_dataset():\n",
    "    def tokenize_and_annotate_names(example):\n",
    "        example[\"tokens\"] = example[\"text\"].split(\" \")\n",
    "        example[\"ner_tags\"] = [\n",
    "            1 if idx == 0 else 2 for idx, _ in enumerate(example[\"tokens\"])\n",
    "        ]\n",
    "        return example\n",
    "\n",
    "    # generate list of full names\n",
    "    nd = NameDataset()\n",
    "    country_codes = [country.alpha_2 for country in nd.get_country_codes()]\n",
    "    sample_names = generate_full_names(500, country_codes=country_codes)\n",
    "    random.shuffle(sample_names)\n",
    "\n",
    "    # create HF dataset with required features\n",
    "    names_dataset = Dataset.from_dict({\"text\": sample_names})\n",
    "    names_dataset = names_dataset.map(\n",
    "        tokenize_and_annotate_names, remove_columns=[\"text\"]\n",
    "    )\n",
    "    names_dataset = names_dataset.cast(FEATURES)\n",
    "\n",
    "    return names_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86ee6a22c2e74083b26fe51546b0926b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/52472 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79f3c065e04b4812b44517c5e62fc6f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/52472 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "names_dataset = build_names_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['Ð’Ð¸ÐºÑ‚Ð¾Ñ€', 'Sochirca'], 'ner_tags': [1, 2]}\n",
      "\n",
      "{'tokens': ['Hugo', 'Inacio'], 'ner_tags': [1, 2]}\n",
      "\n",
      "{'tokens': ['Fransiska', 'Manalu'], 'ner_tags': [1, 2]}\n",
      "\n",
      "{'tokens': ['Mark', 'Danladi'], 'ner_tags': [1, 2]}\n",
      "\n",
      "{'tokens': ['Nilima', 'Bappy'], 'ner_tags': [1, 2]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sample in names_dataset.select(range(5)):\n",
    "    print(sample)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_dataset_traintest = names_dataset.train_test_split(test_size=0.3, seed=42)\n",
    "names_dataset_val = names_dataset_traintest[\"test\"].train_test_split(\n",
    "    test_size=0.5, seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_dataset_dict = DatasetDict(\n",
    "    {\n",
    "        \"train\": names_dataset_traintest[\"train\"],\n",
    "        \"validation\": names_dataset_val[\"train\"],\n",
    "        \"test\": names_dataset_val[\"test\"],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset_dict = DatasetDict()\n",
    "for key in processed_dataset_dict.keys():\n",
    "    full_dataset_dict[key] = concatenate_datasets(\n",
    "        [processed_dataset_dict[key], names_dataset_dict[key]]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 122254\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 20417\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 32293\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "842772275bf1455ca47eb3a113f8d220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bdc2e5bcd2344e9bac8f6bc1460125e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/123 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7465d72999b4e88b94ad099ca56b49f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e84b0ec4f90c4b9491cfd1ec523f42c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/21 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83eec8d044164a1eb25195e6ebfa54ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "224cc62c4bb74545a77c6f8ca7a73fb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/33 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/andrewrreed/fewnerd-person-names-augmented/commit/b55ac0adfaf5276d3dbb3071b7bd44939593f834', commit_message='Upload dataset', commit_description='', oid='b55ac0adfaf5276d3dbb3071b7bd44939593f834', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset_dict.push_to_hub(\"andrewrreed/fewnerd-person-names-augmented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Additional thoughts on data curation for future work**\n",
    "- It might be good to include some short phrases of a 1-5 tokens that are NOT Person entities. This will force the model to not assume every 2-3 word string is a name.... reducing false positives\n",
    "- With a large list of diverse names, you could also experiment with generating synthetic data with an LLM that inserts that name into a random sentence. This would improve diversity and number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train SpanMarker NER model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train_spanmarker.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_spanmarker.py\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import shutil\n",
    "from datasets import load_dataset, concatenate_datasets, Features, Sequence, ClassLabel, Value, DatasetDict\n",
    "from transformers import TrainingArguments\n",
    "from span_marker import SpanMarkerModel, Trainer\n",
    "from span_marker.model_card import SpanMarkerModelCardData\n",
    "from huggingface_hub import upload_folder, upload_file\n",
    "\n",
    "os.environ[\"WANDB_PROJECT\"]=\"spanmarker-ner-single-entity\"\n",
    "\n",
    "def main() -> None:\n",
    "    # Load the dataset, ensure \"tokens\" and \"ner_tags\" columns, and get a list of labels\n",
    "    labels = [\"O\", \"B-PER\", \"I-PER\"]\n",
    "    dataset_id = \"andrewrreed/fewnerd-person-names-augmented\"\n",
    "    dataset = load_dataset(dataset_id)\n",
    "\n",
    "    train_dataset = dataset[\"train\"]\n",
    "    test_dataset = dataset[\"test\"]\n",
    "    val_dataset = dataset[\"validation\"]\n",
    "\n",
    "    # Initialize a SpanMarker model using a pretrained ROBERTA-style encoder\n",
    "    encoder_id = \"roberta-base\"\n",
    "    model_id = \"andrewrreed/span-marker-roberta-base-person-names-augmented\"\n",
    "    model = SpanMarkerModel.from_pretrained(\n",
    "        encoder_id,\n",
    "        labels=labels,\n",
    "        # SpanMarker hyperparameters:\n",
    "        model_max_length=512,\n",
    "        marker_max_length=128,\n",
    "        entity_max_length=8,\n",
    "        # Model card variables\n",
    "        model_card_data=SpanMarkerModelCardData(\n",
    "            model_id=model_id,\n",
    "            dataset_id=dataset_id,\n",
    "            encoder_id=encoder_id,\n",
    "            license=\"cc-by-sa-4.0\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Prepare the ðŸ¤— transformers training arguments\n",
    "    output_dir = Path(\"models\") / model_id\n",
    "    args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        run_name=model_id,\n",
    "        # Training Hyperparameters:\n",
    "        learning_rate=5e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        warmup_ratio=0.1,\n",
    "        bf16=True,  # Replace `bf16` with `fp16` if your hardware can't use bf16.\n",
    "        # Other Training parameters\n",
    "        logging_first_step=True,\n",
    "        logging_steps=200,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=1000,\n",
    "        eval_steps=1000,\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=3,\n",
    "        dataloader_num_workers=4,\n",
    "        report_to=\"wandb\",\n",
    "    )\n",
    "\n",
    "    # Initialize the trainer using our model, training args & dataset, and train\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "    # Compute & save the metrics on the val set\n",
    "    metrics = trainer.evaluate(val_dataset, metric_key_prefix=\"val\")\n",
    "    trainer.save_metrics(\"val\", metrics)\n",
    "\n",
    "    # Save the model & training script locally\n",
    "    trainer.save_model(output_dir / \"checkpoint-final\")\n",
    "    shutil.copy2(__file__, output_dir / \"checkpoint-final\" / \"train.py\")\n",
    "\n",
    "    # Upload everything to the Hub\n",
    "    # model.push_to_hub(model_id, private=True)\n",
    "    # upload_folder(folder_path=output_dir / \"runs\", path_in_repo=\"runs\", repo_id=model_id)\n",
    "    # upload_file(path_or_fileobj=__file__, path_in_repo=\"train.py\", repo_id=model_id)\n",
    "    # upload_file(path_or_fileobj=output_dir / \"all_results.json\", path_in_repo=\"all_results.json\", repo_id=model_id)\n",
    "    # upload_file(path_or_fileobj=output_dir / \"emissions.csv\", path_in_repo=\"emissions.csv\", repo_id=model_id)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from span_marker import SpanMarkerModel\n",
    "\n",
    "model_id = \"andrewrreed/span-marker-roberta-base-person-names-augmented\"\n",
    "output_dir = Path(\"models\") / model_id / \"checkpoint-final\"\n",
    "model = SpanMarkerModel.from_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25fd29b354014f328f1ed454cfa52c3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/andrewrreed/span-marker-roberta-base-person-names-augmented/commit/42529e5602b47129f22463a8f70cdb9a7be6238c', commit_message='Upload model', commit_description='', oid='42529e5602b47129f22463a8f70cdb9a7be6238c', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Provided path: '/home/ubuntu/hf-notebooks/spanmarker-ner-single-entity/models/andrewrreed/span-marker-roberta-base-person-names-augmented/checkpoint-final/runs' is not a directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m upload_folder, upload_file\n\u001b[0;32m----> 3\u001b[0m \u001b[43mupload_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mruns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_in_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mruns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m upload_file(path_or_fileobj\u001b[38;5;241m=\u001b[39m\u001b[38;5;18m__file__\u001b[39m, path_in_repo\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain.py\u001b[39m\u001b[38;5;124m\"\u001b[39m, repo_id\u001b[38;5;241m=\u001b[39mmodel_id)\n\u001b[1;32m      5\u001b[0m upload_file(\n\u001b[1;32m      6\u001b[0m     path_or_fileobj\u001b[38;5;241m=\u001b[39moutput_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall_results.json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     path_in_repo\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall_results.json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m     repo_id\u001b[38;5;241m=\u001b[39mmodel_id,\n\u001b[1;32m      9\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/hf_api.py:1208\u001b[0m, in \u001b[0;36mfuture_compatible.<locals>._inner\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_as_future(fn, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1207\u001b[0m \u001b[38;5;66;03m# Otherwise, call the function normally\u001b[39;00m\n\u001b[0;32m-> 1208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/hf_api.py:4565\u001b[0m, in \u001b[0;36mHfApi.upload_folder\u001b[0;34m(self, repo_id, folder_path, path_in_repo, commit_message, commit_description, token, repo_type, revision, create_pr, parent_commit, allow_patterns, ignore_patterns, delete_patterns, multi_commits, multi_commits_verbose, run_as_future)\u001b[0m\n\u001b[1;32m   4555\u001b[0m ignore_patterns \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m IGNORE_GIT_FOLDER_PATTERNS\n\u001b[1;32m   4557\u001b[0m delete_operations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_upload_folder_deletions(\n\u001b[1;32m   4558\u001b[0m     repo_id\u001b[38;5;241m=\u001b[39mrepo_id,\n\u001b[1;32m   4559\u001b[0m     repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4563\u001b[0m     delete_patterns\u001b[38;5;241m=\u001b[39mdelete_patterns,\n\u001b[1;32m   4564\u001b[0m )\n\u001b[0;32m-> 4565\u001b[0m add_operations \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_upload_folder_additions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_in_repo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_patterns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_patterns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_patterns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_patterns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4570\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4572\u001b[0m \u001b[38;5;66;03m# Optimize operations: if some files will be overwritten, we don't need to delete them first\u001b[39;00m\n\u001b[1;32m   4573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(add_operations) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/hf_api.py:8253\u001b[0m, in \u001b[0;36m_prepare_upload_folder_additions\u001b[0;34m(folder_path, path_in_repo, allow_patterns, ignore_patterns)\u001b[0m\n\u001b[1;32m   8251\u001b[0m folder_path \u001b[38;5;241m=\u001b[39m Path(folder_path)\u001b[38;5;241m.\u001b[39mexpanduser()\u001b[38;5;241m.\u001b[39mresolve()\n\u001b[1;32m   8252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m folder_path\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[0;32m-> 8253\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProvided path: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not a directory\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   8255\u001b[0m \u001b[38;5;66;03m# List files from folder\u001b[39;00m\n\u001b[1;32m   8256\u001b[0m relpath_to_abspath \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   8257\u001b[0m     path\u001b[38;5;241m.\u001b[39mrelative_to(folder_path)\u001b[38;5;241m.\u001b[39mas_posix(): path\n\u001b[1;32m   8258\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(folder_path\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m**/*\u001b[39m\u001b[38;5;124m\"\u001b[39m))  \u001b[38;5;66;03m# sorted to be deterministic\u001b[39;00m\n\u001b[1;32m   8259\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m path\u001b[38;5;241m.\u001b[39mis_file()\n\u001b[1;32m   8260\u001b[0m }\n",
      "\u001b[0;31mValueError\u001b[0m: Provided path: '/home/ubuntu/hf-notebooks/spanmarker-ner-single-entity/models/andrewrreed/span-marker-roberta-base-person-names-augmented/checkpoint-final/runs' is not a directory"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import upload_folder, upload_file\n",
    "\n",
    "upload_folder(folder_path=output_dir / \"runs\", path_in_repo=\"runs\", repo_id=model_id)\n",
    "upload_file(path_or_fileobj=__file__, path_in_repo=\"train.py\", repo_id=model_id)\n",
    "upload_file(\n",
    "    path_or_fileobj=output_dir / \"all_results.json\",\n",
    "    path_in_repo=\"all_results.json\",\n",
    "    repo_id=model_id,\n",
    ")\n",
    "upload_file(\n",
    "    path_or_fileobj=output_dir / \"emissions.csv\",\n",
    "    path_in_repo=\"emissions.csv\",\n",
    "    repo_id=model_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
