{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train SpanMarker to Improve Single Entity NER Model\n",
    "\n",
    "Goals:\n",
    "1. Build custom dataset specifically for \"Person\" entity\n",
    "2. Train SpanMarker NER model (maybe hyperparameter optimization)\n",
    "3. Evaluate performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install span_marker[wandb] names-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Explore and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'fine_ner_tags'],\n",
       "        num_rows: 131767\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'fine_ner_tags'],\n",
       "        num_rows: 18824\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'fine_ner_tags'],\n",
       "        num_rows: 37648\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_id = \"DFKI-SLT/few-nerd\"\n",
    "dataset = load_dataset(dataset_id, \"supervised\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '0', 'tokens': ['Paul', 'International', 'airport', '.'], 'ner_tags': [0, 0, 0, 0], 'fine_ner_tags': [0, 0, 0, 0]}\n",
      "\n",
      "{'id': '1', 'tokens': ['It', 'starred', 'Hicks', \"'s\", 'wife', ',', 'Ellaline', 'Terriss', 'and', 'Edmund', 'Payne', '.'], 'ner_tags': [0, 0, 7, 0, 0, 0, 7, 7, 0, 7, 7, 0], 'fine_ner_tags': [0, 0, 51, 0, 0, 0, 50, 50, 0, 50, 50, 0]}\n",
      "\n",
      "{'id': '2', 'tokens': ['``', 'Time', '``', 'magazine', 'said', 'the', 'film', 'was', '``', 'a', 'multimillion', 'dollar', 'improvisation', 'that', 'does', 'everything', 'but', 'what', 'the', 'title', 'promises', \"''\", 'and', 'suggested', 'that', '``', 'writer', 'George', 'Axelrod', '(', '``', 'The', 'Seven', 'Year', 'Itch', '``', ')', 'and', 'director', 'Richard', 'Quine', 'should', 'have', 'taken', 'a', 'hint', 'from', 'Holden', '[', \"'s\", 'character', 'Richard', 'Benson', ']', ',', 'who', 'writes', 'his', 'movie', ',', 'takes', 'a', 'long', 'sober', 'look', 'at', 'what', 'he', 'has', 'wrought', ',', 'and', 'burns', 'it', '.', \"''\"], 'ner_tags': [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 7, 7, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 7, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'fine_ner_tags': [0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 51, 51, 0, 0, 6, 6, 6, 6, 0, 0, 0, 0, 53, 53, 0, 0, 0, 0, 0, 0, 54, 0, 0, 0, 54, 54, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sample in dataset[\"train\"].select(range(3)):\n",
    "    print(sample)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'art', 'building', 'event', 'location', 'organization', 'other', 'person', 'product']\n"
     ]
    }
   ],
   "source": [
    "# inspect labels\n",
    "labels = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset that consists of examples with `Person` entity only\n",
    "\n",
    "**Note** - Exactly half of the examples in each split contain Person entities, while other half don't have any. This is to help with diversity of training data and model generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import ClassLabel, Features, Sequence, Value\n",
    "\n",
    "\n",
    "FEATURES = Features(\n",
    "    {\n",
    "        \"tokens\": Sequence(feature=Value(dtype=\"string\")),\n",
    "        \"ner_tags\": Sequence(feature=ClassLabel(names=[\"O\", \"B-PER\", \"I-PER\"])),\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "def get_fewnerd(entity_idx):\n",
    "    \"\"\"Loads FewNERD dataset and reformats labels to only include those for the specified entity.\n",
    "\n",
    "    Args:\n",
    "        entity_idx (int): The index of the entity from FewNERD dataset to keep.\n",
    "\n",
    "    Returns:\n",
    "        dataset: The filtered dataset containing `ner_tags` for only the specified entity.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def mapper(sample):\n",
    "        sample[\"ner_tags\"] = [int(tag == entity_idx) for tag in sample[\"ner_tags\"]]\n",
    "        sample[\"ner_tags\"] = [\n",
    "            2 if tag == 1 and idx > 0 and sample[\"ner_tags\"][idx - 1] == 1 else tag\n",
    "            for idx, tag in enumerate(sample[\"ner_tags\"])\n",
    "        ]\n",
    "        return sample\n",
    "\n",
    "    dataset = load_dataset(\"DFKI-SLT/few-nerd\", \"supervised\")\n",
    "    dataset = dataset.map(mapper, remove_columns=[\"id\", \"fine_ner_tags\"])\n",
    "    dataset = dataset.cast(FEATURES)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3579355ba4ff4be8a01039c591e1f440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/131767 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ccd7949deea49d098dce87048c8e274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18824 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c8eb6b104a54bd8b718a72fedda27c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/37648 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa6785442c924064a1893e7e73b0ba8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/131767 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "839fe272815047e2948e51c7c64c2d81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/18824 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d06ef25853d4a80b487aa0c6a29204a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/37648 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 131767\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 18824\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 37648\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fewnerd_dataset = get_fewnerd(7)\n",
    "fewnerd_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from datasets import concatenate_datasets, DatasetDict, Dataset\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "def has_person(sample):\n",
    "    return bool(sum(sample[\"ner_tags\"]))\n",
    "\n",
    "\n",
    "def has_no_person(sample):\n",
    "    return not has_person(sample)\n",
    "\n",
    "\n",
    "def preprocess_raw_dataset(raw_dataset):\n",
    "    \"\"\"\n",
    "    Preprocesses the raw dataset by filtering out examples that do not contain a person entity,\n",
    "    balancing the dataset by randomly selecting and including examples without a person entity,\n",
    "    and concatenating the filtered datasets.\n",
    "\n",
    "    Args:\n",
    "        raw_dataset (Dataset): The raw dataset to be preprocessed.\n",
    "\n",
    "    Returns:\n",
    "        Dataset: The preprocessed dataset.\n",
    "    \"\"\"\n",
    "    dataset_person = raw_dataset.filter(has_person)\n",
    "    dataset_no_person = raw_dataset.filter(has_no_person)\n",
    "    dataset_no_person = dataset_no_person.select(\n",
    "        random.sample(range(len(dataset_no_person)), k=len(dataset_person))\n",
    "    )\n",
    "    dataset = concatenate_datasets([dataset_person, dataset_no_person])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23c9d62092814e9884e10836748585a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/131767 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2034a288d68746e580b6b4e1310fa4fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/131767 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce9288b0014a41879d9dc7c0cfd74c1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/37648 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f3aa4b8a68e40479417070c8998431f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/37648 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f937c797de254cbf87f2045072f10c0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/18824 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93d697119f4f42179ec50515fd25b384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/18824 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 85524\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 12546\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 24422\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare dataset\n",
    "processed_fewnerd_train = preprocess_raw_dataset(fewnerd_dataset[\"train\"])\n",
    "processed_fewnerd_test = preprocess_raw_dataset(fewnerd_dataset[\"test\"])\n",
    "processed_fewnerd_val = preprocess_raw_dataset(fewnerd_dataset[\"validation\"])\n",
    "\n",
    "processed_dataset_dict = DatasetDict(\n",
    "    {\n",
    "        \"train\": processed_fewnerd_train,\n",
    "        \"validation\": processed_fewnerd_val,\n",
    "        \"test\": processed_fewnerd_test,\n",
    "    }\n",
    ")\n",
    "processed_dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['It', 'starred', 'Hicks', \"'s\", 'wife', ',', 'Ellaline', 'Terriss', 'and', 'Edmund', 'Payne', '.'], 'ner_tags': [0, 0, 1, 0, 0, 0, 1, 2, 0, 1, 2, 0]}\n",
      "\n",
      "{'tokens': ['``', 'Time', '``', 'magazine', 'said', 'the', 'film', 'was', '``', 'a', 'multimillion', 'dollar', 'improvisation', 'that', 'does', 'everything', 'but', 'what', 'the', 'title', 'promises', \"''\", 'and', 'suggested', 'that', '``', 'writer', 'George', 'Axelrod', '(', '``', 'The', 'Seven', 'Year', 'Itch', '``', ')', 'and', 'director', 'Richard', 'Quine', 'should', 'have', 'taken', 'a', 'hint', 'from', 'Holden', '[', \"'s\", 'character', 'Richard', 'Benson', ']', ',', 'who', 'writes', 'his', 'movie', ',', 'takes', 'a', 'long', 'sober', 'look', 'at', 'what', 'he', 'has', 'wrought', ',', 'and', 'burns', 'it', '.', \"''\"], 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "\n",
      "{'tokens': ['Imelda', 'de', \"'\", 'Lambertazzi', 'is', 'a', '``', 'melodramma', 'tragico', \"''\", 'or', 'tragic', 'opera', 'in', 'two', 'acts', 'by', 'Gaetano', 'Donizett', 'i', 'from', 'a', 'libretto', 'by', 'Andrea', 'Leone', 'Tottola', ',', 'based', 'on', 'the', 'tragedy', '``', 'Imelda', \"''\", 'by', 'Gabriele', 'Sperduti', '.'], 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "\n",
      "{'tokens': ['In', '1783', 'Campbell', 'married', 'Olympia', 'Elizabeth', '(', 'died', '1794', ')', ',', 'eldest', 'daughter', 'of', 'William', 'Morshead', 'of', 'Cartuther', ',', 'Cornwall', '.'], 'ner_tags': [0, 0, 1, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0]}\n",
      "\n",
      "{'tokens': ['Neville', 'performed', 'a', 'Spinning', 'Back', 'Kick', 'on', 'Tozawa', 'to', 'retain', 'the', 'title', '.'], 'ner_tags': [1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]}\n",
      "\n",
      "{'tokens': ['From', 'her', 'locations', 'Mrs.', 'Strong', 'created', 'papers', 'for', 'the', 'Duke', 'of', 'Windsor', 'and', 'Wallis', ',', 'The', 'Duchess', 'of', 'Windsor', ',', 'Barbara', 'Hutton', ',', 'the', 'Rockefeller', ',', 'Astor', ',', 'Vanderbilt', ',', 'and', 'DuPont', 'families', ',', 'as', 'well', 'as', 'Bette', 'Davis', ',', 'Diana', 'Vreeland', ',', 'Jacqueline', 'Bouvier', 'Kennedy', ',', 'Barbara', 'Paley', ',', 'and', 'other', 'icons', 'of', 'style', '.'], 'ner_tags': [0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 2, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 2, 0, 1, 2, 0, 1, 2, 2, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0]}\n",
      "\n",
      "{'tokens': ['As', 'a', 'result', 'of', 'outdoing', 'the', 'competition', ',', 'Binion', 'received', 'death', 'threats', ',', 'although', 'eventually', 'casinos', 'raised', 'their', 'limits', 'to', 'keep', 'up', 'with', 'him', '.'], 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "\n",
      "{'tokens': ['During', 'medical', 'school', ',', 'Reitman', 'often', 'worked', 'as', 'the', 'house', 'doctor', 'at', 'sporting', 'events', 'at', 'the', 'Boston', 'Garden', '.'], 'ner_tags': [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "\n",
      "{'tokens': ['Formerly', 'known', 'as', 'the', 'Communiplex', ',', 'it', 'is', 'home', 'to', 'the', 'WHL', \"'s\", 'Prince', 'Albert', 'Raiders', 'and', 'the', 'Prince', 'Albert', 'Mintos', '.'], 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 1, 2, 0]}\n",
      "\n",
      "{'tokens': ['The', 'chapel', 'of', 'Stalmine', 'was', 'first', 'mentioned', 'about', '1200', 'and', 'a', 'cemetery', 'was', 'consecrated', 'in', '1230', '.'], 'ner_tags': [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sample in processed_dataset_dict[\"train\"].select(range(10)):\n",
    "    print(sample)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augment the dataset with additional examples\n",
    "\n",
    "Peoples names often occur as standalone strings rather than just being included in a sentence with contextual information. To improve performance on names occuring in this paradigm, let's create some specific examples to augment our dataset with.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from names_dataset import NameDataset\n",
    "\n",
    "\n",
    "def generate_full_names(n, country_codes):\n",
    "    \"\"\"Creates a list of full names from the top `n` first and last names for each country.\n",
    "\n",
    "    Randomly mixes first and last names.\n",
    "\n",
    "    Uses this library: https://github.com/philipperemy/name-dataset\n",
    "\n",
    "    Args:\n",
    "        n (int): The number of first and last names to use for each country.\n",
    "        country_codes (list): A list of country codes to use.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of full names.\n",
    "    \"\"\"\n",
    "    nd = NameDataset()\n",
    "    full_names = []\n",
    "    for code in country_codes:\n",
    "        first_names = nd.get_top_names(n=math.ceil(n / 2), country_alpha2=code)[code]\n",
    "        first_names = first_names[\"M\"] + first_names[\"F\"]\n",
    "\n",
    "        last_names = nd.get_top_names(n=n, country_alpha2=code, use_first_names=False)[\n",
    "            code\n",
    "        ]\n",
    "\n",
    "        random.seed(42)\n",
    "        random.shuffle(first_names)\n",
    "        random.shuffle(last_names)\n",
    "\n",
    "        if len(first_names) != len(last_names):\n",
    "            first_names = first_names[: len(last_names)]\n",
    "\n",
    "        full_names += [\n",
    "            f\"{first} {last}\" for first, last in zip(first_names, last_names)\n",
    "        ]\n",
    "\n",
    "    return full_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_names_dataset():\n",
    "    def tokenize_and_annotate_names(example):\n",
    "        example[\"tokens\"] = example[\"text\"].split(\" \")\n",
    "        example[\"ner_tags\"] = [\n",
    "            1 if idx == 0 else 2 for idx, _ in enumerate(example[\"tokens\"])\n",
    "        ]\n",
    "        return example\n",
    "\n",
    "    # generate list of full names\n",
    "    nd = NameDataset()\n",
    "    country_codes = [country.alpha_2 for country in nd.get_country_codes()]\n",
    "    sample_names = generate_full_names(500, country_codes=country_codes)\n",
    "    random.shuffle(sample_names)\n",
    "\n",
    "    # create HF dataset with required features\n",
    "    names_dataset = Dataset.from_dict({\"text\": sample_names})\n",
    "    names_dataset = names_dataset.map(\n",
    "        tokenize_and_annotate_names, remove_columns=[\"text\"]\n",
    "    )\n",
    "    names_dataset = names_dataset.cast(FEATURES)\n",
    "\n",
    "    return names_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86ee6a22c2e74083b26fe51546b0926b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/52472 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79f3c065e04b4812b44517c5e62fc6f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/52472 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "names_dataset = build_names_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['Виктор', 'Sochirca'], 'ner_tags': [1, 2]}\n",
      "\n",
      "{'tokens': ['Hugo', 'Inacio'], 'ner_tags': [1, 2]}\n",
      "\n",
      "{'tokens': ['Fransiska', 'Manalu'], 'ner_tags': [1, 2]}\n",
      "\n",
      "{'tokens': ['Mark', 'Danladi'], 'ner_tags': [1, 2]}\n",
      "\n",
      "{'tokens': ['Nilima', 'Bappy'], 'ner_tags': [1, 2]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sample in names_dataset.select(range(5)):\n",
    "    print(sample)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_dataset_traintest = names_dataset.train_test_split(test_size=0.3, seed=42)\n",
    "names_dataset_val = names_dataset_traintest[\"test\"].train_test_split(\n",
    "    test_size=0.5, seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_dataset_dict = DatasetDict(\n",
    "    {\n",
    "        \"train\": names_dataset_traintest[\"train\"],\n",
    "        \"validation\": names_dataset_val[\"train\"],\n",
    "        \"test\": names_dataset_val[\"test\"],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset_dict = DatasetDict()\n",
    "for key in processed_dataset_dict.keys():\n",
    "    full_dataset_dict[key] = concatenate_datasets(\n",
    "        [processed_dataset_dict[key], names_dataset_dict[key]]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 122254\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 20417\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 32293\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train SpanMarker NER model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
