{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face Inference Endpoints x Langchain Chat Models\n",
    "\n",
    "\n",
    "Open source LLMs are becoming strong general purpose agents. The purpose of this notebook is to demonstrate how to make use of open-source LLMs within as chat models within Langchain to enable their usage and experimentation with agent-based pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers langchain text-generation langchain-experimental python-dotenv jinja2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load env vars for LangSmith\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, let's see if how Llama-2 Chat wrapper works\n",
    "\n",
    "This section builds upon this [Langchain integration](https://python.langchain.com/docs/integrations/chat/llama2_chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.llms import HuggingFaceTextGenInference\n",
    "from langchain_experimental.chat_models import Llama2Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewreed/Documents/hf-notebooks/.venv/lib/python3.10/site-packages/pydantic/_internal/_fields.py:149: UserWarning: Field \"model_id\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# model = \"https://b64oqapulf4lv8w1.us-east-1.aws.endpoints.huggingface.cloud\"\n",
    "model = \"https://zsg12cshlzvfl3l3.us-east-1.aws.endpoints.huggingface.cloud\"\n",
    "hf_token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "\n",
    "llm = HuggingFaceTextGenInference(\n",
    "    inference_server_url=model,\n",
    "    max_new_tokens=512,\n",
    "    top_k=50,\n",
    "    temperature=0.1,\n",
    "    repetition_penalty=1.03,\n",
    "    server_kwargs={\n",
    "        \"headers\": {\n",
    "            \"Authorization\": f\"Bearer {hf_token}\",\n",
    "            \"Content-Type\": \"application/json\",\n",
    "        }\n",
    "    },\n",
    ")\n",
    "\n",
    "model = Llama2Chat(llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You're a helpful assistant\"),\n",
    "    HumanMessage(content=\"What is the purpose of model regularization?\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"  Model regularization is a technique used in machine learning to prevent overfitting, improve generalization, and promote interpretability of a model. Overfitting occurs when a model is trained too well on the training data and fails to generalize well to new, unseen data. This can result in poor performance on the test data.\\n\\nThe purpose of model regularization is to add a penalty term to the loss function that discourages the model from fitting the training data too closely. By adding this penalty term, the model is forced to find a simpler solution that generalizes better to new data.\\n\\nThere are several types of regularization techniques, including:\\n\\n1. L1 Regularization (Lasso): This adds a penalty term to the loss function based on the absolute value of the model's weights. L1 regularization tends to produce sparse models, where some of the weights are set to zero.\\n2. L2 Regularization (Ridge): This adds a penalty term to the loss function based on the square of the model's weights. L2 regularization tends to produce models with smaller weights, which can help to reduce overfitting.\\n3. Dropout Regularization: This is a type of regularization that is applied during training. It randomly sets a fraction of the model's neurons to zero during training, effectively creating an ensemble of different sub-networks. This helps to prevent any single neuron from dominating the model's behavior.\\n4. Early Stopping: This is a regularization technique that stops training a model before it overfits the training data. The model is trained until the validation loss stops improving, indicating that the model has reached a good balance between accuracy and complexity.\\n5. Batch Normalization: This is a regularization technique that normalizes the inputs to each layer of the model. This helps to reduce the effect of internal covariate shift and improve the generalization of the model.\\n6. Weight Decay: This is a regularization technique that adds a penalty term to the loss function based on the magnitude of the model's weights. This helps to prevent the model from relying too heavily on any single weight.\\n\\nBy using regularization techniques, machine learning models can be prevented from overfitting to the training data, improved their ability to generalize to new data, and made more interpretable by reducing the complexity of the model.\")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s>[INST] <<SYS>>\\nYou're a helpful assistant\\n<</SYS>>\\n\\nWhat is the purpose of model regularization? [/INST]\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._to_chat_prompt(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test in a LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruff: noqa: E402\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "\n",
    "template_messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
    "]\n",
    "prompt_template = ChatPromptTemplate.from_messages(template_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['chat_history', 'text'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessage(content='You are a helpful assistant.'), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], template='{text}'))])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "chain = LLMChain(llm=model, prompt=prompt_template, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = chain.run(\n",
    "    text=\"What can I see in Vienna? Propose a few locations. Names only, no details.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Certainly! Vienna is a beautiful city with a rich history and culture, offering countless attractions to explore. Here are some of the top locations to consider visiting:\n",
      "\n",
      "1. Sch√∂nbrunn Palace\n",
      "2. St. Stephen's Cathedral\n",
      "3. Hofburg Palace\n",
      "4. Belvedere Palace\n",
      "5. Prater Park\n",
      "6. MuseumsQuartier\n",
      "7. Ringstrasse\n",
      "8. Vienna State Opera\n",
      "9. Albertina Museum\n",
      "10. Rathausplatz\n"
     ]
    }
   ],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = chain.run(text=\"Tell me more about #7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Of course! The seventh location I mentioned is the Ringstrasse, which is a grand boulevard that encircles the historic center of Vienna. The Ringstrasse is home to many of the city's most famous landmarks and attractions, including:\n",
      "\n",
      "1. Vienna State Opera: A world-renowned opera house known for its opulent architecture and high-quality productions.\n",
      "2. Parliament Building: A grand neo-Gothic structure that serves as the seat of Austria's federal government.\n",
      "3. Town Hall: A beautiful building with a stunning clock tower that offers panoramic views of the city.\n",
      "4. Imperial Palace: A former royal residence that now houses several museums and event spaces.\n",
      "5. Museum of Natural History: A popular museum featuring exhibits on natural history, including dinosaurs, fossils, and minerals.\n",
      "6. Museum of Art History: A museum showcasing works of art from the Middle Ages to the present day.\n",
      "7. Vienna University: A prestigious university with a beautiful neo-Renaissance main building.\n",
      "8. Augarten Park: A large park with walking paths, gardens, and a beautiful view of the city.\n",
      "\n",
      "The Ringstrasse is a must-visit destination for anyone interested in history, culture, and architecture. It offers a unique glimpse into Vienna's rich heritage and is a great place to start your exploration of the city.\n"
     ]
    }
   ],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now lets build our own for `HuggingFaceH4/zephyr-7b-beta`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|system|>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\\\n\\\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.</s>\\n<|user|>\\nHow many helicopters can a human eat in one sitting?</s>\\n<|assistant|>\\n\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_input = \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\\\n\\\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\n",
    "user_input = \"How many helicopters can a human eat in one sitting?\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_input},\n",
    "    {\"role\": \"user\", \"content\": user_input},\n",
    "]\n",
    "tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{% for message in messages %}\n",
      "{% if message['role'] == 'user' %}\n",
      "{{ '<|user|>\n",
      "' + message['content'] + eos_token }}\n",
      "{% elif message['role'] == 'system' %}\n",
      "{{ '<|system|>\n",
      "' + message['content'] + eos_token }}\n",
      "{% elif message['role'] == 'assistant' %}\n",
      "{{ '<|assistant|>\n",
      "'  + message['content'] + eos_token }}\n",
      "{% endif %}\n",
      "{% if loop.last and add_generation_prompt %}\n",
      "{{ '<|assistant|>' }}\n",
      "{% endif %}\n",
      "{% endfor %}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = messages[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': 'What is the purpose of model regularization?',\n",
       " 'additional_kwargs': {},\n",
       " 'type': 'human',\n",
       " 'example': False}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://zsg12cshlzvfl3l3.us-east-1.aws.endpoints.huggingface.cloud'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.inference_server_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = list_inference_endpoints()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HuggingFaceH4/zephyr-7b-beta'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint.repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import get_inference_endpoint, list_inference_endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = get_inference_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Optional, Union\n",
    "\n",
    "from huggingface_hub import list_inference_endpoints\n",
    "from langchain.callbacks.manager import (\n",
    "    AsyncCallbackManagerForLLMRun,\n",
    "    CallbackManagerForLLMRun,\n",
    ")\n",
    "from langchain.chat_models.base import BaseChatModel\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    BaseMessage,\n",
    "    ChatGeneration,\n",
    "    ChatResult,\n",
    "    HumanMessage,\n",
    "    LLMResult,\n",
    "    SystemMessage,\n",
    ")\n",
    "\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"You are a helpful, respectful and honest assistant.\"\"\"\n",
    "\n",
    "\n",
    "class HFInferenceEndpointWrapper(BaseChatModel):\n",
    "    \"\"\"\n",
    "    Wrapper for using HuggingFaceTextGenInference LLM as a ChatModel.\n",
    "\n",
    "    Adapted from: https://python.langchain.com/docs/integrations/chat/llama2_chat\n",
    "    \"\"\"\n",
    "\n",
    "    llm: LLM\n",
    "    tokenizer: Any\n",
    "    system_message: SystemMessage = SystemMessage(content=DEFAULT_SYSTEM_PROMPT)\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._resolve_model_id()\n",
    "\n",
    "    # llm: LLM\n",
    "    # tokenizer: Any\n",
    "    # system_message: SystemMessage = SystemMessage(content=DEFAULT_SYSTEM_PROMPT)\n",
    "\n",
    "    def _generate(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "        llm_input = self._to_chat_prompt(messages)\n",
    "        llm_result = self.llm._generate(\n",
    "            prompts=[llm_input], stop=stop, run_manager=run_manager, **kwargs\n",
    "        )\n",
    "        return self._to_chat_result(llm_result)\n",
    "\n",
    "    async def _agenerate(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "        llm_input = self._to_chat_prompt(messages)\n",
    "        llm_result = await self.llm._agenerate(\n",
    "            prompts=[llm_input], stop=stop, run_manager=run_manager, **kwargs\n",
    "        )\n",
    "        return self._to_chat_result(llm_result)\n",
    "\n",
    "    def _to_chat_prompt(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "    ) -> str:\n",
    "        \"\"\"Convert a list of messages into a prompt format expected by wrapped LLM.\"\"\"\n",
    "        if not messages:\n",
    "            raise ValueError(\"at least one HumanMessage must be provided\")\n",
    "\n",
    "        if not isinstance(messages[0], SystemMessage):\n",
    "            messages = [self.system_message] + messages\n",
    "\n",
    "        if not isinstance(messages[1], HumanMessage):\n",
    "            raise ValueError(\n",
    "                \"messages list must start with a SystemMessage or UserMessage\"\n",
    "            )\n",
    "\n",
    "        if not isinstance(messages[-1], HumanMessage):\n",
    "            raise ValueError(\"last message must be a HumanMessage\")\n",
    "\n",
    "        messages = [self._to_chatml_format(m) for m in messages]\n",
    "\n",
    "        return self.tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "    def _to_chatml_format(\n",
    "        self, message: Union[AIMessage, SystemMessage, HumanMessage]\n",
    "    ) -> dict:\n",
    "        \"\"\"Convert LangChain message to ChatML format.\"\"\"\n",
    "\n",
    "        if isinstance(message, SystemMessage):\n",
    "            role = \"system\"\n",
    "        elif isinstance(message, AIMessage):\n",
    "            role = \"assistant\"\n",
    "        elif isinstance(message, HumanMessage):\n",
    "            role = \"user\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown message type: {type(message)}\")\n",
    "\n",
    "        return {\"role\": role, \"content\": message.content}\n",
    "\n",
    "    @staticmethod\n",
    "    def _to_chat_result(llm_result: LLMResult) -> ChatResult:\n",
    "        chat_generations = []\n",
    "\n",
    "        for g in llm_result.generations[0]:\n",
    "            chat_generation = ChatGeneration(\n",
    "                message=AIMessage(content=g.text), generation_info=g.generation_info\n",
    "            )\n",
    "            chat_generations.append(chat_generation)\n",
    "\n",
    "        return ChatResult(\n",
    "            generations=chat_generations, llm_output=llm_result.llm_output\n",
    "        )\n",
    "\n",
    "    def _resolve_model_id(self):\n",
    "        \"\"\"Resolve the model id for the given inference server url\"\"\"\n",
    "        available_endpoints = list_inference_endpoints(\"*\")\n",
    "\n",
    "        for endpoint in available_endpoints:\n",
    "            if endpoint.url == self.llm.inference_server_url:\n",
    "                return endpoint.repository\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Could not find model id for inference server provided: {self.llm.inference_server_url}. Check to ensure the HF token you're using has access to the endpoint.\"\n",
    "                )\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"hf-ie-style\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You're a helpful assistant\"),\n",
    "    HumanMessage(content=\"What is the purpose of model regularization?\"),\n",
    "]\n",
    "\n",
    "model = HFInferenceEndpointWrapper(llm=llm, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|system|>\\nYou're a helpful assistant</s>\\n<|user|>\\nWhat is the purpose of model regularization?</s>\\n<|assistant|>\\n\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._to_chat_prompt(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Model regularization is a technique used in machine learning to prevent overfitting, which occurs when a model is trained too well on a limited dataset and fails to generalize well to new, unseen data. Overfitting occurs when a model learns the noise or random fluctuations in the training data rather than the underlying patterns. By adding a penalty term to the loss function, regularization encourages the model to find a simpler solution that generalizes better to new data.\\n\\nThere are several types of regularization, including:\\n\\n1. L1 regularization (Lasso): This adds a penalty term to the loss function based on the absolute value of the model's weights. L1 regularization tends to produce sparse models, where some weights are set to zero.\\n2. L2 regularization (Ridge): This adds a penalty term to the loss function based on the square of the model's weights. L2 regularization tends to produce models with smaller weights.\\n3. Dropout regularization: This is a type of regularization that is applied during training by randomly setting a fraction of the model's neurons to zero. This forces the model to learn multiple representations of the data, which can help to prevent overfitting.\\n4. Early stopping: This is a type of regularization that involves monitoring the validation loss during training and stopping the training process when the validation loss stops improving. This helps to prevent overfitting by stopping the training process before the model has a chance to fit the noise in the training data.\\n\\nRegularization can be applied to different layers of the neural network, such as the input layer, hidden layers, or output layer. It can also be applied to different parts of the network, such as the weights, biases, or activations.\\n\\nBy using regularization techniques, machine learning models can be improved in terms of their generalization ability, which can lead to better performance on unseen data and improved accuracy in real-world applications.\")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
