{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face Inference Endpoints x Langchain Chat Models\n",
    "\n",
    "\n",
    "Open source LLMs are becoming strong general purpose agents. The goal of this notebook is to demonstrate how to make use of open-source LLMs as chat models via [Hugging Face Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index) with [LangChain's ChatModel abstraction](https://python.langchain.com/docs/modules/model_io/chat/) to enable their usage and experimentation with agent-based workflows.\n",
    "\n",
    "In particular, we will:\n",
    "1. Utilize the [HuggingFaceTextGenInference](https://python.langchain.com/docs/integrations/llms/huggingface_textgen_inference) integration to call Inference Endpoints that are serving LLMs via [Text Generation Inference (TGI)](https://huggingface.co/docs/text-generation-inference/index)\n",
    "2. Create a wrapper around the `BaseChatModel` class that interfaces between LangChain's [Chat Messages](https://python.langchain.com/docs/modules/model_io/chat/#messages) and the hosted LLM by leveraging [Hugging Face's Chat Templates](https://huggingface.co/docs/transformers/chat_templating).\n",
    "3. Use an open-source LLM to power an `ChatAgent` pipeline\n",
    "\n",
    "\n",
    "\n",
    "> Note: To run this notebook, you'll need to have:\n",
    "> - an LLM deployed via a Hugging Face Inference Endpoint (the LLM must have a `chat_template` defined in its `tokenizer_config.json`)\n",
    "> - A Hugging Face Token with access to the deployed endpoint saved as an environment variable: `HUGGINGFACEHUB_API_TOKEN`\n",
    "> - A SerpAPI key saved as an environment variable: `SERPAPI_API_KEY`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers langchain langchain-experimental text-generation python-dotenv jinja2 google-search-results langchainhub numexpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instantiate an LLM with `HuggingFaceTextGenInference`\n",
    "\n",
    "You'll need to have a running Inference Endpoint available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.llms import HuggingFaceTextGenInference\n",
    "\n",
    "ENDPOINT_URL = \"https://b64oqapulf4lv8w1.us-east-1.aws.endpoints.huggingface.cloud\"\n",
    "HF_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "\n",
    "llm = HuggingFaceTextGenInference(\n",
    "    inference_server_url=ENDPOINT_URL,\n",
    "    max_new_tokens=512,\n",
    "    top_k=50,\n",
    "    temperature=0.1,\n",
    "    repetition_penalty=1.03,\n",
    "    server_kwargs={\n",
    "        \"headers\": {\n",
    "            \"Authorization\": f\"Bearer {HF_TOKEN}\",\n",
    "            \"Content-Type\": \"application/json\",\n",
    "        }\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a wrapper for `BaseChatModel` to apply chat templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Optional, Union\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from huggingface_hub import list_inference_endpoints\n",
    "from langchain.callbacks.manager import (\n",
    "    AsyncCallbackManagerForLLMRun,\n",
    "    CallbackManagerForLLMRun,\n",
    ")\n",
    "from langchain.chat_models.base import BaseChatModel\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    BaseMessage,\n",
    "    ChatGeneration,\n",
    "    ChatResult,\n",
    "    HumanMessage,\n",
    "    LLMResult,\n",
    "    SystemMessage,\n",
    ")\n",
    "\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"You are a helpful, respectful and honest assistant.\"\"\"\n",
    "\n",
    "\n",
    "class HFInferenceEndpointChatWrapper(BaseChatModel):\n",
    "    \"\"\"\n",
    "    Wrapper for using HuggingFaceTextGenInference LLM as a ChatModel.\n",
    "\n",
    "    Upon instantiating this class, the model_id is resolved from the inference_server_url provided to the LLM,\n",
    "    and the appropriate tokenizer is loaded from the HuggingFace Hub.\n",
    "\n",
    "    Adapted from: https://python.langchain.com/docs/integrations/chat/llama2_chat\n",
    "    \"\"\"\n",
    "\n",
    "    llm: LLM\n",
    "    tokenizer: Any\n",
    "    system_message: SystemMessage = SystemMessage(content=DEFAULT_SYSTEM_PROMPT)\n",
    "    model_id: Optional[str] = None\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._resolve_model_id()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n",
    "\n",
    "    def _generate(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "        llm_input = self._to_chat_prompt(messages)\n",
    "        llm_result = self.llm._generate(\n",
    "            prompts=[llm_input], stop=stop, run_manager=run_manager, **kwargs\n",
    "        )\n",
    "        return self._to_chat_result(llm_result)\n",
    "\n",
    "    async def _agenerate(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "        llm_input = self._to_chat_prompt(messages)\n",
    "        llm_result = await self.llm._agenerate(\n",
    "            prompts=[llm_input], stop=stop, run_manager=run_manager, **kwargs\n",
    "        )\n",
    "        return self._to_chat_result(llm_result)\n",
    "\n",
    "    def _to_chat_prompt(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "    ) -> str:\n",
    "        \"\"\"Convert a list of messages into a prompt format expected by wrapped LLM.\"\"\"\n",
    "        if not messages:\n",
    "            raise ValueError(\"at least one HumanMessage must be provided\")\n",
    "\n",
    "        if not isinstance(messages[0], SystemMessage):\n",
    "            messages = [self.system_message] + messages\n",
    "\n",
    "        if not isinstance(messages[1], HumanMessage):\n",
    "            raise ValueError(\n",
    "                \"messages list must start with a SystemMessage or UserMessage\"\n",
    "            )\n",
    "\n",
    "        if not isinstance(messages[-1], HumanMessage):\n",
    "            raise ValueError(\"last message must be a HumanMessage\")\n",
    "\n",
    "        messages = [self._to_chatml_format(m) for m in messages]\n",
    "\n",
    "        return self.tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "    def _to_chatml_format(\n",
    "        self, message: Union[AIMessage, SystemMessage, HumanMessage]\n",
    "    ) -> dict:\n",
    "        \"\"\"Convert LangChain message to ChatML format.\"\"\"\n",
    "\n",
    "        if isinstance(message, SystemMessage):\n",
    "            role = \"system\"\n",
    "        elif isinstance(message, AIMessage):\n",
    "            role = \"assistant\"\n",
    "        elif isinstance(message, HumanMessage):\n",
    "            role = \"user\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown message type: {type(message)}\")\n",
    "\n",
    "        return {\"role\": role, \"content\": message.content}\n",
    "\n",
    "    @staticmethod\n",
    "    def _to_chat_result(llm_result: LLMResult) -> ChatResult:\n",
    "        chat_generations = []\n",
    "\n",
    "        for g in llm_result.generations[0]:\n",
    "            chat_generation = ChatGeneration(\n",
    "                message=AIMessage(content=g.text), generation_info=g.generation_info\n",
    "            )\n",
    "            chat_generations.append(chat_generation)\n",
    "\n",
    "        return ChatResult(\n",
    "            generations=chat_generations, llm_output=llm_result.llm_output\n",
    "        )\n",
    "\n",
    "    def _resolve_model_id(self):\n",
    "        \"\"\"Resolve the model_id from the LLM's inference_server_url\"\"\"\n",
    "        available_endpoints = list_inference_endpoints(\"*\")\n",
    "\n",
    "        for endpoint in available_endpoints:\n",
    "            if endpoint.url == self.llm.inference_server_url:\n",
    "                self.model_id = endpoint.repository\n",
    "\n",
    "        if not self.model_id:\n",
    "            raise ValueError(\n",
    "                f\"Could not find model id for inference server provided: {self.llm.inference_server_url}.\\\n",
    "                    Check to ensure the HF token you're using has access to the endpoint.\"\n",
    "            )\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return f\"{self.model_id.lower()}-style\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the model and some messages to pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You're a helpful assistant\"),\n",
    "    HumanMessage(\n",
    "        content=\"What happens when an unstoppable force meets an immovable object?\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "chat_model = HFInferenceEndpointChatWrapper(llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect which model and corresponding chat template is being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HuggingFaceH4/zephyr-7b-beta'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model.model_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect how the chat messages are formatted for the LLM call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|system|>\\nYou're a helpful assistant</s>\\n<|user|>\\nWhat happens when an unstoppable force meets an immovable object?</s>\\n<|assistant|>\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model._to_chat_prompt(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the popular idiom, when an unstoppable force meets an immovable object, there is a paradoxical situation where both forces seem to contradict each other's very nature. The force that is absolutely unstoppable should overcome the object that is completely immovable, but in this scenario, both forces are presented as equally powerful and unyielding. This paradox raises philosophical questions about the nature of force, objectivity, and the limits of logic. In reality, such a scenario is impossible, as it defies the laws of physics, and both forces cannot exist simultaneously in the same place and time.\n"
     ]
    }
   ],
   "source": [
    "res = chat_model.invoke(messages)\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Take it for a spin!\n",
    "\n",
    "Here we'll test out `Zephyr-7B-beta` as a zero-shot ReAct Agent. The example below is taken from [here](https://python.langchain.com/docs/modules/agents/agent_types/react#using-chat-models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.utilities import SerpAPIWrapper\n",
    "from langchain import hub\n",
    "from langchain.agents.format_scratchpad import format_log_to_str\n",
    "from langchain.tools.render import render_text_description\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents.output_parsers import (\n",
    "    ReActJsonSingleInputOutputParser,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the agent with a `react-json` style prompt and access to a search engine and calculator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup tools\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "\n",
    "# setup ReAct style prompt\n",
    "prompt = hub.pull(\"hwchase17/react-json\")\n",
    "prompt = prompt.partial(\n",
    "    tools=render_text_description(tools),\n",
    "    tool_names=\", \".join([t.name for t in tools]),\n",
    ")\n",
    "\n",
    "# define the agent\n",
    "chat_model_with_stop = chat_model.bind(stop=[\"\\nObservation\"])\n",
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"agent_scratchpad\": lambda x: format_log_to_str(x[\"intermediate_steps\"]),\n",
    "    }\n",
    "    | prompt\n",
    "    | chat_model_with_stop\n",
    "    | ReActJsonSingleInputOutputParser()\n",
    ")\n",
    "\n",
    "# instantiate AgentExecutor\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mQuestion: Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\n",
      "\n",
      "Thought: I need to use the Search tool to find out who Leo DiCaprio's current girlfriend is. Then, I can use the Calculator tool to raise her current age to the power of 0.43.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Search\",\n",
      "  \"action_input\": \"leo dicaprio girlfriend\"\n",
      "}\n",
      "```\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mLeonardo DiCaprio looked typically understated as he stepped out in London with his girlfriend Vittoria Ceretti and her family on Thursday.\u001b[0m\u001b[32;1m\u001b[1;3mNow, let's find out Vittoria Ceretti's current age.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Search\",\n",
      "  \"action_input\": \"vittoria ceretti age\"\n",
      "}\n",
      "```\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m25 years\u001b[0m\u001b[32;1m\u001b[1;3mNow, let's use the Calculator tool to raise Vittoria Ceretti's age to the power of 0.43.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Calculator\",\n",
      "  \"action_input\": \"25^0.43\"\n",
      "}\n",
      "```\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mAnswer: 3.991298452658078\u001b[0m\u001b[32;1m\u001b[1;3mFinal Answer: Leo DiCaprio's current girlfriend is Vittoria Ceretti, and when her age of 25 is raised to the power of 0.43, it equals approximately 3.9913.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': \"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\",\n",
       " 'output': \"Leo DiCaprio's current girlfriend is Vittoria Ceretti, and when her age of 25 is raised to the power of 0.43, it equals approximately 3.9913.\"}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wahoo! Our open-source 7b parameter Zephyr model was able to:\n",
    "\n",
    "1. Plan out a series of actions: `I need to use the Search tool to find out who Leo DiCaprio's current girlfriend is. Then, I can use the Calculator tool to raise her current age to the power of 0.43.`\n",
    "2. Then execute a search using the SerpAPI tool to find who Leo DiCaprio's current girlfriend is\n",
    "3. Execute another search to find her age\n",
    "4. And finally use a calculator tool to calculate her age raised to the power of 0.43\n",
    "\n",
    "I'm excited to see how far open-source LLM's can go as general purpose reasoning agents. Give it a try yourself!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
