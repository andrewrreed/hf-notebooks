{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying Mixtral on AWS Inferentia2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Spin up an EC2 instance\n",
    "\n",
    "Here we'll be using:\n",
    "- [Hugging Face Neuron Deep Learning AMI](https://aws.amazon.com/marketplace/pp/prodview-gr3e6yiscria2)\n",
    "- `ml.inf2.24xlarge` instance type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Convert `Mixtral-8x7B-Instruct-v0.1` to AWS Neuron with `optimum-neuron`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimium[neuronx] comes pre-installed, but upgrade to latest incase: \n",
    "# https://huggingface.co/docs/optimum-neuron/installation\n",
    "!pip install --upgrade-strategy eager optimum[neuronx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# login for gated repo\n",
    "from huggingface_hub import interpreter_login\n",
    "\n",
    "interpreter_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** \n",
    "1. The Mixtral implementation in `aws-neuron/transformers-neuronx` has [a requirement](https://github.com/aws-neuron/transformers-neuronx/blob/0623de20a3934f8d1b3cb73e1672138657134d7f/src/transformers_neuronx/mixtral/config.py#L57) on the value for `tp_degree`. \n",
    "- `tp_degree` is [auto-populated](https://github.com/huggingface/optimum-neuron/blob/7439a2d32808ce19ddece2d9a34d047af46dd1b1/optimum/neuron/modeling_decoder.py#L174) in `optimum-neuron` to be equal to `num_cores`.\n",
    "- For this reason, you can only compile the model with the `num_cores` set to 8 or 16 (32 is only for tranium instances)\n",
    "\n",
    "2. When compiling with `sequence_length==32k`, I noticed the following error:\n",
    "- > Estimated peak HBM usage (32.264610) exceeds 16GB. Neff won't be able to load on chip - Please open a support ticket at https://github.com/aws-neuron/aws-neuron-sdk/issues/new\n",
    "- Each decoder block will need to allocate a static KV cache that is proportional to batch_size * sequence_length, and it seems for Neuron devices that 32k will not actually fit.. I used 8k here, but it's possible the value could be higher (need to test this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from optimum.neuron import NeuronModelForCausalLM\n",
    "\n",
    "# model id you want to compile\n",
    "vanilla_model_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "# configs for compiling model\n",
    "compiler_args = {\"num_cores\": 8 , \"auto_cast_type\": \"bf16\"}\n",
    "input_shapes = {\n",
    "  \"sequence_length\": 8192, #32768, # max length to generate\n",
    "  \"batch_size\": 1 # batch size for the model\n",
    "  }\n",
    "\n",
    "llm = NeuronModelForCausalLM.from_pretrained(vanilla_model_id, export=True, **input_shapes, **compiler_args)\n",
    "tokenizer = AutoTokenizer.from_pretrained(vanilla_model_id)\n",
    "\n",
    "# Save locally or upload to the HuggingFace Hub\n",
    "save_directory = \"./mixtral-neuron-bs1-sl8k\"\n",
    "llm.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Push to Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: first remove safetensors checkpoint\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "shutil.rmtree(os.path.join(save_directory, \"checkpoint\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.push_to_hub(repository_id=\"andrewrreed/Mixtral-8x7B-Instruct-v0.1-neuron-bs1-sl8k\", save_directory=save_directory, use_auth_token=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deploy with TGI\n",
    "\n",
    "**Note:** Here we're only specifying the first 4 neuron devices corresponding to the `num_cores` above (each device has two cores)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "docker run -p 8080:80 \\\n",
    "       -v $(pwd)/data:/data \\\n",
    "       --device=/dev/neuron0 \\\n",
    "       --device=/dev/neuron1 \\\n",
    "       --device=/dev/neuron2 \\\n",
    "       --device=/dev/neuron3 \\\n",
    "       -e HF_TOKEN=${HF_TOKEN} \\\n",
    "       ghcr.io/huggingface/neuronx-tgi:latest \\\n",
    "       --model-id andrewrreed/Mixtral-8x7B-Instruct-v0.1-neuron-bs1-sl8k \\\n",
    "       --max-batch-size 1 \\\n",
    "       --max-input-length 8191 \\\n",
    "       --max-total-tokens 8192\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Query the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"generated_text\":\"\\n\\nAWS Inferentia2 is a new machine learning inference chip designed and built by Amazon Web Services (AWS). It is optimized for deep learning inference workloads and is designed to deliver high performance and low latency at a low cost.\\n\\nThere are several reasons why you might want to use AWS Inferentia2:\\n\\n1. High performance: AWS Inferentia2 is designed to deliver high performance for deep learning inference workloads. It can process up to 128 TOPS (tera operations per second) and has a memory bandwidth of up to 3,000 GB/s. This allows it to handle large, complex models and deliver fast, accurate results.\\n2. Low latency: AWS Inferentia2 is also designed to deliver low latency for inference workloads. It has a latency of less than 1 millisecond for many common inference tasks, which is important for applications that require real-time responses.\\n3. Low cost: AWS Inferentia2 is designed to be cost-effective, with a price that is significantly lower than other inference chips. This makes it an attractive option for organizations that want to use machine learning inference but are concerned about the cost.\\n4. Integration with AWS: AWS Inferentia2 is fully integrated with the AWS ecosystem, which makes it easy to use and manage. It can be used with AWS machine learning services, such as Amazon SageMaker, and can be deployed on AWS infrastructure, such as Amazon EC2 and AWS Lambda.\\n\\nOverall, AWS Inferentia2 is a powerful and cost-effective option for organizations that want to use machine learning inference. It is optimized for deep learning workloads and is fully integrated with the AWS ecosystem, making it easy to use and manage.</s>\"}"
     ]
    }
   ],
   "source": [
    "!curl 127.0.0.1:8080/generate \\\n",
    "    -X POST \\\n",
    "    -d '{\"inputs\":\"What is AWS inferentia2 and why would I want to use it?\",\"parameters\":{\"max_new_tokens\":1024}}' \\\n",
    "    -H 'Content-Type: application/json'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
