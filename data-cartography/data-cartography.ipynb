{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cartography for Multi-class Classification\n",
    "\n",
    "Goals:\n",
    "1. Subclass `Trainer` and overwrite to save out `ids`, `logits`, `gold_label` for each example after each training step.\n",
    "2. Use plotting utilities from Data Cartography repo to generate plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.36.2)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.16.1)\n",
      "Requirement already satisfied: evaluate in /opt/conda/lib/python3.10/site-packages (0.4.1)\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.26.1)\n",
      "Collecting peft\n",
      "  Downloading peft-0.7.1-py3-none-any.whl (168 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: ipywidgets in /opt/conda/lib/python3.10/site-packages (8.1.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /opt/conda/lib/python3.10/site-packages (from torch) (2.18.1)\n",
      "Requirement already satisfied: triton==2.1.0 in /opt/conda/lib/python3.10/site-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.7)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.9 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (4.0.9)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (5.14.1)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.9 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (3.0.9)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (8.20.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.42)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Installing collected packages: peft\n",
      "Successfully installed peft-0.7.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers torch datasets evaluate accelerate peft ipywidgets scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load, explore, and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"lmsys/toxic-chat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.rename_columns({\"user_input\": \"text\", \"toxicity\": \"label\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['conv_id', 'text', 'model_output', 'human_annotation', 'label', 'jailbreaking', 'openai_moderation'],\n",
       "        num_rows: 5082\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['conv_id', 'text', 'model_output', 'human_annotation', 'label', 'jailbreaking', 'openai_moderation'],\n",
       "        num_rows: 5083\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conv_id': 'e0c9b3e05414814485dbdcb9a29334d502e59803af9c26df03e9d1de5e7afe67',\n",
       " 'text': 'Masturbacja jest proces co oitrzebuje',\n",
       " 'model_output': 'Masturbacja to proces, który może pozytywnie wpłynąć na zdrowie psychiczne i fizyczne człowieka, ponieważ pomaga w relaksie, redukuje stres i pomaga w uśpieniu. Może też być używana jako dodatkowa form',\n",
       " 'human_annotation': True,\n",
       " 'label': 0,\n",
       " 'jailbreaking': 0.0,\n",
       " 'openai_moderation': \"[['sexual', 0.16727015376091003], ['hate', 0.0014814797323197126], ['violence', 7.014749280642718e-05], ['self-harm', 5.5694090406177565e-06], ['sexual/minors', 4.127006832277402e-06], ['violence/graphic', 5.00143471526826e-07], ['hate/threatening', 2.3769071155754773e-09]]\"}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's see if/how many examples will be truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"distilbert-base-uncased\"\n",
    "# model_id = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_seq_length(dataset, tokenizer):\n",
    "    \"\"\"Check to see how many samples are longer than max seq length\"\"\"\n",
    "\n",
    "    print(\"----Number samples longer than model's context length----\")\n",
    "\n",
    "    dataset = dataset.map(\n",
    "        lambda examples: {\"num_tokens\": len(tokenizer(examples[\"text\"])[\"input_ids\"])}\n",
    "    )\n",
    "\n",
    "    for split in [\"test\", \"train\"]:\n",
    "        df = dataset[\"train\"].to_pandas()\n",
    "        count = len(df.loc[df[\"num_tokens\"] >= tokenizer.model_max_length])\n",
    "\n",
    "        print(f\"{split.upper()}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Number samples longer than model's context length----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "617380b30c7a46cbb544f5f9e718fee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5083 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (721 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: 15\n",
      "TRAIN: 15\n"
     ]
    }
   ],
   "source": [
    "check_seq_length(dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First lets add a unique _numeric_ id to each example (required for tensors)\n",
    "def add_id(example, idx):\n",
    "    example[\"id\"] = idx\n",
    "    return example\n",
    "\n",
    "\n",
    "# Apply the function to each example in the dataset\n",
    "dataset = dataset.map(add_id, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52ceac7a9fd7484e836df5550d6320db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5082 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b964bfa9d7d54c66a43f3679d9bb7158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5083 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset.remove_columns(\n",
    "    [\n",
    "        \"conv_id\",\n",
    "        \"text\",\n",
    "        \"model_output\",\n",
    "        \"human_annotation\",\n",
    "        \"jailbreaking\",\n",
    "        \"openai_moderation\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'id', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 5082\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'id', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 5083\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    DataCollatorWithPadding,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=2)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from evaluate import load\n",
    "\n",
    "f1_metric = load(\"f1\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels)[\"f1\"]\n",
    "\n",
    "    return {\"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"toxicity_baseline\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='159' max='159' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [159/159 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.2474513202905655,\n",
       " 'eval_f1': 0.7290780141843971,\n",
       " 'eval_runtime': 12.4033,\n",
       " 'eval_samples_per_second': 409.81,\n",
       " 'eval_steps_per_second': 12.819,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(tokenized_dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.model.save_pretrained(\"toxicity_baseline_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mTrainingArguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moutput_dir\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moverwrite_output_dir\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdo_train\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdo_eval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdo_predict\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mevaluation_strategy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIntervalStrategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'no'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mprediction_loss_only\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mper_device_train_batch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mper_device_eval_batch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mper_gpu_train_batch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mper_gpu_eval_batch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0meval_accumulation_steps\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0meval_delay\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlearning_rate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5e-05\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mweight_decay\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0madam_beta1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0madam_beta2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.999\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0madam_epsilon\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-08\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnum_train_epochs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_steps\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlr_scheduler_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSchedulerType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'linear'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlr_scheduler_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mfactory\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mwarmup_ratio\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mwarmup_steps\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlog_level\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'passive'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlog_level_replica\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'warning'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlog_on_each_node\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlogging_dir\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlogging_strategy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIntervalStrategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'steps'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlogging_first_step\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlogging_steps\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlogging_nan_inf_filter\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msave_strategy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIntervalStrategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'steps'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msave_steps\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msave_total_limit\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msave_safetensors\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msave_on_each_node\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msave_only_model\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mno_cuda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0muse_cpu\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0muse_mps_device\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mseed\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdata_seed\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mjit_mode_eval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0muse_ipex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbf16\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mfp16\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mfp16_opt_level\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'O1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mhalf_precision_backend\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbf16_full_eval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mfp16_full_eval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtf32\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlocal_rank\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mddp_backend\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtpu_num_cores\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtpu_metrics_debug\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDebugOption\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdataloader_drop_last\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0meval_steps\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdataloader_num_workers\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpast_index\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrun_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdisable_tqdm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mremove_unused_columns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlabel_names\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mload_best_model_at_end\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmetric_for_best_model\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mgreater_is_better\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mignore_data_skip\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mfsdp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFSDPOption\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mfsdp_min_num_params\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mfsdp_config\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mfsdp_transformer_layer_cls_to_wrap\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdeepspeed\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlabel_smoothing_factor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moptim\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOptimizerNames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'adamw_torch'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moptim_args\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0madafactor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mgroup_by_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlength_column_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'length'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreport_to\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mddp_find_unused_parameters\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mddp_bucket_cap_mb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mddp_broadcast_buffers\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdataloader_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdataloader_persistent_workers\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mskip_memory_metrics\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0muse_legacy_prediction_loop\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpush_to_hub\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mhub_model_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mhub_strategy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHubStrategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'every_save'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mhub_token\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mhub_private_repo\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mhub_always_push\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mgradient_checkpointing\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mgradient_checkpointing_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minclude_inputs_for_metrics\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mfp16_backend\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpush_to_hub_model_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpush_to_hub_organization\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpush_to_hub_token\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmp_parameters\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mauto_find_batch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mfull_determinism\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtorchdynamo\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mray_scope\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'last'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mddp_timeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1800\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtorch_compile\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtorch_compile_backend\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtorch_compile_mode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdispatch_batches\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msplit_batches\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minclude_tokens_per_second\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minclude_num_input_tokens_seen\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mneftune_noise_alpha\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "TrainingArguments is the subset of the arguments we use in our example scripts **which relate to the training loop\n",
      "itself**.\n",
      "\n",
      "Using [`HfArgumentParser`] we can turn this class into\n",
      "[argparse](https://docs.python.org/3/library/argparse#module-argparse) arguments that can be specified on the\n",
      "command line.\n",
      "\n",
      "Parameters:\n",
      "    output_dir (`str`):\n",
      "        The output directory where the model predictions and checkpoints will be written.\n",
      "    overwrite_output_dir (`bool`, *optional*, defaults to `False`):\n",
      "        If `True`, overwrite the content of the output directory. Use this to continue training if `output_dir`\n",
      "        points to a checkpoint directory.\n",
      "    do_train (`bool`, *optional*, defaults to `False`):\n",
      "        Whether to run training or not. This argument is not directly used by [`Trainer`], it's intended to be used\n",
      "        by your training/evaluation scripts instead. See the [example\n",
      "        scripts](https://github.com/huggingface/transformers/tree/main/examples) for more details.\n",
      "    do_eval (`bool`, *optional*):\n",
      "        Whether to run evaluation on the validation set or not. Will be set to `True` if `evaluation_strategy` is\n",
      "        different from `\"no\"`. This argument is not directly used by [`Trainer`], it's intended to be used by your\n",
      "        training/evaluation scripts instead. See the [example\n",
      "        scripts](https://github.com/huggingface/transformers/tree/main/examples) for more details.\n",
      "    do_predict (`bool`, *optional*, defaults to `False`):\n",
      "        Whether to run predictions on the test set or not. This argument is not directly used by [`Trainer`], it's\n",
      "        intended to be used by your training/evaluation scripts instead. See the [example\n",
      "        scripts](https://github.com/huggingface/transformers/tree/main/examples) for more details.\n",
      "    evaluation_strategy (`str` or [`~trainer_utils.IntervalStrategy`], *optional*, defaults to `\"no\"`):\n",
      "        The evaluation strategy to adopt during training. Possible values are:\n",
      "\n",
      "            - `\"no\"`: No evaluation is done during training.\n",
      "            - `\"steps\"`: Evaluation is done (and logged) every `eval_steps`.\n",
      "            - `\"epoch\"`: Evaluation is done at the end of each epoch.\n",
      "\n",
      "    prediction_loss_only (`bool`, *optional*, defaults to `False`):\n",
      "        When performing evaluation and generating predictions, only returns the loss.\n",
      "    per_device_train_batch_size (`int`, *optional*, defaults to 8):\n",
      "        The batch size per GPU/XPU/TPU/MPS/NPU core/CPU for training.\n",
      "    per_device_eval_batch_size (`int`, *optional*, defaults to 8):\n",
      "        The batch size per GPU/XPU/TPU/MPS/NPU core/CPU for evaluation.\n",
      "    gradient_accumulation_steps (`int`, *optional*, defaults to 1):\n",
      "        Number of updates steps to accumulate the gradients for, before performing a backward/update pass.\n",
      "\n",
      "        <Tip warning={true}>\n",
      "\n",
      "        When using gradient accumulation, one step is counted as one step with backward pass. Therefore, logging,\n",
      "        evaluation, save will be conducted every `gradient_accumulation_steps * xxx_step` training examples.\n",
      "\n",
      "        </Tip>\n",
      "\n",
      "    eval_accumulation_steps (`int`, *optional*):\n",
      "        Number of predictions steps to accumulate the output tensors for, before moving the results to the CPU. If\n",
      "        left unset, the whole predictions are accumulated on GPU/NPU/TPU before being moved to the CPU (faster but\n",
      "        requires more memory).\n",
      "    eval_delay (`float`, *optional*):\n",
      "        Number of epochs or steps to wait for before the first evaluation can be performed, depending on the\n",
      "        evaluation_strategy.\n",
      "    learning_rate (`float`, *optional*, defaults to 5e-5):\n",
      "        The initial learning rate for [`AdamW`] optimizer.\n",
      "    weight_decay (`float`, *optional*, defaults to 0):\n",
      "        The weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights in [`AdamW`]\n",
      "        optimizer.\n",
      "    adam_beta1 (`float`, *optional*, defaults to 0.9):\n",
      "        The beta1 hyperparameter for the [`AdamW`] optimizer.\n",
      "    adam_beta2 (`float`, *optional*, defaults to 0.999):\n",
      "        The beta2 hyperparameter for the [`AdamW`] optimizer.\n",
      "    adam_epsilon (`float`, *optional*, defaults to 1e-8):\n",
      "        The epsilon hyperparameter for the [`AdamW`] optimizer.\n",
      "    max_grad_norm (`float`, *optional*, defaults to 1.0):\n",
      "        Maximum gradient norm (for gradient clipping).\n",
      "    num_train_epochs(`float`, *optional*, defaults to 3.0):\n",
      "        Total number of training epochs to perform (if not an integer, will perform the decimal part percents of\n",
      "        the last epoch before stopping training).\n",
      "    max_steps (`int`, *optional*, defaults to -1):\n",
      "        If set to a positive number, the total number of training steps to perform. Overrides `num_train_epochs`.\n",
      "        For a finite dataset, training is reiterated through the dataset (if all data is exhausted) until\n",
      "        `max_steps` is reached.\n",
      "    lr_scheduler_type (`str` or [`SchedulerType`], *optional*, defaults to `\"linear\"`):\n",
      "        The scheduler type to use. See the documentation of [`SchedulerType`] for all possible values.\n",
      "    lr_scheduler_kwargs ('dict', *optional*, defaults to {}):\n",
      "        The extra arguments for the lr_scheduler. See the documentation of each scheduler for possible values.\n",
      "    warmup_ratio (`float`, *optional*, defaults to 0.0):\n",
      "        Ratio of total training steps used for a linear warmup from 0 to `learning_rate`.\n",
      "    warmup_steps (`int`, *optional*, defaults to 0):\n",
      "        Number of steps used for a linear warmup from 0 to `learning_rate`. Overrides any effect of `warmup_ratio`.\n",
      "    log_level (`str`, *optional*, defaults to `passive`):\n",
      "        Logger log level to use on the main process. Possible choices are the log levels as strings: 'debug',\n",
      "        'info', 'warning', 'error' and 'critical', plus a 'passive' level which doesn't set anything and keeps the\n",
      "        current log level for the Transformers library (which will be `\"warning\"` by default).\n",
      "    log_level_replica (`str`, *optional*, defaults to `\"warning\"`):\n",
      "        Logger log level to use on replicas. Same choices as `log_level`\"\n",
      "    log_on_each_node (`bool`, *optional*, defaults to `True`):\n",
      "        In multinode distributed training, whether to log using `log_level` once per node, or only on the main\n",
      "        node.\n",
      "    logging_dir (`str`, *optional*):\n",
      "        [TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to\n",
      "        *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.\n",
      "    logging_strategy (`str` or [`~trainer_utils.IntervalStrategy`], *optional*, defaults to `\"steps\"`):\n",
      "        The logging strategy to adopt during training. Possible values are:\n",
      "\n",
      "            - `\"no\"`: No logging is done during training.\n",
      "            - `\"epoch\"`: Logging is done at the end of each epoch.\n",
      "            - `\"steps\"`: Logging is done every `logging_steps`.\n",
      "\n",
      "    logging_first_step (`bool`, *optional*, defaults to `False`):\n",
      "        Whether to log and evaluate the first `global_step` or not.\n",
      "    logging_steps (`int` or `float`, *optional*, defaults to 500):\n",
      "        Number of update steps between two logs if `logging_strategy=\"steps\"`. Should be an integer or a float in\n",
      "        range `[0,1)`. If smaller than 1, will be interpreted as ratio of total training steps.\n",
      "    logging_nan_inf_filter (`bool`, *optional*, defaults to `True`):\n",
      "        Whether to filter `nan` and `inf` losses for logging. If set to `True` the loss of every step that is `nan`\n",
      "        or `inf` is filtered and the average loss of the current logging window is taken instead.\n",
      "\n",
      "        <Tip>\n",
      "\n",
      "        `logging_nan_inf_filter` only influences the logging of loss values, it does not change the behavior the\n",
      "        gradient is computed or applied to the model.\n",
      "\n",
      "        </Tip>\n",
      "\n",
      "    save_strategy (`str` or [`~trainer_utils.IntervalStrategy`], *optional*, defaults to `\"steps\"`):\n",
      "        The checkpoint save strategy to adopt during training. Possible values are:\n",
      "\n",
      "            - `\"no\"`: No save is done during training.\n",
      "            - `\"epoch\"`: Save is done at the end of each epoch.\n",
      "            - `\"steps\"`: Save is done every `save_steps`.\n",
      "    save_steps (`int` or `float`, *optional*, defaults to 500):\n",
      "        Number of updates steps before two checkpoint saves if `save_strategy=\"steps\"`. Should be an integer or a\n",
      "        float in range `[0,1)`. If smaller than 1, will be interpreted as ratio of total training steps.\n",
      "    save_total_limit (`int`, *optional*):\n",
      "        If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in\n",
      "        `output_dir`. When `load_best_model_at_end` is enabled, the \"best\" checkpoint according to\n",
      "        `metric_for_best_model` will always be retained in addition to the most recent ones. For example, for\n",
      "        `save_total_limit=5` and `load_best_model_at_end`, the four last checkpoints will always be retained\n",
      "        alongside the best model. When `save_total_limit=1` and `load_best_model_at_end`, it is possible that two\n",
      "        checkpoints are saved: the last one and the best one (if they are different).\n",
      "    save_safetensors (`bool`, *optional*, defaults to `True`):\n",
      "        Use [safetensors](https://huggingface.co/docs/safetensors) saving and loading for state dicts instead of\n",
      "        default `torch.load` and `torch.save`.\n",
      "    save_on_each_node (`bool`, *optional*, defaults to `False`):\n",
      "        When doing multi-node distributed training, whether to save models and checkpoints on each node, or only on\n",
      "        the main one.\n",
      "\n",
      "        This should not be activated when the different nodes use the same storage as the files will be saved with\n",
      "        the same names for each node.\n",
      "    save_only_model (`bool`, *optional*, defaults to `False`):\n",
      "        When checkpointing, whether to only save the model, or also the optimizer, scheduler & rng state.\n",
      "        Note that when this is true, you won't be able to resume training from checkpoint.\n",
      "        This enables you to save storage by not storing the optimizer, scheduler & rng state.\n",
      "        You can only load the model using `from_pretrained` with this option set to `True`.\n",
      "    use_cpu (`bool`, *optional*, defaults to `False`):\n",
      "        Whether or not to use cpu. If set to False, we will use cuda or mps device if available.\n",
      "    seed (`int`, *optional*, defaults to 42):\n",
      "        Random seed that will be set at the beginning of training. To ensure reproducibility across runs, use the\n",
      "        [`~Trainer.model_init`] function to instantiate the model if it has some randomly initialized parameters.\n",
      "    data_seed (`int`, *optional*):\n",
      "        Random seed to be used with data samplers. If not set, random generators for data sampling will use the\n",
      "        same seed as `seed`. This can be used to ensure reproducibility of data sampling, independent of the model\n",
      "        seed.\n",
      "    jit_mode_eval (`bool`, *optional*, defaults to `False`):\n",
      "        Whether or not to use PyTorch jit trace for inference.\n",
      "    use_ipex (`bool`, *optional*, defaults to `False`):\n",
      "        Use Intel extension for PyTorch when it is available. [IPEX\n",
      "        installation](https://github.com/intel/intel-extension-for-pytorch).\n",
      "    bf16 (`bool`, *optional*, defaults to `False`):\n",
      "        Whether to use bf16 16-bit (mixed) precision training instead of 32-bit training. Requires Ampere or higher\n",
      "        NVIDIA architecture or using CPU (use_cpu) or Ascend NPU. This is an experimental API and it may change.\n",
      "    fp16 (`bool`, *optional*, defaults to `False`):\n",
      "        Whether to use fp16 16-bit (mixed) precision training instead of 32-bit training.\n",
      "    fp16_opt_level (`str`, *optional*, defaults to 'O1'):\n",
      "        For `fp16` training, Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']. See details on\n",
      "        the [Apex documentation](https://nvidia.github.io/apex/amp).\n",
      "    fp16_backend (`str`, *optional*, defaults to `\"auto\"`):\n",
      "        This argument is deprecated. Use `half_precision_backend` instead.\n",
      "    half_precision_backend (`str`, *optional*, defaults to `\"auto\"`):\n",
      "        The backend to use for mixed precision training. Must be one of `\"auto\", \"apex\", \"cpu_amp\"`. `\"auto\"` will\n",
      "        use CPU/CUDA AMP or APEX depending on the PyTorch version detected, while the other choices will force the\n",
      "        requested backend.\n",
      "    bf16_full_eval (`bool`, *optional*, defaults to `False`):\n",
      "        Whether to use full bfloat16 evaluation instead of 32-bit. This will be faster and save memory but can harm\n",
      "        metric values. This is an experimental API and it may change.\n",
      "    fp16_full_eval (`bool`, *optional*, defaults to `False`):\n",
      "        Whether to use full float16 evaluation instead of 32-bit. This will be faster and save memory but can harm\n",
      "        metric values.\n",
      "    tf32 (`bool`, *optional*):\n",
      "        Whether to enable the TF32 mode, available in Ampere and newer GPU architectures. The default value depends\n",
      "        on PyTorch's version default of `torch.backends.cuda.matmul.allow_tf32`. For more details please refer to\n",
      "        the [TF32](https://huggingface.co/docs/transformers/performance#tf32) documentation. This is an\n",
      "        experimental API and it may change.\n",
      "    local_rank (`int`, *optional*, defaults to -1):\n",
      "        Rank of the process during distributed training.\n",
      "    ddp_backend (`str`, *optional*):\n",
      "        The backend to use for distributed training. Must be one of `\"nccl\"`, `\"mpi\"`, `\"ccl\"`, `\"gloo\"`, `\"hccl\"`.\n",
      "    tpu_num_cores (`int`, *optional*):\n",
      "        When training on TPU, the number of TPU cores (automatically passed by launcher script).\n",
      "    dataloader_drop_last (`bool`, *optional*, defaults to `False`):\n",
      "        Whether to drop the last incomplete batch (if the length of the dataset is not divisible by the batch size)\n",
      "        or not.\n",
      "    eval_steps (`int` or `float`, *optional*):\n",
      "        Number of update steps between two evaluations if `evaluation_strategy=\"steps\"`. Will default to the same\n",
      "        value as `logging_steps` if not set. Should be an integer or a float in range `[0,1)`. If smaller than 1,\n",
      "        will be interpreted as ratio of total training steps.\n",
      "    dataloader_num_workers (`int`, *optional*, defaults to 0):\n",
      "        Number of subprocesses to use for data loading (PyTorch only). 0 means that the data will be loaded in the\n",
      "        main process.\n",
      "    past_index (`int`, *optional*, defaults to -1):\n",
      "        Some models like [TransformerXL](../model_doc/transformerxl) or [XLNet](../model_doc/xlnet) can make use of\n",
      "        the past hidden states for their predictions. If this argument is set to a positive int, the `Trainer` will\n",
      "        use the corresponding output (usually index 2) as the past state and feed it to the model at the next\n",
      "        training step under the keyword argument `mems`.\n",
      "    run_name (`str`, *optional*):\n",
      "        A descriptor for the run. Typically used for [wandb](https://www.wandb.com/) and\n",
      "        [mlflow](https://www.mlflow.org/) logging.\n",
      "    disable_tqdm (`bool`, *optional*):\n",
      "        Whether or not to disable the tqdm progress bars and table of metrics produced by\n",
      "        [`~notebook.NotebookTrainingTracker`] in Jupyter Notebooks. Will default to `True` if the logging level is\n",
      "        set to warn or lower (default), `False` otherwise.\n",
      "    remove_unused_columns (`bool`, *optional*, defaults to `True`):\n",
      "        Whether or not to automatically remove the columns unused by the model forward method.\n",
      "\n",
      "        (Note that this behavior is not implemented for [`TFTrainer`] yet.)\n",
      "    label_names (`List[str]`, *optional*):\n",
      "        The list of keys in your dictionary of inputs that correspond to the labels.\n",
      "\n",
      "        Will eventually default to the list of argument names accepted by the model that contain the word \"label\",\n",
      "        except if the model used is one of the `XxxForQuestionAnswering` in which case it will also include the\n",
      "        `[\"start_positions\", \"end_positions\"]` keys.\n",
      "    load_best_model_at_end (`bool`, *optional*, defaults to `False`):\n",
      "        Whether or not to load the best model found during training at the end of training. When this option is\n",
      "        enabled, the best checkpoint will always be saved. See\n",
      "        [`save_total_limit`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.save_total_limit)\n",
      "        for more.\n",
      "\n",
      "        <Tip>\n",
      "\n",
      "        When set to `True`, the parameters `save_strategy` needs to be the same as `evaluation_strategy`, and in\n",
      "        the case it is \"steps\", `save_steps` must be a round multiple of `eval_steps`.\n",
      "\n",
      "        </Tip>\n",
      "\n",
      "    metric_for_best_model (`str`, *optional*):\n",
      "        Use in conjunction with `load_best_model_at_end` to specify the metric to use to compare two different\n",
      "        models. Must be the name of a metric returned by the evaluation with or without the prefix `\"eval_\"`. Will\n",
      "        default to `\"loss\"` if unspecified and `load_best_model_at_end=True` (to use the evaluation loss).\n",
      "\n",
      "        If you set this value, `greater_is_better` will default to `True`. Don't forget to set it to `False` if\n",
      "        your metric is better when lower.\n",
      "    greater_is_better (`bool`, *optional*):\n",
      "        Use in conjunction with `load_best_model_at_end` and `metric_for_best_model` to specify if better models\n",
      "        should have a greater metric or not. Will default to:\n",
      "\n",
      "        - `True` if `metric_for_best_model` is set to a value that isn't `\"loss\"` or `\"eval_loss\"`.\n",
      "        - `False` if `metric_for_best_model` is not set, or set to `\"loss\"` or `\"eval_loss\"`.\n",
      "    ignore_data_skip (`bool`, *optional*, defaults to `False`):\n",
      "        When resuming training, whether or not to skip the epochs and batches to get the data loading at the same\n",
      "        stage as in the previous training. If set to `True`, the training will begin faster (as that skipping step\n",
      "        can take a long time) but will not yield the same results as the interrupted training would have.\n",
      "    fsdp (`bool`, `str` or list of [`~trainer_utils.FSDPOption`], *optional*, defaults to `''`):\n",
      "        Use PyTorch Distributed Parallel Training (in distributed training only).\n",
      "\n",
      "        A list of options along the following:\n",
      "\n",
      "        - `\"full_shard\"`: Shard parameters, gradients and optimizer states.\n",
      "        - `\"shard_grad_op\"`: Shard optimizer states and gradients.\n",
      "        - `\"hybrid_shard\"`: Apply `FULL_SHARD` within a node, and replicate parameters across nodes.\n",
      "        - `\"hybrid_shard_zero2\"`: Apply `SHARD_GRAD_OP` within a node, and replicate parameters across nodes.\n",
      "        - `\"offload\"`: Offload parameters and gradients to CPUs (only compatible with `\"full_shard\"` and\n",
      "          `\"shard_grad_op\"`).\n",
      "        - `\"auto_wrap\"`: Automatically recursively wrap layers with FSDP using `default_auto_wrap_policy`.\n",
      "    fsdp_config (`str` or `dict`, *optional*):\n",
      "        Config to be used with fsdp (Pytorch Distributed Parallel Training). The value is either a location of\n",
      "        fsdp json config file (e.g., `fsdp_config.json`) or an already loaded json file as `dict`.\n",
      "\n",
      "        A List of config and its options:\n",
      "            - min_num_params (`int`, *optional*, defaults to `0`):\n",
      "                FSDP's minimum number of parameters for Default Auto Wrapping. (useful only when `fsdp` field is\n",
      "                passed).\n",
      "            - transformer_layer_cls_to_wrap (`List[str]`, *optional*):\n",
      "                List of transformer layer class names (case-sensitive) to wrap, e.g, `BertLayer`, `GPTJBlock`,\n",
      "                `T5Block` .... (useful only when `fsdp` flag is passed).\n",
      "            - backward_prefetch (`str`, *optional*)\n",
      "                FSDP's backward prefetch mode. Controls when to prefetch next set of parameters (useful only when\n",
      "                `fsdp` field is passed).\n",
      "\n",
      "                A list of options along the following:\n",
      "\n",
      "                - `\"backward_pre\"` : Prefetches the next set of parameters before the current set of parameter's\n",
      "                  gradient\n",
      "                    computation.\n",
      "                - `\"backward_post\"` : This prefetches the next set of parameters after the current set of\n",
      "                  parameter’s\n",
      "                    gradient computation.\n",
      "            - forward_prefetch (`bool`, *optional*, defaults to `False`)\n",
      "                FSDP's forward prefetch mode (useful only when `fsdp` field is passed).\n",
      "                 If `\"True\"`, then FSDP explicitly prefetches the next upcoming all-gather while executing in the\n",
      "                 forward pass.\n",
      "            - limit_all_gathers (`bool`, *optional*, defaults to `False`)\n",
      "                FSDP's limit_all_gathers (useful only when `fsdp` field is passed).\n",
      "                 If `\"True\"`, FSDP explicitly synchronizes the CPU thread to prevent too many in-flight\n",
      "                 all-gathers.\n",
      "            - use_orig_params (`bool`, *optional*, defaults to `True`)\n",
      "                If `\"True\"`, allows non-uniform `requires_grad` during init, which means support for interspersed\n",
      "                frozen and trainable paramteres. Useful in cases such as parameter-efficient fine-tuning. Please\n",
      "                refer this\n",
      "                [blog](https://dev-discuss.pytorch.org/t/rethinking-pytorch-fully-sharded-data-parallel-fsdp-from-first-principles/1019\n",
      "            - sync_module_states (`bool`, *optional*, defaults to `True`)\n",
      "                If `\"True\"`, each individually wrapped FSDP unit will broadcast module parameters from rank 0 to\n",
      "                ensure they are the same across all ranks after initialization\n",
      "            - activation_checkpointing (`bool`, *optional*, defaults to `False`):\n",
      "                If `\"True\"`, activation checkpointing is a technique to reduce memory usage by clearing activations of\n",
      "                certain layers and recomputing them during a backward pass. Effectively, this trades extra\n",
      "                computation time for reduced memory usage.\n",
      "            - xla (`bool`, *optional*, defaults to `False`):\n",
      "                Whether to use PyTorch/XLA Fully Sharded Data Parallel Training. This is an experimental feature\n",
      "                and its API may evolve in the future.\n",
      "            - xla_fsdp_settings (`dict`, *optional*)\n",
      "                The value is a dictionary which stores the XLA FSDP wrapping parameters.\n",
      "\n",
      "                For a complete list of options, please see [here](\n",
      "                https://github.com/pytorch/xla/blob/master/torch_xla/distributed/fsdp/xla_fully_sharded_data_parallel.py).\n",
      "            - xla_fsdp_grad_ckpt (`bool`, *optional*, defaults to `False`):\n",
      "                Will use gradient checkpointing over each nested XLA FSDP wrapped layer. This setting can only be\n",
      "                used when the xla flag is set to true, and an auto wrapping policy is specified through\n",
      "                fsdp_min_num_params or fsdp_transformer_layer_cls_to_wrap.\n",
      "\n",
      "    deepspeed (`str` or `dict`, *optional*):\n",
      "        Use [Deepspeed](https://github.com/microsoft/deepspeed). This is an experimental feature and its API may\n",
      "        evolve in the future. The value is either the location of DeepSpeed json config file (e.g.,\n",
      "        `ds_config.json`) or an already loaded json file as a `dict`\"\n",
      "    label_smoothing_factor (`float`, *optional*, defaults to 0.0):\n",
      "        The label smoothing factor to use. Zero means no label smoothing, otherwise the underlying onehot-encoded\n",
      "        labels are changed from 0s and 1s to `label_smoothing_factor/num_labels` and `1 - label_smoothing_factor +\n",
      "        label_smoothing_factor/num_labels` respectively.\n",
      "    debug (`str` or list of [`~debug_utils.DebugOption`], *optional*, defaults to `\"\"`):\n",
      "        Enable one or more debug features. This is an experimental feature.\n",
      "\n",
      "        Possible options are:\n",
      "\n",
      "        - `\"underflow_overflow\"`: detects overflow in model's input/outputs and reports the last frames that led to\n",
      "          the event\n",
      "        - `\"tpu_metrics_debug\"`: print debug metrics on TPU\n",
      "\n",
      "        The options should be separated by whitespaces.\n",
      "    optim (`str` or [`training_args.OptimizerNames`], *optional*, defaults to `\"adamw_torch\"`):\n",
      "        The optimizer to use: adamw_hf, adamw_torch, adamw_torch_fused, adamw_apex_fused, adamw_anyprecision or\n",
      "        adafactor.\n",
      "    optim_args (`str`, *optional*):\n",
      "        Optional arguments that are supplied to AnyPrecisionAdamW.\n",
      "    group_by_length (`bool`, *optional*, defaults to `False`):\n",
      "        Whether or not to group together samples of roughly the same length in the training dataset (to minimize\n",
      "        padding applied and be more efficient). Only useful if applying dynamic padding.\n",
      "    length_column_name (`str`, *optional*, defaults to `\"length\"`):\n",
      "        Column name for precomputed lengths. If the column exists, grouping by length will use these values rather\n",
      "        than computing them on train startup. Ignored unless `group_by_length` is `True` and the dataset is an\n",
      "        instance of `Dataset`.\n",
      "    report_to (`str` or `List[str]`, *optional*, defaults to `\"all\"`):\n",
      "        The list of integrations to report the results and logs to. Supported platforms are `\"azure_ml\"`,\n",
      "        `\"clearml\"`, `\"codecarbon\"`, `\"comet_ml\"`, `\"dagshub\"`, `\"dvclive\"`, `\"flyte\"`, `\"mlflow\"`, `\"neptune\"`,\n",
      "        `\"tensorboard\"`, and `\"wandb\"`. Use `\"all\"` to report to all integrations installed, `\"none\"` for no\n",
      "        integrations.\n",
      "    ddp_find_unused_parameters (`bool`, *optional*):\n",
      "        When using distributed training, the value of the flag `find_unused_parameters` passed to\n",
      "        `DistributedDataParallel`. Will default to `False` if gradient checkpointing is used, `True` otherwise.\n",
      "    ddp_bucket_cap_mb (`int`, *optional*):\n",
      "        When using distributed training, the value of the flag `bucket_cap_mb` passed to `DistributedDataParallel`.\n",
      "    ddp_broadcast_buffers (`bool`, *optional*):\n",
      "        When using distributed training, the value of the flag `broadcast_buffers` passed to\n",
      "        `DistributedDataParallel`. Will default to `False` if gradient checkpointing is used, `True` otherwise.\n",
      "    dataloader_pin_memory (`bool`, *optional*, defaults to `True`):\n",
      "        Whether you want to pin memory in data loaders or not. Will default to `True`.\n",
      "    dataloader_persistent_workers (`bool`, *optional*, defaults to `False`):\n",
      "        If True, the data loader will not shut down the worker processes after a dataset has been consumed once.\n",
      "        This allows to maintain the workers Dataset instances alive. Can potentially speed up training, but will\n",
      "        increase RAM usage. Will default to `False`.\n",
      "    skip_memory_metrics (`bool`, *optional*, defaults to `True`):\n",
      "        Whether to skip adding of memory profiler reports to metrics. This is skipped by default because it slows\n",
      "        down the training and evaluation speed.\n",
      "    push_to_hub (`bool`, *optional*, defaults to `False`):\n",
      "        Whether or not to push the model to the Hub every time the model is saved. If this is activated,\n",
      "        `output_dir` will begin a git directory synced with the repo (determined by `hub_model_id`) and the content\n",
      "        will be pushed each time a save is triggered (depending on your `save_strategy`). Calling\n",
      "        [`~Trainer.save_model`] will also trigger a push.\n",
      "\n",
      "        <Tip warning={true}>\n",
      "\n",
      "        If `output_dir` exists, it needs to be a local clone of the repository to which the [`Trainer`] will be\n",
      "        pushed.\n",
      "\n",
      "        </Tip>\n",
      "\n",
      "    resume_from_checkpoint (`str`, *optional*):\n",
      "        The path to a folder with a valid checkpoint for your model. This argument is not directly used by\n",
      "        [`Trainer`], it's intended to be used by your training/evaluation scripts instead. See the [example\n",
      "        scripts](https://github.com/huggingface/transformers/tree/main/examples) for more details.\n",
      "    hub_model_id (`str`, *optional*):\n",
      "        The name of the repository to keep in sync with the local *output_dir*. It can be a simple model ID in\n",
      "        which case the model will be pushed in your namespace. Otherwise it should be the whole repository name,\n",
      "        for instance `\"user_name/model\"`, which allows you to push to an organization you are a member of with\n",
      "        `\"organization_name/model\"`. Will default to `user_name/output_dir_name` with *output_dir_name* being the\n",
      "        name of `output_dir`.\n",
      "\n",
      "        Will default to the name of `output_dir`.\n",
      "    hub_strategy (`str` or [`~trainer_utils.HubStrategy`], *optional*, defaults to `\"every_save\"`):\n",
      "        Defines the scope of what is pushed to the Hub and when. Possible values are:\n",
      "\n",
      "        - `\"end\"`: push the model, its configuration, the tokenizer (if passed along to the [`Trainer`]) and a\n",
      "          draft of a model card when the [`~Trainer.save_model`] method is called.\n",
      "        - `\"every_save\"`: push the model, its configuration, the tokenizer (if passed along to the [`Trainer`]) and\n",
      "          a draft of a model card each time there is a model save. The pushes are asynchronous to not block\n",
      "          training, and in case the save are very frequent, a new push is only attempted if the previous one is\n",
      "          finished. A last push is made with the final model at the end of training.\n",
      "        - `\"checkpoint\"`: like `\"every_save\"` but the latest checkpoint is also pushed in a subfolder named\n",
      "          last-checkpoint, allowing you to resume training easily with\n",
      "          `trainer.train(resume_from_checkpoint=\"last-checkpoint\")`.\n",
      "        - `\"all_checkpoints\"`: like `\"checkpoint\"` but all checkpoints are pushed like they appear in the output\n",
      "          folder (so you will get one checkpoint folder per folder in your final repository)\n",
      "\n",
      "    hub_token (`str`, *optional*):\n",
      "        The token to use to push the model to the Hub. Will default to the token in the cache folder obtained with\n",
      "        `huggingface-cli login`.\n",
      "    hub_private_repo (`bool`, *optional*, defaults to `False`):\n",
      "        If True, the Hub repo will be set to private.\n",
      "    hub_always_push (`bool`, *optional*, defaults to `False`):\n",
      "        Unless this is `True`, the `Trainer` will skip pushing a checkpoint when the previous push is not finished.\n",
      "    gradient_checkpointing (`bool`, *optional*, defaults to `False`):\n",
      "        If True, use gradient checkpointing to save memory at the expense of slower backward pass.\n",
      "    gradient_checkpointing_kwargs (`dict`, *optional*, defaults to `None`):\n",
      "        Key word arguments to be passed to the `gradient_checkpointing_enable` method.\n",
      "    include_inputs_for_metrics (`bool`, *optional*, defaults to `False`):\n",
      "        Whether or not the inputs will be passed to the `compute_metrics` function. This is intended for metrics\n",
      "        that need inputs, predictions and references for scoring calculation in Metric class.\n",
      "    auto_find_batch_size (`bool`, *optional*, defaults to `False`)\n",
      "        Whether to find a batch size that will fit into memory automatically through exponential decay, avoiding\n",
      "        CUDA Out-of-Memory errors. Requires accelerate to be installed (`pip install accelerate`)\n",
      "    full_determinism (`bool`, *optional*, defaults to `False`)\n",
      "        If `True`, [`enable_full_determinism`] is called instead of [`set_seed`] to ensure reproducible results in\n",
      "        distributed training. Important: this will negatively impact the performance, so only use it for debugging.\n",
      "    torchdynamo (`str`, *optional*):\n",
      "        If set, the backend compiler for TorchDynamo. Possible choices are `\"eager\"`, `\"aot_eager\"`, `\"inductor\"`,\n",
      "        `\"nvfuser\"`, `\"aot_nvfuser\"`, `\"aot_cudagraphs\"`, `\"ofi\"`, `\"fx2trt\"`, `\"onnxrt\"` and `\"ipex\"`.\n",
      "    ray_scope (`str`, *optional*, defaults to `\"last\"`):\n",
      "        The scope to use when doing hyperparameter search with Ray. By default, `\"last\"` will be used. Ray will\n",
      "        then use the last checkpoint of all trials, compare those, and select the best one. However, other options\n",
      "        are also available. See the [Ray documentation](\n",
      "        https://docs.ray.io/en/latest/tune/api_docs/analysis.html#ray.tune.ExperimentAnalysis.get_best_trial) for\n",
      "        more options.\n",
      "    ddp_timeout (`int`, *optional*, defaults to 1800):\n",
      "        The timeout for `torch.distributed.init_process_group` calls, used to avoid GPU socket timeouts when\n",
      "        performing slow operations in distributed runnings. Please refer the [PyTorch documentation]\n",
      "        (https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group) for more\n",
      "        information.\n",
      "    use_mps_device (`bool`, *optional*, defaults to `False`):\n",
      "        This argument is deprecated.`mps` device will be used if it is available similar to `cuda` device.\n",
      "    torch_compile (`bool`, *optional*, defaults to `False`):\n",
      "        Whether or not to compile the model using PyTorch 2.0\n",
      "        [`torch.compile`](https://pytorch.org/get-started/pytorch-2.0/).\n",
      "\n",
      "        This will use the best defaults for the [`torch.compile`\n",
      "        API](https://pytorch.org/docs/stable/generated/torch.compile.html?highlight=torch+compile#torch.compile).\n",
      "        You can customize the defaults with the argument `torch_compile_backend` and `torch_compile_mode` but we\n",
      "        don't guarantee any of them will work as the support is progressively rolled in in PyTorch.\n",
      "\n",
      "        This flag and the whole compile API is experimental and subject to change in future releases.\n",
      "    torch_compile_backend (`str`, *optional*):\n",
      "        The backend to use in `torch.compile`. If set to any value, `torch_compile` will be set to `True`.\n",
      "\n",
      "        Refer to the PyTorch doc for possible values and note that they may change across PyTorch versions.\n",
      "\n",
      "        This flag is experimental and subject to change in future releases.\n",
      "    torch_compile_mode (`str`, *optional*):\n",
      "        The mode to use in `torch.compile`. If set to any value, `torch_compile` will be set to `True`.\n",
      "\n",
      "        Refer to the PyTorch doc for possible values and note that they may change across PyTorch versions.\n",
      "\n",
      "        This flag is experimental and subject to change in future releases.\n",
      "    split_batches (`bool`, *optional*):\n",
      "        Whether or not the accelerator should split the batches yielded by the dataloaders across the devices\n",
      "        during distributed training. If\n",
      "\n",
      "        set to `True`, the actual batch size used will be the same on any kind of distributed processes, but it\n",
      "        must be a\n",
      "\n",
      "        round multiple of the number of processes you are using (such as GPUs).\n",
      "    include_tokens_per_second (`bool`, *optional*):\n",
      "        Whether or not to compute the number of tokens per second per device for training speed metrics.\n",
      "\n",
      "        This will iterate over the entire training dataloader once beforehand,\n",
      "\n",
      "        and will slow down the entire process.\n",
      "\n",
      "    include_num_input_tokens_seen (`bool`, *optional*):\n",
      "        Whether or not to track the number of input tokens seen throughout training.\n",
      "\n",
      "        May be slower in distributed training as gather operations must be called.\n",
      "\n",
      "    neftune_noise_alpha (`Optional[float]`):\n",
      "        If not `None`, this will activate NEFTune noise embeddings. This can drastically improve model performance\n",
      "        for instruction fine-tuning. Check out the [original paper](https://arxiv.org/abs/2310.05914) and the\n",
      "        [original code](https://github.com/neelsjain/NEFTune). Support transformers `PreTrainedModel` and also\n",
      "        `PeftModel` from peft.\n",
      "\u001b[0;31mFile:\u001b[0m           /opt/conda/lib/python3.10/site-packages/transformers/training_args.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "?TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup `Trainer` for data cartography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdDataCollatorWithPadding(DataCollatorWithPadding):\n",
    "    \"\"\"Data collator that removes \"id\" col from features for padding, then adds it back\"\"\"\n",
    "\n",
    "    def __call__(self, features):\n",
    "        # remove \"id\" col from features for padding\n",
    "        if \"id\" in features[0]:\n",
    "            ids = [feature.pop(\"id\") for feature in features]\n",
    "\n",
    "        # use parent class method for padding\n",
    "        batch = super().__call__(features)\n",
    "\n",
    "        # add \"id\" col back to features\n",
    "        batch[\"id\"] = ids\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_data_collator = IdDataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from peft import PeftModel\n",
    "from transformers.modeling_utils import unwrap_model\n",
    "from transformers.models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n",
    "from transformers.utils import is_peft_available\n",
    "\n",
    "\n",
    "def _is_peft_model(model):\n",
    "    return is_peft_available() and isinstance(model, PeftModel)\n",
    "\n",
    "\n",
    "class DataCartographyTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        \"\"\"\n",
    "        How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
    "\n",
    "        Subclass and override for custom behavior.\n",
    "        \"\"\"\n",
    "        if self.label_smoother is not None and \"labels\" in inputs:\n",
    "            labels = inputs.pop(\"labels\")\n",
    "        else:\n",
    "            labels = None\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            labels=inputs[\"labels\"],\n",
    "        )\n",
    "\n",
    "        # Save data cartography outputs\n",
    "        if model.training:\n",
    "            if \"id\" in inputs:\n",
    "                ids_out = inputs[\"id\"]\n",
    "\n",
    "            logits_out = outputs[\"logits\"].detach().cpu().tolist()\n",
    "            labels_out = inputs[\"labels\"].detach().cpu().tolist()\n",
    "\n",
    "            current_epoch = int(self.state.epoch)\n",
    "\n",
    "            directory = Path(self.args.output_dir) / \"training_dynamics\"\n",
    "            os.makedirs(directory, exist_ok=True)\n",
    "            filename = directory / f\"dynamics_epoch_{current_epoch}.jsonl\"\n",
    "\n",
    "            with open(filename, \"a\") as f:\n",
    "                for id, logit, label in zip(ids_out, logits_out, labels_out):\n",
    "                    data_dict = {\n",
    "                        \"guid\": id,\n",
    "                        f\"logits_epoch_{current_epoch}\": logit,\n",
    "                        \"gold\": label,\n",
    "                    }\n",
    "                    json_line = json.dumps(data_dict)\n",
    "                    f.write(json_line + \"\\n\")\n",
    "\n",
    "        # Save past state if it exists\n",
    "        # TODO: this needs to be fixed and made cleaner later.\n",
    "        if self.args.past_index >= 0:\n",
    "            self._past = outputs[self.args.past_index]\n",
    "\n",
    "        if labels is not None:\n",
    "            unwrapped_model = unwrap_model(model)\n",
    "            if _is_peft_model(unwrapped_model):\n",
    "                model_name = unwrapped_model.base_model.model._get_name()\n",
    "            else:\n",
    "                model_name = unwrapped_model._get_name()\n",
    "            if model_name in MODEL_FOR_CAUSAL_LM_MAPPING_NAMES.values():\n",
    "                loss = self.label_smoother(outputs, labels, shift_labels=True)\n",
    "            else:\n",
    "                loss = self.label_smoother(outputs, labels)\n",
    "        else:\n",
    "            if isinstance(outputs, dict) and \"loss\" not in outputs:\n",
    "                raise ValueError(\n",
    "                    \"The model did not return a loss from the inputs, only the following keys: \"\n",
    "                    f\"{','.join(outputs.keys())}. For reference, the inputs it received are {','.join(inputs.keys())}.\"\n",
    "                )\n",
    "            # We don't use .loss here since the model may return tuples instead of ModelOutput.\n",
    "            loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.337900</td>\n",
       "      <td>0.651507</td>\n",
       "      <td>0.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.337500</td>\n",
       "      <td>0.660949</td>\n",
       "      <td>0.626263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.313100</td>\n",
       "      <td>0.662676</td>\n",
       "      <td>0.612245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory TESTING/checkpoint-4 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory TESTING/checkpoint-8 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory TESTING/checkpoint-12 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12, training_loss=0.3294784526030223, metrics={'train_runtime': 19.7297, 'train_samples_per_second': 15.206, 'train_steps_per_second': 0.608, 'total_flos': 28461862483104.0, 'train_loss': 0.3294784526030223, 'epoch': 3.0})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"TESTING\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    push_to_hub=False,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "trainer = DataCartographyTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"].select(range(100)),\n",
    "    eval_dataset=tokenized_dataset[\"test\"].select(range(100)),\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=id_data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
