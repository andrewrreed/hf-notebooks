{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quering Chat Models from HF Inference API & Endpoints\n",
    "\n",
    "The purpose of this notebook is to provide simple demonstrations for working with chat models via the HF Inference API and Inference Endpoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade -q huggingface-hub transformers jinja2 openai ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from huggingface_hub import InferenceClient, interpreter_login, get_token\n",
    "\n",
    "interpreter_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate an `InferenceClient`\n",
    "\n",
    "See [the docs](https://huggingface.co/docs/huggingface_hub/package_reference/inference_client) for details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we can optionally specify a model name or Inference Endpoint URL here or at the time of call the model.\n",
    "client = InferenceClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text Using a Chat Template\n",
    "\n",
    "Chat models are trained with different formats for converting multi-turn conversations into a single tokenizable string. Using a format different from the that which a model was trained with will usually cause severe, silent performance degradation, so matching the format used during training is extremely important.\n",
    "\n",
    "For example, Llama2 uses the following prompt structure to delineate between system, user, and assistant dialog turns:\n",
    "\n",
    "```\n",
    "<s>[INST] <<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n<user_prompt> [/INST]{assistant_response}</s><s>[INST]{user_prompt}[/INST] {assistant_response}\n",
    "```\n",
    "\n",
    "The special tokens, and usage schema vary model to model. To make sure we're using the correct format, we can make use of a models [chat template](https://huggingface.co/docs/transformers/main/en/chat_templating) via it's tokenizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      "-----\n",
      "<s>[INST] <<SYS>>\n",
      "You are a helpful, respectful and honest assistant that speaks like a pirate.\n",
      "<</SYS>>\n",
      "\n",
      "How many helicopters can a human eat in one sitting? [/INST]\n"
     ]
    }
   ],
   "source": [
    "system_input = (\n",
    "    \"You are a helpful, respectful and honest assistant that speaks like a pirate.\"\n",
    ")\n",
    "user_input = \"How many helicopters can a human eat in one sitting?\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_input},\n",
    "    {\"role\": \"user\", \"content\": user_input},\n",
    "]\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-70b-chat-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "print(f\"PROMPT:\\n-----\\n{prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the `apply_chat_template` method has taken our familiar list of `messages` and converted that into the properly formated string that our model expects. We can use this formatted string to pass to our model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Arrrr, me hearty! I be glad ye asked me that question, but I gotta say, it be a bit of a strange one, savvy? I mean, who's to say how many helicopters a human can eat in one sitting, matey?\n",
      "\n",
      "Now, I know what ye be thinkin', \"But me hearty, what if I be really hungry and I want to eat a whole bunch of helicopters?\"\n"
     ]
    }
   ],
   "source": [
    "response = client.text_generation(prompt, model=model_id, max_new_tokens=100)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about models _without_ a system prompt?\n",
    "\n",
    "Some models like Mistral & Mixtral weren't trained with a system prompt in their prompt structure. They look like this:\n",
    "\n",
    "```\n",
    "<s> [INST] Instruction [/INST] Model answer</s> [INST] Follow-up instruction [/INST]\n",
    "```\n",
    "\n",
    "In this case, if we want to use a system prompt, we can preprend it to our first instruction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      "-----\n",
      "<s>[INST] You are a helpful, respectful and honest assistant that speaks like a pirate. How many helicopters can a human eat in one sitting? [/INST]\n"
     ]
    }
   ],
   "source": [
    "system_input = (\n",
    "    \"You are a helpful, respectful and honest assistant that speaks like a pirate.\"\n",
    ")\n",
    "user_input = \"How many helicopters can a human eat in one sitting?\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": system_input + \" \" + user_input},\n",
    "]\n",
    "\n",
    "model_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "print(f\"PROMPT:\\n-----\\n{prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Arr matey, I be a pirate assistant, not a doctor of human digestion! In all me years on the seven seas, I never heard of a human eatin' a helicopter, one sitting or otherwise. That be a dangerous and impossible task, as helicopters be made of metal and other such unchewable parts. Best to leave the eatin' to the chickens and pigs, they be more suited for it, I'm sure.\n"
     ]
    }
   ],
   "source": [
    "response = client.text_generation(prompt, model=model_id, max_new_tokens=500)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Messages API\n",
    "\n",
    "Text Generation Inference (TGI) now offers a [Messages API](https://huggingface.co/blog/tgi-messages-api), making it directly compatible with the OpenAI Chat Completion API. This means that any existing scripts that use OpenAI models (via the OpenAI client library or third-party tools like LangChain or LlamaIndex) can be directly swapped out to use any open LLM running on a TGI endpoint. Lets see how.\n",
    "\n",
    "_Note: We have [a full example in the Hugging Face Cookbook](https://huggingface.co/learn/cookbook/en/tgi_messages_api_demo) to explain this in more detail._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Arr matey, I be a pirate assistant, not a swabbin' nutritionist! However, I can tell ye that it be physically impossible for a human to consume a helicopter, as they be made of steel, glass, and other materials not fit for consumption. So, ye can rest yer doubts, as no amount of helicopters be edible to a human in one sitting or any number of sittings! Arrr!"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# endpoint_url = \"<your-endpoint-url>\" # if you are using a dedicated Inference Endpoint\n",
    "model_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=f\"https://api-inference.huggingface.co/models/{model_id}/v1\",  # for serverless Inference Endpoints\n",
    "    # base_url=f\"{endpoint_url}/v1/\",  # for dedicated Inference Endpoints\n",
    "    api_key=get_token(),\n",
    ")\n",
    "\n",
    "\n",
    "system_input = (\n",
    "    \"You are a helpful, respectful and honest assistant that speaks like a pirate.\"\n",
    ")\n",
    "user_input = \"How many helicopters can a human eat in one sitting?\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": system_input + \" \" + user_input},\n",
    "]\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    model=model_id,\n",
    "    messages=messages,\n",
    "    stream=True,\n",
    "    max_tokens=500,\n",
    ")\n",
    "\n",
    "for message in chat_completion:\n",
    "    if message.choices[0].finish_reason != \"eos_token\":\n",
    "        print(message.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**\n",
    "\n",
    "- When deploying a IE with TGI, you must use `task: Text Generation` and use `client.text_generation`. This means you must handle chat template formatting on your own.\n",
    "- When deploying a IE with TGI with `task: Conversational`, you cannot use the `client.conversational` class. You'll get an error: `Make sure 'conversational' task is supported by the model.` So TGI only supports text generation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
