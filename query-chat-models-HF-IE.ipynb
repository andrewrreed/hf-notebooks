{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface-hub in ./.venv/lib/python3.10/site-packages (0.19.4)\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.10/site-packages (4.35.2)\n",
      "Collecting jinja2\n",
      "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from huggingface-hub) (3.13.1)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.10/site-packages (from huggingface-hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.venv/lib/python3.10/site-packages (from huggingface-hub) (4.66.1)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.10/site-packages (from huggingface-hub) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.10/site-packages (from huggingface-hub) (4.8.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.10/site-packages (from huggingface-hub) (2023.10.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.10/site-packages (from huggingface-hub) (6.0.1)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in ./.venv/lib/python3.10/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.10/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.10/site-packages (from transformers) (1.26.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./.venv/lib/python3.10/site-packages (from transformers) (0.4.0)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Using cached MarkupSafe-2.1.3-cp310-cp310-macosx_10_9_universal2.whl (17 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests->huggingface-hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests->huggingface-hub) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests->huggingface-hub) (2023.11.17)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests->huggingface-hub) (3.3.2)\n",
      "Installing collected packages: MarkupSafe, jinja2\n",
      "Successfully installed MarkupSafe-2.1.3 jinja2-3.1.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface-hub transformers jinja2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewreed/Documents/customers/olto/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = InferenceClient(\n",
    "    model=\"https://i3w111raiwkgpyqj.us-east-1.aws.endpoints.huggingface.cloud\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test `Intel/neural-chat-7b-v3-1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.62k/1.62k [00:00<00:00, 12.4MB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 3.55MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 16.9MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 2.38MB/s]\n"
     ]
    }
   ],
   "source": [
    "model_id = \"Intel/neural-chat-7b-v3-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Neural-Chat-7B-V3-1 model is a large language model (LLM) developed by OpenAI. It is based on the GPT-3.5 architecture and trained on a vast amount of text data. This model is designed to generate human-like text, understand and respond to various inputs, and perform tasks such as language translation, summarization, and text completion.\n",
      "\n",
      "The model works by processing input text through a complex neural network, which consists of many layers of artificial neurons. These neurons are connected and interact with each other, allowing the model to learn patterns and relationships within the text. The model then uses this learned knowledge to generate output text that is coherent and relevant to the input.\n",
      "\n",
      "In summary, the Neural-Chat-7B-V3-1 model works by utilizing a sophisticated neural network architecture to analyze and process large amounts of text data, enabling it to generate human-like responses and perform various language-related tasks.\n"
     ]
    }
   ],
   "source": [
    "# use the proper prompt format\n",
    "system_input = \"You are a chatbot developed by Intel. Please answer all questions to the best of your ability.\"\n",
    "user_input = \"How does the neural-chat-7b-v3-1 model work?\"\n",
    "prompt = f\"### System:\\n{system_input}\\n### User:\\n{user_input}\\n### Assistant:\\n\"\n",
    "\n",
    "# generate text\n",
    "out = client.text_generation(prompt, max_new_tokens=500)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test `meta-llama/Llama-2-7b-chat-hf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Thank you for your question! I'm here to help you in a responsible and safe manner. However, I must inform you that the neural-chat-7b-v3-1 model is a complex and proprietary AI language model developed by Meta AI, and I cannot provide a detailed explanation of its inner workings.\n",
      "\n",
      "The neural-chat-7b-v3-1 model is a state-of-the-art language model that uses a combination of deep learning techniques, including transformer architectures and multi-layer perceptrons, to generate human-like text. However, the exact workings of the model are not publicly disclosed by Meta AI, as it is a proprietary technology.\n",
      "\n",
      "While I cannot provide a detailed explanation of the model's workings, I can tell you that it has been trained on a large corpus of text data and has been designed to generate coherent and contextually relevant text. The model has been used in a variety of applications, including chatbots, language translation, and text summarization.\n",
      "\n",
      "If you have any further questions or concerns, please feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "# use the proper prompt format\n",
    "system_input = \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\\\n\\\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_input},\n",
    "    {\"role\": \"user\", \"content\": user_input},\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "# generate text\n",
    "client = InferenceClient()\n",
    "out = client.text_generation(prompt, max_new_tokens=500, model=model_id)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Open Questions**\n",
    "\n",
    "\n",
    "- When deploying a IE with TGI with `task: Text Generation`, can you use a `client.conversational`?\n",
    "    - No, you get an error: `Make sure 'conversational' task is supported by the model.`\n",
    "- When deploying a IE with TGI with `task: Conversational`, can you use a `client.conversational`?\n",
    "    - No, you get an error: `Make sure 'conversational' task is supported by the model.` So TGI only supports text generation.\n",
    "\n",
    "\n",
    "**My Findings**\n",
    "- When deploying a IE with TGI, you must use `task: Text Generation` and use `client.text_generation`. This means you must handle chat template formatting on your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
