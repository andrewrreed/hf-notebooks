{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Open AI to Open LLMs with Messages API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade -q huggingface_hub langchain langchain-community langchainhub langchain-openai llama-index chromadb bs4 sentence_transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# enter API key\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HF_API_KEY = getpass.getpass()\n",
    "\n",
    "# enter OpenAI key (for RAG embeddings)\n",
    "# os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Inference Endpoint using `huggingface_hub`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `huggingface_hub` Python library allows you to programatically create and manage Inference Endpoints which just a few steps. Here, we'll use it to deploy the powerful [Mixtral-8x7B-Instruct-v0.1](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1) as an endpoint running on [Text Generation Inference](https://huggingface.co/docs/text-generation-inference/index), our high performance inference solution for serving LLMs in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to specify the endpoint name and model repository for the text-generation task. A protected Inference Endpoint means a valid HF token is required to access the deployed API. We also need to configure the hardware requirements like vendor, region, accelerator, instance type, and size. You can check out the list of available resources [here](https://api.endpoints.huggingface.cloud/#get-/v2/provider)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import create_inference_endpoint\n",
    "\n",
    "endpoint = create_inference_endpoint(\n",
    "    \"nous-hermes-2-mixtral-8x7b-demo\",\n",
    "    repository=\"NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\",\n",
    "    framework=\"pytorch\",\n",
    "    task=\"text-generation\",\n",
    "    accelerator=\"gpu\",\n",
    "    vendor=\"aws\",\n",
    "    region=\"us-east-1\",\n",
    "    type=\"protected\",\n",
    "    instance_type=\"p4de\",\n",
    "    instance_size=\"2xlarge\",\n",
    "    namespace=\"HF-test-lab\",\n",
    "    custom_image={\n",
    "        \"health_route\": \"/health\",\n",
    "        \"env\": {\n",
    "            \"MAX_INPUT_LENGTH\": \"4096\",\n",
    "            \"MAX_BATCH_PREFILL_TOKENS\": \"4096\",\n",
    "            \"MAX_TOTAL_TOKENS\": \"32000\",\n",
    "            \"MAX_BATCH_TOTAL_TOKENS\": \"1024000\",\n",
    "            \"MODEL_ID\": \"/repository\",\n",
    "        },\n",
    "        # \"url\": \"ghcr.io/huggingface/text-generation-inference:1.4.0\",  # must be >= 1.4.0\n",
    "        \"url\": \"ghcr.io/huggingface/text-generation-inference:sha-ee1cf51\",\n",
    "    },\n",
    ")\n",
    "\n",
    "endpoint.wait()\n",
    "print(endpoint.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import create_inference_endpoint\n",
    "\n",
    "endpoint = create_inference_endpoint(\n",
    "    \"zephyr-7b-beta-arr-test\",\n",
    "    repository=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    framework=\"pytorch\",\n",
    "    task=\"text-generation\",\n",
    "    accelerator=\"gpu\",\n",
    "    vendor=\"aws\",\n",
    "    region=\"us-east-1\",\n",
    "    type=\"protected\",\n",
    "    instance_type=\"g5.2xlarge\",\n",
    "    instance_size=\"medium\",\n",
    "    namespace=\"HF-test-lab\",\n",
    "    custom_image={\n",
    "        \"health_route\": \"/health\",\n",
    "        \"env\": {\n",
    "            \"MAX_INPUT_LENGTH\": \"16384\",\n",
    "            \"MAX_BATCH_PREFILL_TOKENS\": \"16384\",\n",
    "            \"MAX_TOTAL_TOKENS\": \"18432\",\n",
    "            \"MAX_BATCH_TOTAL_TOKENS\": \"18432\",\n",
    "            \"MODEL_ID\": \"/repository\",\n",
    "        },\n",
    "        # \"url\": \"ghcr.io/huggingface/text-generation-inference:1.4.0\",  # must be >= 1.4.0\n",
    "        \"url\": \"ghcr.io/huggingface/text-generation-inference:sha-ee1cf51\",\n",
    "    },\n",
    ")\n",
    "\n",
    "endpoint.wait()\n",
    "print(endpoint.status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will take a few minutes for our deployment to spin up. We can utilize the `.wait()` utility to block the running thread until the endpoint reaches a final \"running\" state. Once its running, we can run a quick check to see everything is working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nSci-fi writer William Gibson said that the ‚Äúthe future is here, it‚Äôs just not evenly distributed.‚Äù Open-source software enthusiasts would perhaps understand his words as a metaphor on the dissemination of technological knowledge and its global implications.\\n\\nAs cryptocurrencies and digital identity storage/access technologies gain ground, so should the understanding of the underlying technical principles they employ become an increasingly important part of education and public awareness.\\n\\nThe pressing need'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint.client.text_generation(\n",
    "    \"<s>[INST] Why is open-source software important? [/INST]\",\n",
    "    max_new_tokens=100,\n",
    "    do_sample=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we now have a working deployment! But notice how we needed to carefully format the prompt according to the model's instruction format? While our [chat templates](https://huggingface.co/docs/transformers/chat_templating) handle all of this nuance, the new Messages API makes things even simpler..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Messages API via the OpenAI SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The added support for messages in TGI makes Inference Endpoints directly compatibile with the OpenAI Chat Completion API. This means that any existing scripts that use OpenAI models via the OpenAI client libraries can be directly swapped out to use any open-source LLM running on a TGI endpoint!\n",
    "\n",
    "The example below shows how to make this transition to stream responses from our Inference Endpoint. Simply replace the `base_url` with your endpoint URL (be sure to include `v1/` the suffix) and populate the `api_key` field with a valid Hugging Face user token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://y3y8movzdnat3ydy.us-east-1.aws.endpoints.huggingface.cloud'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "An open-source software is of great importance because of several reasons:\n",
      "\n",
      "1. Cost: Open-source software is free, or at least less costly than proprietary software. Since the source code is available, users can download, install, and use the software at no direct cost.\n",
      "\n",
      "2. Customization: The source code in open-source software is open, meaning developers and users can modify and customize the software to meet specific needs.\n",
      "\n",
      "3. Security: Since the source code is transparent, developers and users can review it to identify and rectify any security vulnerabilities. Conversely, commonly used proprietary software may hide security weaknesses, making them difficult to detect.\n",
      "\n",
      "4. Improved reliability: Open-source software tends to be more reliable than proprietary software because more developers, users and companies are actively contributing to its development. This creates a wide and collaborative community that maintains, tests and improves the software.\n",
      "\n",
      "5. Collaboration: Open-source software promotes collaboration because developers can learn from each other's code, creating a shared resource that benefits all users. The collaborative nature of open-source software also engages active communities of users, developers, and contributors.\n",
      "\n",
      "6. Promotes innovation: Open-source software encourages innovation by providing opportunities for developers to test their ideas and learn through collaboration. This promotes a culture of innovation that creates new technologies that can further the industry.\n",
      "\n",
      "7. Interoperability: Open-source software promotes interoperability between different software applications and systems. Since open-source software uses standardized practices, components and protocols, integration between different software is typically more straightforward, cost-effective and less time-consuming.\n",
      "\n",
      "8. Accessibility: Open-source software makes technology more accessible to people who might not have the resources to purchase expensive proprietary software. Furthermore, it's often simpler to use open-source software for niche use-cases that do not justify commercial investments.\n",
      "\n",
      "These reasons demonstrate that open-source software is not only beneficial for individuals and organizations, but it also enhances the overall software development and technology ecosystem.</s>"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# BASE_URL = endpoint.url\n",
    "BASE_URL = \"https://gw38hkbdke51vu7l.us-east-1.aws.endpoints.huggingface.cloud\"\n",
    "\n",
    "# init the client but point it to TGI\n",
    "client = OpenAI(\n",
    "    base_url=os.path.join(BASE_URL, \"v1/\"),\n",
    "    api_key=HF_API_KEY,\n",
    ")\n",
    "chat_completion = client.chat.completions.create(\n",
    "    model=\"tgi\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Why is open-source software important?\",\n",
    "        },\n",
    "    ],\n",
    "    stream=True,\n",
    "    max_tokens=500,\n",
    ")\n",
    "\n",
    "# iterate and print stream\n",
    "for message in chat_completion:\n",
    "    print(message.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# model_id = \"NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\"\n",
    "model_id = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open-source software is important for several reasons:\n",
      "\n",
      "1. Cost-effective: Open-source software is free to use, modify, and distribute, which makes it a cost-effective alternative to proprietary software. This is particularly beneficial for small businesses, startups, and non-profit organizations with limited budgets.\n",
      "\n",
      "2. Customizable: Since the source code is available, users can modify and customize the software to suit their specific needs. This level of customization is not possible with proprietary software, which can be a significant limitation for some users.\n",
      "\n",
      "3. Community-driven: Open-source software is developed and maintained by a community of developers who collaborate and share their knowledge. This community-driven approach ensures that the software is constantly being improved and updated, making it more reliable and secure.\n",
      "\n",
      "4. Transparency: The open-source model promotes transparency, as the source code is available for anyone to see and review. This transparency helps to identify and address any security vulnerabilities or bugs in the software, making it more secure than proprietary software.\n",
      "\n",
      "5. Innovation: Open-source software encourages innovation, as developers are free to build upon the existing codebase and create new features and functionalities. This innovation can lead to the development of new technologies and tools that benefit the wider community.\n",
      "\n",
      "Overall, open-source software is important because it provides users with greater flexibility, cost savings, and access to a community of developers, while also promoting transparency and innovation.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Why is open-source software important ?\"},\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "# print(prompt)\n",
    "# generate text\n",
    "out = endpoint.client.text_generation(prompt, max_new_tokens=500, model=model_id)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|assistant|>\n",
      "Open-source software is important for several reasons:\n",
      "\n",
      "1. Cost-effective: Open-source software is free to use, modify, and distribute, which makes it a cost-effective alternative to proprietary software. This is particularly beneficial for small businesses, startups, and organizations with limited budgets.\n",
      "\n",
      "2. Customizable: Since the source code is available, users can modify and customize the software to suit their specific needs. This level of customization is not possible with proprietary software, which can be a significant limitation for some users.\n",
      "\n",
      "3. Community-driven: Open-source software is developed and maintained by a community of developers, who contribute to the project voluntarily. This community-driven approach ensures that the software is constantly being improved and updated, and that any issues are addressed promptly.\n",
      "\n",
      "4. Security: Since the source code is available, security vulnerabilities can be identified and addressed more quickly than with proprietary software. Additionally, the community of developers can collaborate to develop security patches and updates, which can help to mitigate potential threats.\n",
      "\n",
      "5. Compatibility: Open-source software is often more compatible with other open-source software and hardware, which can make it a more attractive option for users who prefer to work with open-source technologies.\n",
      "\n",
      "6. Learning opportunities: Since the source code is available, users can learn how the software is built and how it works, which can provide valuable learning opportunities for developers and IT professionals.\n",
      "\n",
      "Overall, open-source software is important because it offers a range of benefits that are not available with proprietary software, including cost-effectiveness, customizability, community-driven development, security, compatibility, and learning opportunities.\n"
     ]
    }
   ],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|assistant|>\\nDeep learning is a subfield of machine learning that uses artificial neural networks with multiple layers to learn and make predictions or decisions based on large datasets. These neural networks, inspired by the structure of the human brain, can learn and adapt to complex patterns and relationships in data, enabling them to make more accurate predictions and decisions than traditional machine learning algorithms. Deep learning techniques are commonly used in applications such as image and speech recognition, natural language processing, and autonomous driving.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use IE with Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"<|assistant|>\\nOpen-source software is important for several reasons:\\n\\n1. It's free: Open-source software is free to use, distribute, and modify. This means that anyone can download, install, and use it without paying any licensing fees.\\n\\n2. It's customizable: Since the source code is available, users can modify and customize the software to suit their specific needs. This allows for greater flexibility and customization than propriet\")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"tgi\",\n",
    "    openai_api_key=HF_API_KEY,\n",
    "    openai_api_base=os.path.join(BASE_URL, \"v1/\"),\n",
    ")\n",
    "llm.invoke(\"Why is open-source software important?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82897f93173a4540919ba644d970a793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbc281c9e64543b38b95bff7c73b7124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62226956416b45779eb63151082e4dfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54a34f44f92a4bf6ab2819b6f53b57f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4e234e8a9fd4ae0ad66311bad7215d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6e9ff5877ac4b6dbcbbe1a0660f33ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/191 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'context': [Document(page_content='To overcome this weakness, amongst other approaches, one can integrate the LLM into a system where it can call tools: such a system is called an LLM agent.\\nIn this post, we explain the inner workings of ReAct agents, then show how to build them using the ChatHuggingFace class recently integrated in LangChain. Finally, we benchmark several open-source LLMs against GPT-3.5 and GPT-4.', metadata={'description': 'We‚Äôre on a journey to advance and democratize artificial intelligence through open source and open science.', 'language': 'No language found.', 'source': 'https://huggingface.co/blog/open-source-llms-as-agents', 'title': 'Open-source LLMs as LangChain Agents'}),\n",
       "  Document(page_content='Since the open-source models were not specifically fine-tuned for calling functions in the given output format, they are at a slight disadvantage compared to the OpenAI agents.\\nDespite this, some models perform really well! üí™\\nHere‚Äôs an example of Mixtral-8x7B answering the question: ‚ÄúWhich city has a larger population, Guiyang or Tacheng?‚Äù\\nThought: To answer this question, I need to find the current populations of both Guiyang and Tacheng. I will use the search tool to find this information.\\nAction:\\n{', metadata={'description': 'We‚Äôre on a journey to advance and democratize artificial intelligence through open source and open science.', 'language': 'No language found.', 'source': 'https://huggingface.co/blog/open-source-llms-as-agents', 'title': 'Open-source LLMs as LangChain Agents'}),\n",
       "  Document(page_content='Agents Showdown: how do open-source LLMs perform as general purpose reasoning agents?\\n\\t\\n\\nYou can find the code for this benchmark here.\\n\\n\\n\\n\\n\\n\\t\\tEvaluation\\n\\t\\n\\nWe want to measure how open-source LLMs perform as general purpose reasoning agents. Thus we select questions requiring using logic and the use of basic tools: a calculator and access to internet search.\\nThe final dataset is a combination of samples from 3 other datasets:', metadata={'description': 'We‚Äôre on a journey to advance and democratize artificial intelligence through open source and open science.', 'language': 'No language found.', 'source': 'https://huggingface.co/blog/open-source-llms-as-agents', 'title': 'Open-source LLMs as LangChain Agents'}),\n",
       "  Document(page_content='Open-source LLMs as LangChain Agents\\n\\t\\n\\nPublished\\n\\t\\t\\t\\tJanuary 24, 2024\\nUpdate on GitHub\\n\\nm-ric\\nAymeric Roucher\\n\\n\\n\\n\\nJofthomas\\nJoffrey THOMAS\\n\\n\\n\\n\\nandrewrreed\\nAndrew Reed\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\tTL;DR\\n\\t\\n\\nOpen-source LLMs have now reached a performance level that makes them suitable reasoning engines for powering agent workflows: Mixtral even surpasses GPT-3.5 on our benchmark, and its performance could easily be further enhanced with fine-tuning.\\n\\n\\n\\n\\n\\n\\t\\tIntroduction', metadata={'description': 'We‚Äôre on a journey to advance and democratize artificial intelligence through open source and open science.', 'language': 'No language found.', 'source': 'https://huggingface.co/blog/open-source-llms-as-agents', 'title': 'Open-source LLMs as LangChain Agents'})],\n",
       " 'question': 'According to this article which open-source model is the best for an agent behaviour?',\n",
       " 'answer': '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Load, chunk and index the contents of the blog\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://huggingface.co/blog/open-source-llms-as-agents\",),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# declare an HF embedding model\n",
    "hf_embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-large-en-v1.5\")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=hf_embeddings)\n",
    "\n",
    "# Retrieve and generate using the relevant snippets of the blog\n",
    "retriever = vectorstore.as_retriever()\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain_from_docs = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ").assign(answer=rag_chain_from_docs)\n",
    "\n",
    "rag_chain_with_source.invoke(\n",
    "    \"According to this article which open-source model is the best for an agent behaviour?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LlamaIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import OpenAI, OpenAILike\n",
    "\n",
    "llm = OpenAILike(\n",
    "    model=\"tgi\",\n",
    "    api_key=HF_API_KEY,\n",
    "    api_base=BASE_URL + \"/v1/\",\n",
    "    is_chat_model=True,\n",
    "    is_local=False,\n",
    "    is_function_calling_model=False,\n",
    "    context_window=4096,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionResponse(text='<|assistant|>\\nOpen-source software is important for several reasons:\\n\\n1. Cost-effective: Open-source software is free to use, modify, and distribute, which makes it a cost-effective alternative to proprietary software. This is particularly beneficial for small businesses, startups, and organizations with limited budgets.\\n\\n2. Customizable: Since the source code is available, users can modify and customize the software to suit their specific needs. This level', additional_kwargs={}, raw={'id': '', 'choices': [Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='<|assistant|>\\nOpen-source software is important for several reasons:\\n\\n1. Cost-effective: Open-source software is free to use, modify, and distribute, which makes it a cost-effective alternative to proprietary software. This is particularly beneficial for small businesses, startups, and organizations with limited budgets.\\n\\n2. Customizable: Since the source code is available, users can modify and customize the software to suit their specific needs. This level', role='assistant', function_call=None, tool_calls=None))], 'created': 1707145308, 'model': '/repository', 'object': 'text_completion', 'system_fingerprint': '1.4.0-sha-ee1cf51', 'usage': CompletionUsage(completion_tokens=100, prompt_tokens=23, total_tokens=123)}, delta=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.complete(\"Why is open-source software important?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting html2text\n",
      "  Downloading html2text-2020.1.16-py3-none-any.whl (32 kB)\n",
      "Installing collected packages: html2text\n",
      "Successfully installed html2text-2020.1.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58c4b7aed2014e3a8f42927839c3e64e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/779 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19821915d83c4f9abdaa6c12368f3f0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55ccd2b1ede04162bedfb4e8472ff23a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e8f904786044c50bfeb786992a0c06e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c0a8347a34e4a2e9a47ecb144fcd515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be8981edd1ce4272b87ef19f7375e09c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19311b33eb484d22b94fc4745c8a7235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce6932cceb4c49d880c3c2781fb593c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.evaluation import (\n",
    "    DatasetGenerator,\n",
    "    ResponseEvaluator,\n",
    "    QueryResponseEvaluator,\n",
    ")\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index import (\n",
    "    SimpleDirectoryReader,\n",
    "    ServiceContext,\n",
    "    LLMPredictor,\n",
    "    VectorStoreIndex,\n",
    "    load_index_from_storage,\n",
    "    StorageContext,\n",
    ")\n",
    "from llama_index import download_loader\n",
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "from llama_index.query_engine import CitationQueryEngine\n",
    "\n",
    "\n",
    "SimpleWebPageReader = download_loader(\"SimpleWebPageReader\")\n",
    "documents = SimpleWebPageReader(html_to_text=True).load_data(\n",
    "    [\"https://huggingface.co/blog/open-source-llms-as-agents\"]\n",
    ")\n",
    "\n",
    "# loads BAAI/bge-small-en-v1.5\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-large-en-v1.5\")\n",
    "# give the previously instanciated llm to your RAG pipeline\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=llm)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, service_context=service_context, show_progress=True\n",
    ")\n",
    "# play with the similarity_top_k for best results\n",
    "query_engine = CitationQueryEngine.from_args(\n",
    "    index,\n",
    "    similarity_top_k=2,\n",
    ")\n",
    "response = query_engine.query(\n",
    "    \"According to this article which open-source model is the best for an agent behaviour?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Based on the benchmark provided in the article, Mixtral-8x7B is the best performing open-source model for agent behavior, even outperforming GPT-3.5. However, the authors note that Mixtral was not specifically fine-tuned for agent workflows, and with proper fine-tuning, its performance could potentially be even higher. The article also mentions that GPT-4's performance in agent workflows is not\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
