{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Open AI to Open LLMs with Messages API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade -q huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Inference Endpoint using `huggingface_hub`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `huggingface_hub` Python library allows you to programatically create and manage Inference Endpoints which just a few steps. Here, we'll use it to deploy the powerful [Mixtral-8x7B-Instruct-v0.1](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1) as an endpoint running on [Text Generation Inference](https://huggingface.co/docs/text-generation-inference/index), our high performance inference solution for serving LLMs in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to specify the endpoint name and model repository for the text-generation task. A protected Inference Endpoint means a valid HF token is required to access the deployed API. We also need to configure the hardware requirements like vendor, region, accelerator, instance type, and size. You can check out the list of available resources [here](https://api.endpoints.huggingface.cloud/#get-/v2/provider)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import create_inference_endpoint\n",
    "\n",
    "endpoint = create_inference_endpoint(\n",
    "    \"mixtral-8x7b-instruct-v0-1-demo\",\n",
    "    repository=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "    framework=\"pytorch\",\n",
    "    task=\"text-generation\",\n",
    "    accelerator=\"gpu\",\n",
    "    vendor=\"aws\",\n",
    "    region=\"us-east-1\",\n",
    "    type=\"protected\",\n",
    "    instance_type=\"p4de\",\n",
    "    instance_size=\"2xlarge\",\n",
    "    namespace=\"HF-test-lab\",\n",
    "    custom_image={\n",
    "        \"health_route\": \"/health\",\n",
    "        \"env\": {\n",
    "            \"MAX_INPUT_LENGTH\": \"1024\",\n",
    "            \"MAX_BATCH_PREFILL_TOKENS\": \"2048\",\n",
    "            \"MAX_TOTAL_TOKENS\": \"32000\",\n",
    "            \"MAX_BATCH_TOTAL_TOKENS\": \"1024000\",\n",
    "            \"MODEL_ID\": \"/repository\",\n",
    "        },\n",
    "        # \"url\": \"ghcr.io/huggingface/text-generation-inference:1.4.0\",  # must be >= 1.4.0\n",
    "        \"url\": \"ghcr.io/huggingface/text-generation-inference:sha-ee1cf51\",\n",
    "    },\n",
    ")\n",
    "\n",
    "endpoint.wait()\n",
    "print(endpoint.status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will take a few minutes for our deployment to spin up. We can utilize the `.wait()` utility to block the running thread until the endpoint reaches a final \"running\" state. Once its running, we can run a quick check to see everything is working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.client.text_generation(\n",
    "    \"<s>[INST] Why is open-source so important? [/INST]\",\n",
    "    max_new_tokens=100,\n",
    "    do_sample=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we now have a working deployment! But notice how we needed to carefully format the prompt according to the model's instruction format? While our [chat templates](https://huggingface.co/docs/transformers/chat_templating) handle all of this nuance, the new Messages API makes things even simpler..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Messages API via the OpenAI SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The added support for messages in TGI makes Inference Endpoints directly compatibile with the OpenAI Chat Completion API. This means that any existing scripts that use OpenAI models via the OpenAI client libraries can be directly swapped out to use any open-source LLM running on a TGI endpoint!\n",
    "\n",
    "The example below shows how to make this transition to stream responses from our Inference Endpoint. Simply replace the `base_url` with your endpoint URL (be sure to include `v1/` the suffix) and populate the `api_key` field with a valid Hugging Face user token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Open-source software (OSS) is important for several reasons:\n",
      "\n",
      "1. Cost-effective: OSS is typically free to use, which can help organizations save money on software licensing fees.\n",
      "2. Flexibility and customization: Because the source code is openly available, users can modify and customize OSS to meet their specific needs.\n",
      "3. Community-driven development: OSS is often developed and maintained by a community of developers, which can lead to faster innovation and bug fixes compared to proprietary software.\n",
      "4. Transparency: The open-source nature of the software allows users to see exactly how it works, which can lead to increased trust and security.\n",
      "5. Interoperability: OSS is often designed to be compatible with a wide range of platforms and systems, which can make it easier to integrate with existing infrastructure.\n",
      "6. Encourage Innovation: Open-source software allows for a wider range of people to contribute to the development and improvement of the software which can lead to more innovation and new features.\n",
      "7. Reducing Vendor lock-in: By using open-source software, organizations can reduce their dependence on a single vendor and have the freedom to switch to different vendors or solutions if needed.\n",
      "8. Security: The open-source community can review and improve the code, which can lead to more secure software. Additionally, because the source code is available, it can be easier to audit and identify potential security vulnerabilities.\n",
      "9. Education: Open-source software provides an opportunity for developers to learn and improve their skills by studying and modifying the source code.\n",
      "10. Standardization: Open-source software can help to promote standards and interoperability, making it easier for different systems and organizations to work together.</s>"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# init the client but point it to TGI\n",
    "client = OpenAI(\n",
    "    base_url=\"https://ey1416en78lct0cg.us-east-1.aws.endpoints.huggingface.cloud/\"\n",
    "    + \"v1/\",\n",
    "    api_key=\"\",\n",
    ")\n",
    "chat_completion = client.chat.completions.create(\n",
    "    model=\"tgi\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Why is open-source software important?\"},\n",
    "    ],\n",
    "    stream=True,\n",
    "    max_tokens=500,\n",
    ")\n",
    "\n",
    "# iterate and print stream\n",
    "for message in chat_completion:\n",
    "    print(message.choices[0].delta.content, end=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
