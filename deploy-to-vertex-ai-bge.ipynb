{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc6e5f2b",
   "metadata": {},
   "source": [
    "# Deploying a Hugging Face model to Google Vertex AI for Bulk Embedding Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dbbda7",
   "metadata": {},
   "source": [
    "Inspired by the [GCP tutorial]( https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/community-content/pytorch_text_classification_using_vertex_sdk_and_gcloud/pytorch-text-classification-vertex-ai-train-tune-deploy.ipynb) we will deploy a `sentence-transformers` model on a [Vertex AI](https://cloud.google.com/vertex-ai/docs/predictions/deploy-model-api) endpoint. We will use [TorchServe](https://pytorch.org/serve/) to serve a Hugging Face model available on the [Hub](hf.co). To accelerate inference we will also use features from the `optimum` [library](https://github.com/huggingface/optimum) to apply graph optimization and/or quantization to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222da492-d787-42dd-bb80-f59c3dc97bf7",
   "metadata": {},
   "source": [
    "**Resources**:\n",
    "   - On batching:\n",
    "        - https://huggingface.co/docs/transformers/main_classes/pipelines#pipeline-batching\n",
    "   - On TorchServe:\n",
    "        - https://github.com/pytorch/serve/tree/master/examples/Huggingface_Transformers\n",
    "   - On Locust:\n",
    "        - https://medium.com/@tferreiraw/performing-load-tests-with-python-locust-io-62de7d91eebd\n",
    "        - https://medium.com/@ashmi_banerjee/3-step-tutorial-to-performance-test-ml-serving-apis-using-locust-and-fastapi-40e6cc580adc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854097a7",
   "metadata": {},
   "source": [
    "### Set up your local development environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0c5ae2",
   "metadata": {},
   "source": [
    "1. Follow the Google Cloud guide to [setting up a Python development environment](https://cloud.google.com/python/docs/setup) \n",
    "2. [Install and initialize the Cloud SDK.](https://cloud.google.com/sdk/docs/) \n",
    "3. Create a virtual environment (virtualenv, pyenv) with Python 3 (<3.9) and activate the environment\n",
    "4. Launch jupyter notebook from this environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7626c33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://cloud.google.com/products/calculator/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41229c4d",
   "metadata": {},
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e0237e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip -q install --upgrade google-cloud-aiplatform #Vertex AI sdk\n",
    "!pip -q install --upgrade transformers, datasets, locust, locust-plugins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b7143d",
   "metadata": {},
   "source": [
    "### Set up your Google Cloud project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445b5bee",
   "metadata": {},
   "source": [
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager)\n",
    "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project)\n",
    "1. Enable following APIs in your project required for running the tutorial\n",
    "    - [Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com)\n",
    "    - [Cloud Storage API](https://console.cloud.google.com/flows/enableapi?apiid=storage.googleapis.com)\n",
    "    - [Container Registry API](https://console.cloud.google.com/flows/enableapi?apiid=containerregistry.googleapis.com)\n",
    "    - [Cloud Build API](https://console.cloud.google.com/flows/enableapi?apiid=cloudbuild.googleapis.com)\n",
    "   \n",
    "1. Enter your project ID in the cell below. Then run the cell to make sure the Cloud SDK uses the right project for all the commands in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bc4202",
   "metadata": {},
   "source": [
    "### Authenticate to gcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0bcad2",
   "metadata": {},
   "source": [
    " 1. In the Cloud Console, go to the [**Create service account key** page](https://console.cloud.google.com/apis/credentials/serviceaccountkey).,\n",
    " 2. Click **Create service account**.,\n",
    " 3. In the **Service account name** field, enter a name, and click **Create**,\n",
    " 4. In the **Grant this service account access to project** section, click the **Role** drop-down list. Type \\\"Vertex AI\\\" into the filter box, and select **Vertex AI Administrator**. Type \\\"Storage Object Admin\\\" into the filter box, and select **Storage Object Admin**.\n",
    " 5. Click *Create*. A JSON file that contains your key downloads to your local environment.\n",
    " 6. Enter the path to your service account key as the `GOOGLE_APPLICATION_CREDENTIALS` variable in the cell below and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee9cbff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env GOOGLE_APPLICATION_CREDENTIALS ./keys/huggingface-ml-e974975230cc.json #change to your service account key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a4d2a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID:  huggingface-ml\n"
     ]
    }
   ],
   "source": [
    "# Get your Google Cloud project ID using google.auth\n",
    "import google.auth\n",
    "\n",
    "_, PROJECT_ID = google.auth.default()\n",
    "print(\"Project ID: \", PROJECT_ID)\n",
    "\n",
    "#Or set it yourself manually\n",
    "# PROJECT_ID = \"huggingface-ml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01857fb5",
   "metadata": {},
   "source": [
    "### Create a cloud storage bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cb4c6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"gs://andrew-reed-bucket\"  # <---CHANGE THIS TO YOUR BUCKET\n",
    "REGION = \"us-east4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b366365",
   "metadata": {},
   "source": [
    "**If the bucket doesn't exist, run the following:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9c1fb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! gsutil mb -l $REGION $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd343f3",
   "metadata": {},
   "source": [
    "Access the content of the bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20adc9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827daccf",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d25c6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import base64\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import transformers\n",
    "\n",
    "import google.auth\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import gapic as aip\n",
    "from google.protobuf.json_format import MessageToDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa5e61db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook runtime: GPU\n",
      "PyTorch version : 2.0.1+cu118\n",
      "Transformers version : 4.32.1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Notebook runtime: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(f\"PyTorch version : {torch.__version__}\")\n",
    "print(f\"Transformers version : {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4818dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_NAME = \"test_bge_embedder\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed45919e",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841c6e37",
   "metadata": {},
   "source": [
    "#### *Overview*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e05d7bb",
   "metadata": {},
   "source": [
    "Deploying a PyTorch model on [Vertex AI Predictions](https://cloud.google.com/vertex-ai/docs/predictions/getting-predictions) requires to use a custom container that serves online predictions. You will deploy a container running [PyTorch's TorchServe](https://pytorch.org/serve/) tool in order to serve predictions from a fine-tuned sentence transformer model `msmarco-distilbert-base-tas-b` available in [Hugging Face Transformers](https://huggingface.co/sentence-transformers/msmarco-distilbert-base-tas-b). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b69d42",
   "metadata": {},
   "source": [
    "Essentially, to deploy a PyTorch model on Vertex AI Predictions following are the steps:\n",
    "1. Package the trained model artifacts including [default](https://pytorch.org/serve/#default-handlers) or [custom](https://pytorch.org/serve/custom_service.html) handlers by creating an archive file using [Torch model archiver](https://github.com/pytorch/serve/tree/master/model-archiver),\n",
    "2. Build a [custom container](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements) compatible with Vertex AI Predictions to serve the model using Torchserve\n",
    "3. Upload the model with custom container image to serve predictions as a Vertex AI Model resource,\n",
    "4. Create a Vertex AI Endpoint and [deploy the model](https://cloud.google.com/vertex-ai/docs/predictions/deploy-model-api) resource"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035c60c5",
   "metadata": {},
   "source": [
    "### Save model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3372ad06-ee72-46fd-ae1b-e3195ab505c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = \"./\"+APP_NAME+\"_predictor\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0825e6ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "007c198258104afc966d0d6b77943d09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da4a82ce0dcc4665b31593c823cdbdf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8fe2966e06d4867a10eb58baaa7379e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "090fc3bfa1fa4938906f49a2ad8c49e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30f5c7b0a65e49eb86035bf360bd869d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/720 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16f52c9d220d4995af568250198d5fbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_id = \"BAAI/bge-large-en\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModel.from_pretrained(model_id)\n",
    "\n",
    "pt_save_directory = os.path.join(MODEL_DIR, \"model\")\n",
    "\n",
    "tokenizer.save_pretrained(pt_save_directory)\n",
    "model.save_pretrained(pt_save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65cd329a-1385-451c-a7e4-7c0228f642df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def cls_pooling(pipeline_output):\n",
    "    \"\"\"\n",
    "    Return the [CLS] token embedding\n",
    "    \"\"\"\n",
    "    return [_h[0][0] for _h in pipeline_output]\n",
    "        \n",
    "        \n",
    "tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)\n",
    "model = AutoModel.from_pretrained(pt_save_directory)\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "pipe = pipeline(\"feature-extraction\", model=model, tokenizer=tokenizer, batch_size=BATCH_SIZE)\n",
    "\n",
    "input_texts = [\"I like the new ORT pipeline\", \"blah blah blah\", \"Hello, I'm andrew\"]\n",
    "embeddings = cls_pooling(pipe(input_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17a81e9d-c0da-466d-ab51-6a700247e746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1024)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings), len(embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d1b219",
   "metadata": {},
   "source": [
    "### Create a custom model handler "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2eb813",
   "metadata": {},
   "source": [
    "Please refer to the [TorchServe documentation](https://pytorch.org/serve/custom_service.html) for defining a custom handler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8d0f6b68-9e9f-4061-a89e-f66025c1569a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use int for value, False to disable batching\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9de29252",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s $APP_NAME $BATCH_SIZE\n",
    "\n",
    "# %%writefile test_bge_embedder_predictor/custom_handler.py\n",
    "APP_NAME=$1\n",
    "BATCH_SIZE=$2\n",
    "\n",
    "cat << EOF > ./${APP_NAME}_predictor/custom_handler.py\n",
    "\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, pipeline\n",
    "\n",
    "from ts.torch_handler.base_handler import BaseHandler\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class SentenceTransformersHandler(BaseHandler):\n",
    "    \"\"\"\n",
    "    The handler takes an input string and returns the embedding \n",
    "    based on the serialized transformers checkpoint.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(SentenceTransformersHandler, self).__init__()\n",
    "        self.initialized = False\n",
    "        self.batch_size = $BATCH_SIZE\n",
    "\n",
    "    def initialize(self, ctx):\n",
    "        \"\"\" Loads the model.onnx file and initialized the model object.\n",
    "        Instantiates Tokenizer for preprocessor to use and a feature extraction pipeline\n",
    "        \"\"\"\n",
    "        self.manifest = ctx.manifest\n",
    "\n",
    "        properties = ctx.system_properties\n",
    "        model_dir = properties.get(\"model_dir\")\n",
    "        self.device = torch.device(\"cuda:\" + str(properties.get(\"gpu_id\")) if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Read model serialize/pt file\n",
    "        serialized_file = self.manifest[\"model\"][\"serializedFile\"]\n",
    "        model_pt_path = os.path.join(model_dir, serialized_file)\n",
    "        if not os.path.isfile(model_pt_path):\n",
    "            raise RuntimeError(\"Missing the model.onnx or pytorch_model.bin file\")\n",
    "        \n",
    "        # Load model\n",
    "        self.model = AutoModel.from_pretrained(model_dir)\n",
    "        logger.debug('Transformer model from path {0} loaded successfully'.format(model_dir))\n",
    "        logger.info(f\"model_pt_path: {model_pt_path}\")\n",
    "        logger.info(f\"model_dir: {model_dir}\")\n",
    "        \n",
    "        # Ensure to use the same tokenizer used during training\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_dir, model_max_length=512)\n",
    "        \n",
    "        # Create an optimum pipeline\n",
    "        # Use BetterTransformer for fused kernel + sparsity optimizations\n",
    "        # https://medium.com/pytorch/bettertransformer-out-of-the-box-performance-for-huggingface-transformers-3fbe27d50ab2\n",
    "        if self.batch_size:\n",
    "            logger.info(f\"----Loading pipeline with batch_size={self.batch_size}\")\n",
    "            self.pipeline = pipeline(\"feature-extraction\",\n",
    "                                     model=self.model, \n",
    "                                     tokenizer=self.tokenizer, \n",
    "                                     device=self.device, \n",
    "                                     accelerator=\"bettertransformer\",\n",
    "                                     batch_size=self.batch_size)\n",
    "        else:\n",
    "            logger.info(f\"----Loading pipeline without batching\")\n",
    "            self.pipeline = pipeline(\"feature-extraction\",\n",
    "                                     model=self.model, \n",
    "                                     tokenizer=self.tokenizer, \n",
    "                                     device=self.device, \n",
    "                                     accelerator=\"bettertransformer\")\n",
    "\n",
    "        self.initialized = True\n",
    "\n",
    "    def preprocess(self, requests):\n",
    "        \"\"\" Preprocessing input request by tokenizing\n",
    "            Extend with your own preprocessing steps as needed\n",
    "            \n",
    "            [{'data': b'I am creating an endpoint using TorchServe and HF transformers\\n'}]\n",
    "        \"\"\"\n",
    "        # print(f'{\"---\"*20}')\n",
    "        # print(requests)\n",
    "        # print(type(requests))\n",
    "        # print(len(requests))\n",
    "        # print(f'{\"---\"*20}')\n",
    "        \n",
    "        input_texts = []\n",
    "        for idx, request in enumerate(requests):\n",
    "            text = request.get(\"data\")\n",
    "            if text is None:\n",
    "                text = request.get(\"body\")\n",
    "                \n",
    "            text = text.decode('utf-8')\n",
    "            input_texts.append(text)\n",
    "        logger.info(\"Received text: '%s'\", input_texts)\n",
    "        \n",
    "        return input_texts\n",
    "\n",
    "    def inference(self, input_texts):\n",
    "        \"\"\" Predict the class of a text using a trained transformer model.\n",
    "        \"\"\"\n",
    "        \n",
    "        def cls_pooling(pipeline_output):\n",
    "            \"\"\"\n",
    "            Return the [CLS] token embedding\n",
    "            \"\"\"\n",
    "            return [_h[0][0] for _h in pipeline_output]\n",
    "        \n",
    "        embeddings = cls_pooling(self.pipeline(input_texts))\n",
    "\n",
    "        logger.info(f\"Model embedded: {len(embeddings)}\")\n",
    "        return embeddings\n",
    "\n",
    "    def postprocess(self, inference_output):\n",
    "        return inference_output\n",
    "    \n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f44f516",
   "metadata": {},
   "source": [
    "### Create custom container image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fb4a9a",
   "metadata": {},
   "source": [
    "**Create a Dockerfile with TorchServe as base image**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbb394c",
   "metadata": {},
   "source": [
    "**NB**: to define the right Torchserve parameters such as `workers` please consult (https://github.com/pytorch/serve/blob/master/docs/performance_guide.md) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ca79194d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./test_bge_embedder_predictor/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%bash -s $APP_NAME $BATCH_SIZE\n",
    "\n",
    "APP_NAME=$1\n",
    "BATCH_SIZE=$2\n",
    "\n",
    "cat << EOF > ./${APP_NAME}_predictor/Dockerfile\n",
    "\n",
    "FROM pytorch/torchserve:latest-gpu\n",
    "\n",
    "# install dependencies\n",
    "RUN python3 -m pip install --upgrade pip\n",
    "RUN pip3 install transformers\n",
    "\n",
    "USER model-server\n",
    "\n",
    "# copy model artifacts, custom handler and other dependencies\n",
    "COPY custom_handler.py /home/model-server/\n",
    "COPY ./model/ / /home/model-server/\n",
    "\n",
    "# create torchserve configuration file\n",
    "USER root\n",
    "RUN printf \"\\nservice_envelope=json\" >> /home/model-server/config.properties\n",
    "RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home/model-server/config.properties\n",
    "RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /home/model-server/config.properties\n",
    "RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /home/model-server/config.properties\n",
    "RUN if [ \"$BATCH_SIZE\" = False ]; then \\\n",
    "        : \\\n",
    "        else \\\n",
    "        printf '\\nmodels={\\\n",
    "          \"$APP_NAME\": {\\\n",
    "            \"1.0\": {\\\n",
    "                \"defaultVersion\": true,\\\n",
    "                \"marName\": \"$APP_NAME.mar\",\\\n",
    "                \"minWorkers\": 1,\\\n",
    "                \"maxWorkers\": 8,\\\n",
    "                \"batchSize\": \"$BATCH_SIZE\",\\\n",
    "                \"maxBatchDelay\": 100,\\\n",
    "                \"responseTimeout\": 200\\\n",
    "            }\\\n",
    "          }}' >> /home/model-server/config.properties; \\\n",
    "    fi\n",
    "    \n",
    "# expose health and prediction listener ports from the image\n",
    "EXPOSE 7080\n",
    "EXPOSE 7081\n",
    "\n",
    "# create model archive file packaging model artifacts and dependencies\n",
    "RUN torch-model-archiver -f \\\n",
    "  --model-name=$APP_NAME \\\n",
    "  --version=1.0 \\\n",
    "  --serialized-file=/home/model-server/pytorch_model.bin \\\n",
    "  --handler=/home/model-server/custom_handler.py \\\n",
    "  --extra-files \"/home/model-server/config.json,/home/model-server/tokenizer.json,/home/model-server/tokenizer_config.json,/home/model-server/special_tokens_map.json,/home/model-server/vocab.txt\" \\\n",
    "  --export-path=/home/model-server/model-store\n",
    "\n",
    "# run Torchserve HTTP serve to respond to prediction requests\n",
    "CMD [\"torchserve\", \\\n",
    "     \"--start\", \\\n",
    "     \"--ts-config=/home/model-server/config.properties\", \\a\n",
    "     \"--models\", \\\n",
    "     \"$APP_NAME=$APP_NAME.mar\", \\\n",
    "     \"--model-store\", \\\n",
    "     \"/home/model-server/model-store\"]\n",
    "\n",
    "EOF\n",
    "\n",
    "echo \"Writing ./${APP_NAME}_predictor/Dockerfile\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "cc89ea12-0e5e-4308-863c-5aae33058f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_DIR = APP_NAME+\"_predictor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "98567898-71ce-4d2b-9f86-d9565973c1b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('test_bge_embedder', 'test_bge_embedder_predictor')"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "APP_NAME, APP_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca594d5b",
   "metadata": {},
   "source": [
    "**Build container**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e2b160e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUSTOM_PREDICTOR_IMAGE_URI = gcr.io/huggingface-ml/pytorch_predict_test_bge_embedder\n"
     ]
    }
   ],
   "source": [
    "CUSTOM_PREDICTOR_IMAGE_URI = f\"gcr.io/{PROJECT_ID}/pytorch_predict_{APP_NAME}\"\n",
    "print(f\"CUSTOM_PREDICTOR_IMAGE_URI = {CUSTOM_PREDICTOR_IMAGE_URI}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7d09ce47",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  1.342GB\n",
      "Step 1/16 : FROM pytorch/torchserve:latest-gpu\n",
      " ---> 04eef250c14e\n",
      "Step 2/16 : RUN python3 -m pip install --upgrade pip\n",
      " ---> Using cache\n",
      " ---> 2f06486ed62f\n",
      "Step 3/16 : RUN pip3 install transformers\n",
      " ---> Using cache\n",
      " ---> 4ed472215157\n",
      "Step 4/16 : USER model-server\n",
      " ---> Using cache\n",
      " ---> 4fa0cd41a25e\n",
      "Step 5/16 : COPY custom_handler.py /home/model-server/\n",
      " ---> 7580986c8753\n",
      "Step 6/16 : COPY ./model/ / /home/model-server/\n",
      " ---> 6cb9c33fbc7b\n",
      "Step 7/16 : USER root\n",
      " ---> Running in 1a10cec8c6bc\n",
      "Removing intermediate container 1a10cec8c6bc\n",
      " ---> a91921ac1cff\n",
      "Step 8/16 : RUN printf \"\\nservice_envelope=json\" >> /home/model-server/config.properties\n",
      " ---> Running in 1dadb9c7aaf0\n",
      "Removing intermediate container 1dadb9c7aaf0\n",
      " ---> 0933c09bbd31\n",
      "Step 9/16 : RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home/model-server/config.properties\n",
      " ---> Running in 61a0618d1592\n",
      "Removing intermediate container 61a0618d1592\n",
      " ---> 2f9eb17850d4\n",
      "Step 10/16 : RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /home/model-server/config.properties\n",
      " ---> Running in 6fd8f9b06a59\n",
      "Removing intermediate container 6fd8f9b06a59\n",
      " ---> 9cad06c0a5ac\n",
      "Step 11/16 : RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /home/model-server/config.properties\n",
      " ---> Running in bf413a5f90f7\n",
      "Removing intermediate container bf413a5f90f7\n",
      " ---> 6f891b69b22b\n",
      "Step 12/16 : RUN if [ \"32\" = False ]; then         :         else         printf '\\nmodels={          \"test_bge_embedder\": {            \"1.0\": {                \"defaultVersion\": true,                \"marName\": \"test_bge_embedder.mar\",                \"minWorkers\": 1,                \"maxWorkers\": 8,                \"batchSize\": \"32\",                \"maxBatchDelay\": 100,                \"responseTimeout\": 200            }          }}' >> /home/model-server/config.properties;     fi\n",
      " ---> Running in 887d3c78cd4d\n",
      "Removing intermediate container 887d3c78cd4d\n",
      " ---> 5c45f04f755a\n",
      "Step 13/16 : EXPOSE 7080\n",
      " ---> Running in 15ed4388ee48\n",
      "Removing intermediate container 15ed4388ee48\n",
      " ---> 56c0d6128341\n",
      "Step 14/16 : EXPOSE 7081\n",
      " ---> Running in 736b49614849\n",
      "Removing intermediate container 736b49614849\n",
      " ---> 90df1f27f9f4\n",
      "Step 15/16 : RUN torch-model-archiver -f   --model-name=test_bge_embedder   --version=1.0   --serialized-file=/home/model-server/pytorch_model.bin   --handler=/home/model-server/custom_handler.py   --extra-files \"/home/model-server/config.json,/home/model-server/tokenizer.json,/home/model-server/tokenizer_config.json,/home/model-server/special_tokens_map.json,/home/model-server/vocab.txt\"   --export-path=/home/model-server/model-store\n",
      " ---> Running in abf24e78c8c7\n",
      "Removing intermediate container abf24e78c8c7\n",
      " ---> a477311172a9\n",
      "Step 16/16 : CMD [\"torchserve\",      \"--start\",      \"--ts-config=/home/model-server/config.properties\",      \"--models\",      \"test_bge_embedder=test_bge_embedder.mar\",      \"--model-store\",      \"/home/model-server/model-store\"]\n",
      " ---> Running in e075bbbb3a5b\n",
      "Removing intermediate container e075bbbb3a5b\n",
      " ---> 766caa40952d\n",
      "Successfully built 766caa40952d\n",
      "Successfully tagged gcr.io/huggingface-ml/pytorch_predict_test_bge_embedder:latest\n"
     ]
    }
   ],
   "source": [
    "!docker build \\\n",
    "  --tag=$CUSTOM_PREDICTOR_IMAGE_URI \\\n",
    "  ./$APP_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97df6271",
   "metadata": {},
   "source": [
    "**Test API locally**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c284be67-6783-4237-bc08-7b90f73dcbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d8c16e4de0f1b3fb868f1e89eebfb924b43584df12d8106820747e2d79b227dd\n"
     ]
    }
   ],
   "source": [
    "!docker run -td --rm -p 7080:7080 --gpus all --name=$APP_NAME $CUSTOM_PREDICTOR_IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4816b243",
   "metadata": {},
   "source": [
    "1. Health check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f66cc11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"status\": \"Healthy\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!curl http://localhost:7080/ping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd925c3f-d3a4-44f4-b52e-62a06de2f571",
   "metadata": {},
   "source": [
    "2. Send request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2dcbddc7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"predictions\": [[-0.3966214656829834, 0.13638117909431458, -0.7998953461647034, 0.17386069893836975, -0.32081007957458496, 0.20749905705451965, -0.430697500705719, 0.18672336637973785, 0.6046302914619446, 0.37625569105148315, 0.8786728978157043, 0.08130515366792679, 0.7122514843940735, -0.5567383170127869, -0.3595656752586365, 0.13959753513336182, -0.7658876776695251, -0.6352641582489014, -0.38556528091430664, -0.08639071136713028, -0.19624273478984833, -0.370313823223114, -0.7243615984916687, -0.9205985069274902, -0.689978301525116, 0.7531246542930603, 0.053085941821336746, -0.04321198910474777, 1.5050652027130127, 1.1163926124572754, -0.7939869165420532, -0.8258238434791565, 0.2806147336959839, -1.0460847616195679, -0.4688354730606079, -0.8166561126708984, 0.4421796202659607, -0.7080443501472473, -0.9239137768745422, -0.642957866191864, 0.11196565628051758, 0.1299777328968048, 0.6359308362007141, -1.1601362228393555, -1.0760389566421509, 0.31392139196395874, -0.11758226901292801, -0.3506949543952942, 0.3052726089954376, -0.059829261153936386, -0.3177773952484131, 0.4941027760505676, 0.2933858335018158, 0.1444871425628662, -0.09043711423873901, 0.20646224915981293, -0.37968191504478455, -0.3034849166870117, -0.8020312190055847, 0.08728399872779846, 0.6855151057243347, 0.5730102062225342, 0.3094993233680725, -0.9763081669807434, 0.5870273113250732, 0.835360050201416, -0.24337144196033478, -0.02856515720486641, 0.2041519433259964, -0.5132887363433838, -0.22309990227222443, 0.22145040333271027, -0.43467333912849426, -0.9606346487998962, 0.24215686321258545, 0.6729783415794373, 0.27795717120170593, -0.41520383954048157, -0.20291857421398163, 0.5625733137130737, 0.3311264216899872, 1.084069848060608, -0.10361042618751526, 0.3973957896232605, -0.7328456044197083, -0.09377974271774292, 0.14871327579021454, -0.3730209469795227, -0.19174213707447052, -0.12730298936367035, -0.09678641706705093, 0.6973597407341003, 0.151514932513237, 0.2977498173713684, 0.274114727973938, 0.9433118104934692, -0.7917332649230957, 0.31075114011764526, -0.1367333084344864, 0.9158764481544495, 0.7314509749412537, 1.014475703239441, -0.46263933181762695, 0.33707889914512634, -0.7849408984184265, -0.11563001573085785, 0.10436049848794937, -0.13668261468410492, -0.5316161513328552, -0.9626961946487427, -0.1922183632850647, -0.4910103976726532, 0.3833783268928528, -0.12163107097148895, -0.17370252311229706, 0.07969440519809723, -0.32988104224205017, 0.4833300709724426, -0.3030248284339905, 0.6313797831535339, 0.041849762201309204, -0.1229269802570343, 0.8447116613388062, -1.0565826892852783, -0.07356484234333038, -0.6543387174606323, -0.030344177037477493, 0.6749336123466492, -0.17448782920837402, -0.5836319923400879, 0.06292417645454407, -0.9638888835906982, 0.6102584600448608, 0.8525118231773376, -0.18105246126651764, 0.10244124382734299, -0.15874138474464417, 0.8620677590370178, 0.3567448556423187, -1.1495215892791748, -0.2260182946920395, -0.2759545147418976, 0.33215630054473877, 1.7881258726119995, 0.1783536970615387, -0.021879374980926514, 0.11781081557273865, -0.13566334545612335, -1.2799677848815918, 0.1663062870502472, -0.435886949300766, 0.26133543252944946, -0.13501091301441193, -0.19248321652412415, -0.275865375995636, 0.01871652528643608, -0.3443857431411743, 0.03896601125597954, 0.2044437676668167, 0.4724757969379425, -0.30376285314559937, 0.6190528273582458, -0.4203357398509979, 0.9929957985877991, -0.2627313435077667, 0.44931939244270325, -0.5808301568031311, -0.4284118413925171, -0.06514706462621689, -0.7060654759407043, 0.203550785779953, -0.09319249540567398, 0.1885758489370346, 0.21760967373847961, 0.5993668437004089, 0.01588631235063076, 0.8696792721748352, 0.3959043622016907, 0.29946833848953247, 0.6858079433441162, -0.3583114743232727, 0.39342573285102844, -0.3885774314403534, 1.888692855834961, 0.6828340888023376, -0.18832650780677795, 0.7269896268844604, -0.04173104092478752, -0.5635043978691101, -0.19316741824150085, 0.22376881539821625, 0.8775323629379272, -1.1910829544067383, 0.8829020261764526, -0.1999548375606537, 0.1731109768152237, -0.7114282250404358, 0.143430694937706, -0.3658476173877716, -1.4202743768692017, -0.6244347095489502, 1.123333215713501, -0.8510399460792542, 0.9609729647636414, -0.1391782909631729, -0.2243219017982483, 0.6692752242088318, 0.9294262528419495, -0.10522899031639099, -0.2767382264137268, 0.7995695471763611, 0.874822199344635, -0.8110483288764954, -0.07652156054973602, -0.17130865156650543, 0.034112658351659775, -0.524811863899231, 0.8546360731124878, 0.0647868886590004, -0.34001198410987854, 0.012470276094973087, 0.2090688794851303, 0.9461791515350342, 0.5746755003929138, -0.25369808077812195, 0.13580814003944397, -0.4625324606895447, 0.5778129696846008, 0.016364537179470062, -0.48655587434768677, -0.43527951836586, 0.6671077013015747, 0.007792029529809952, 1.0898444652557373, -0.19359806180000305, 0.45010966062545776, 0.7625822424888611, 0.9896217584609985, -0.4240695834159851, 0.31829094886779785, 0.11371766775846481, 0.09955672919750214, 0.6151646375656128, 0.7793988585472107, -0.6196331977844238, -0.27262163162231445, 0.040115125477313995, -0.2737624943256378, -1.125974178314209, 1.3017261028289795, -1.266704797744751, 0.637485921382904, 0.3337371349334717, 0.5352460145950317, -0.8795695304870605, -0.5810562372207642, -0.27690473198890686, 1.4077534675598145, -0.757043182849884, -0.018477633595466614, 0.4964941740036011, 0.24768660962581635, -0.3561549484729767, 0.005769054871052504, 0.5088698267936707, 0.5470752120018005, -0.38457658886909485, 0.13547608256340027, -0.1302672177553177, -0.8831439018249512, -0.7400323748588562, -0.5776107311248779, -1.047324299812317, -0.36491578817367554, -1.1119390726089478, 0.28433698415756226, 1.2192492485046387, -0.03374022617936134, 0.8222025036811829, 0.0817013680934906, 0.03347451239824295, -0.09511277824640274, -0.29253149032592773, 0.8606639504432678, 0.66192626953125, 0.9731667637825012, -0.4954633116722107, 0.5062441825866699, -0.20334461331367493, 0.7787317633628845, -0.5322514176368713, -0.4261629283428192, 0.20008017122745514, -0.8161988854408264, 0.667510986328125, -0.10326264053583145, -0.025300761684775352, -0.6388343572616577, -0.7730933427810669, -0.23796626925468445, -0.6229566931724548, 0.09137706458568573, -0.25987017154693604, -0.0958142876625061, -0.6271919012069702, 0.40629684925079346, 0.7437009215354919, -0.7927067279815674, 1.1756995916366577, 1.2783236503601074, -0.31310155987739563, 0.7523887157440186, 0.08712565898895264, 0.17915292084217072, -1.4048106670379639, 1.684493064880371, 0.7420653700828552, -0.14763779938220978, -0.7862914204597473, -0.3457438349723816, -0.19842782616615295, 0.013130227103829384, 0.09200092405080795, -0.09044290333986282, -0.4214364290237427, 1.1476974487304688, -0.277253121137619, -1.454000473022461, 0.7272441387176514, -0.8607166409492493, -0.18662898242473602, -0.937839150428772, -0.43756362795829773, 0.36452051997184753, 0.08915505558252335, 0.4449254274368286, -0.4964328706264496, 0.25282907485961914, -0.22076012194156647, 0.5605252981185913, 0.6556509733200073, -0.6803333163261414, 0.656517744064331, 0.7480772137641907, -0.1362469345331192, 0.6162958145141602, 0.7958747744560242, -0.9967977404594421, 0.08532963693141937, -0.006369586568325758, 0.050611481070518494, 0.01647716946899891, 0.6823632121086121, 0.189675435423851, 0.3052453100681305, 1.0408437252044678, -0.8929324746131897, -0.07299742847681046, 0.6264289617538452, 1.0435447692871094, 0.9573855400085449, 0.10989517718553543, 0.5336896181106567, -0.07394921034574509, -0.801025927066803, -0.7851695418357849, 0.9334678053855896, 0.26138731837272644, 1.3604235649108887, -0.6431252360343933, 0.6702200770378113, -0.3814869821071625, -0.5292284488677979, 0.6835213303565979, -1.3062465190887451, -0.3667011559009552, 0.46228963136672974, -0.08864228427410126, 1.1077017784118652, -0.5601519346237183, 0.012040245346724987, -0.14192765951156616, 0.7748821377754211, 0.8260865211486816, 0.005374385043978691, 0.3516116738319397, -0.3294372260570526, 0.5347497463226318, -0.6334707736968994, -0.2957712411880493, -0.2741864323616028, -0.4222263991832733, 0.25665104389190674, -0.21711879968643188, -0.8871667981147766, -0.20495662093162537, 0.5011677145957947, 0.7664725184440613, 1.1733512878417969, 0.2529718577861786, 1.0454533100128174, -0.41678351163864136, 0.17305335402488708, 0.5946333408355713, -0.29183170199394226, -0.025010451674461365, -0.7814303040504456, 0.6105749607086182, 0.8031684756278992, -0.8485285639762878, -0.10127592831850052, -0.34236854314804077, -0.30359983444213867, 0.6908069849014282, 0.045203614979982376, -0.30516427755355835, -0.9880033731460571, 1.028973937034607, 0.42515915632247925, 0.5118342638015747, -0.22363485395908356, -0.3987211585044861, -0.595227837562561, 0.5605702996253967, 0.8579210638999939, -1.1260972023010254, -0.2976346015930176, -0.9435538649559021, 0.07788508385419846, 0.8034093379974365, -0.2303372472524643, -1.0281447172164917, -0.35495227575302124, -0.42421483993530273, -0.8645925521850586, 0.3347013592720032, 0.9689976572990417, 0.014669841155409813, -0.2997157871723175, -0.6512706875801086, -0.25958576798439026, -0.190089613199234, -0.145375058054924, -0.20263881981372833, -0.026949584484100342, 0.6161904335021973, -0.21622234582901, 0.5737918615341187, -0.79991614818573, -0.3256528377532959, 0.08340154588222504, -1.191978931427002, 1.2592861652374268, -0.10262368619441986, -0.5183593034744263, -0.5210952162742615, 0.22806857526302338, 0.9712752103805542, 0.8954472541809082, -0.1238451823592186, -0.16181069612503052, -0.09418568760156631, 0.9511457085609436, -1.0000953674316406, -0.828229546546936, 0.5523877739906311, 0.17345143854618073, -0.2563830316066742, 0.35184764862060547, 0.845363438129425, -0.055583130568265915, 0.2992078363895416, 0.33994412422180176, -0.820556640625, 0.38309261202812195, -0.6617307662963867, 0.6248154044151306, 0.1826585829257965, -0.7223129272460938, 0.35222363471984863, -1.1939455270767212, 0.7930299639701843, 0.20281222462654114, -0.5407845973968506, -0.12124781310558319, -0.8239238262176514, 0.1693756878376007, 0.3758417069911957, -0.49534136056900024, 0.5278310775756836, 0.4643799364566803, -0.2766263484954834, -0.3681905269622803, -0.40790998935699463, 0.6067244410514832, -0.15235233306884766, -0.4756524860858917, -0.2007029950618744, 1.1766575574874878, 0.324329137802124, 0.5609227418899536, -0.9089069366455078, -1.2752342224121094, -0.03952443599700928, -0.5741126537322998, 0.0917120948433876, -0.39959290623664856, -0.19990067183971405, -0.1661747694015503, -0.7153204679489136, -0.5290986895561218, 0.08521246165037155, -0.2110571414232254, 0.07943906635046005, 0.02325025573372841, 0.29297542572021484, -0.07475057989358902, -0.044607218354940414, -0.6166949272155762, 0.9819732308387756, 0.3054634928703308, -1.3329806327819824, -0.5042008757591248, 0.7329347729682922, -0.10623154044151306, 0.6185060143470764, 0.9704774022102356, -0.826292097568512, -0.6553776860237122, -0.4039527475833893, 0.2869218587875366, -0.9108874797821045, -0.1466718167066574, -0.7358863949775696, -0.8391209840774536, -0.3604980707168579, 0.9915859699249268, 0.2669461667537689, -1.021451711654663, 0.2967347204685211, -0.9973606467247009, 0.22592496871948242, -0.6461231708526611, -0.47963622212409973, -0.49795204401016235, 0.5136231780052185, 0.053958479315042496, 0.9899160265922546, -0.15337887406349182, 0.13849300146102905, -0.4967532753944397, -0.15869615972042084, 0.983410656452179, 0.3738165497779846, -0.7538033723831177, -0.6446257829666138, 0.2638740837574005, -0.14871874451637268, 0.05622953921556473, 0.3613950312137604, 0.008403901010751724, 0.09736920893192291, -0.04972570762038231, 0.17792853713035583, -0.936942458152771, -0.26966384053230286, -0.4064565896987915, -0.054737962782382965, 0.9701464772224426, -0.548542320728302, 0.5264431834220886, 0.050390250980854034, 0.7959290742874146, 0.5180389881134033, 0.08856844902038574, -0.46794453263282776, -0.4794997274875641, -0.47405391931533813, -1.0124332904815674, 0.49343279004096985, -0.739465057849884, -0.46457046270370483, -0.19168347120285034, -0.10904661566019058, 1.0772883892059326, -0.24093753099441528, 0.6720455288887024, 1.1889628171920776, -0.0009562966297380626, 0.1073487251996994, -1.5000194311141968, 1.2352184057235718, 0.5857192277908325, -0.3665579855442047, 0.18535636365413666, -0.2646721303462982, -0.5024762749671936, 0.053030990064144135, -0.44348740577697754, -0.6708192229270935, -0.5054014325141907, -0.6902363896369934, 0.6680878400802612, -0.12872202694416046, 0.7662477493286133, 0.18784862756729126, -0.8422996401786804, -1.0342618227005005, 0.43006372451782227, 0.457163542509079, -0.007692253682762384, 0.6578823924064636, 0.29104289412498474, 0.1044992208480835, 0.8174745440483093, -0.41029083728790283, 0.11151859164237976, -0.3967142403125763, 1.6827397346496582, -0.11591384559869766, -0.3852391242980957, 0.4164341688156128, 1.2350176572799683, -1.1760752201080322, -0.9581665992736816, 0.013075119815766811, 0.21197736263275146, 0.1265294998884201, -0.8006851077079773, -0.6647638082504272, -0.3418549597263336, 0.004132628906518221, 0.08254402875900269, 0.46917250752449036, 0.3220785856246948, 0.009289070032536983, -0.1613001972436905, 0.44619297981262207, -0.7906510829925537, 0.20501750707626343, 0.7540888786315918, -0.39966464042663574, -0.7605142593383789, -1.0674166679382324, -0.6413075923919678, -0.5699227452278137, -0.00564119778573513, 0.9150553941726685, -0.22439737617969513, 0.6732641458511353, 1.0460443496704102, -0.20751316845417023, 0.7729240655899048, 0.40624725818634033, 0.474488228559494, 0.7708127498626709, -1.4082119464874268, -1.3385287523269653, -0.31053757667541504, 0.8162187337875366, -0.2204287350177765, 0.3393704891204834, -1.0272529125213623, 0.15142922103405, 0.435712605714798, 0.08363475650548935, -0.22043375670909882, -1.045628309249878, -0.07388618588447571, -1.0223772525787354, -1.111167073249817, -0.5941332578659058, 0.18044596910476685, -0.5745363235473633, 0.40974950790405273, -0.3248538672924042, -0.002401139121502638, -0.3887391686439514, 0.6249955892562866, -0.12381364405155182, -0.06330006569623947, -0.5861718058586121, 0.2653719186782837, -1.1873705387115479, -0.868181586265564, -0.9183186292648315, -0.09664702415466309, -0.013594088144600391, -0.08907108008861542, 0.39326369762420654, 0.3674333989620209, -0.4595962464809418, 0.4614013433456421, -0.030075520277023315, 0.5304797887802124, -0.3084731996059418, -0.6425951719284058, -0.03519127890467644, 0.4781109690666199, -0.7404011487960815, 0.7385441660881042, 0.29490795731544495, -0.8042888045310974, 0.6190452575683594, -0.32212311029434204, -0.2918432056903839, -0.10079975426197052, -0.30388176441192627, 0.3617568612098694, 0.16157713532447815, -0.5780479311943054, -0.24274952709674835, 0.17414730787277222, -0.7525915503501892, 0.6019430160522461, -0.5818277597427368, -0.006729683373123407, -0.01362722460180521, 0.3841826021671295, 0.09763815999031067, 0.5257430076599121, -0.25094568729400635, 0.7015003561973572, -0.18752889335155487, 0.4890705347061157, 0.3156334459781647, -0.13423152267932892, 0.2372988760471344, 0.41437721252441406, -0.40554606914520264, -0.3498794436454773, 0.49544385075569153, 0.6814410090446472, 0.0037771984934806824, 0.3118864595890045, -1.3336122035980225, -0.3666972815990448, -0.44483932852745056, -0.1685517132282257, 0.08299501985311508, 0.6577016711235046, -0.14439430832862854, -0.09040016680955887, -0.022149017080664635, -0.2609271705150604, -1.060257077217102, 0.7998024225234985, -1.7627041339874268, -0.8697611093521118, 0.7481366991996765, -0.7453661561012268, 0.01587851345539093, -0.355249285697937, -1.0438849925994873, 0.5032012462615967, -0.029243020340800285, -0.42805835604667664, 0.15999819338321686, 0.2913771867752075, 0.22806930541992188, 1.2299745082855225, -0.9783389568328857, 0.3884297013282776, -0.16186098754405975, 1.2904350757598877, -0.42305678129196167, -0.9261893033981323, 0.31489038467407227, 1.141723394393921, 0.5541069507598877, 0.3660048544406891, -0.27991393208503723, 0.179663747549057, -0.15781386196613312, 0.6354597806930542, -0.0529550202190876, -0.41499143838882446, -0.4101307690143585, 0.9277324080467224, -0.2161259651184082, 0.3995858132839203, -0.8807416558265686, 0.6788401007652283, 0.2870974540710449, -0.3904704451560974, -0.24454250931739807, 0.9473057389259338, 0.6321347951889038, -0.394137978553772, 0.2837771475315094, 0.42955854535102844, 0.5822681188583374, 0.03700225055217743, -0.5150123834609985, -0.3974699079990387, 0.9278313517570496, 0.5650643706321716, 0.6952468156814575, 0.47052666544914246, 0.4337950646877289, 0.7631087303161621, -0.20886486768722534, -0.2944110035896301, 0.6300909519195557, 0.4817153513431549, -0.7087075114250183, 0.12384025007486343, -0.6257553100585938, 0.0053706481121480465, -0.2224891632795334, 0.14658324420452118, -0.11813188344240189, -0.5435917973518372, 0.06213388592004776, -0.8116087913513184, 0.004409399814903736, 0.8603916168212891, -0.3028428852558136, 0.03451595455408096, 0.05681626498699188, -0.15386834740638733, 0.33709192276000977, 0.7333638668060303, 0.16369377076625824, 0.06659792363643646, 1.1566978693008423, 0.9210981726646423, 0.08371634036302567, 0.5917485952377319, 0.3162928819656372, 0.8348307013511658, -0.4854685962200165, -0.013924620114266872, 0.10706708580255508, -0.03891686350107193, -0.7699523568153381, -0.4383980631828308, -0.7144951224327087, 0.7573999166488647, -0.01595187932252884, -0.26818904280662537, 0.0015660658245906234, -0.7070415019989014, -0.5883757472038269, -0.8603300452232361, 0.09443327784538269, -1.0052694082260132, -0.2287505716085434, 0.6399999856948853, -0.12071622163057327, -0.476035475730896, 1.111099123954773, -0.36033061146736145, 0.898527979850769, 0.24661047756671906, 1.0438309907913208, -0.011510878801345825, 1.4417749643325806, 0.5138086676597595, -0.8879896402359009, 0.037634920328855515, 0.5804992914199829, -0.5673867464065552, -0.8144505023956299, -0.21976038813591003, -0.4047777056694031, -0.6334474086761475, -0.7731560468673706, -0.5014008283615112, -0.8389531970024109, 0.3531281650066376, -0.6880202889442444, -0.9491361379623413, 0.13890786468982697, 0.41012221574783325, -1.1255191564559937, 0.2843640446662903, 0.623284637928009, 1.141279697418213, -0.28763431310653687, -0.7400559782981873, -0.38301995396614075, -0.2340525984764099, -0.0817243903875351, -0.2664516866207123, 0.40692338347435, -0.6981813907623291, -0.5029518008232117, -0.582319974899292, -0.2107168585062027, -0.06603747606277466, -0.35428696870803833, -0.810632586479187, 0.3483106195926666, 0.1869737058877945, 0.7660131454467773, 0.6214496493339539, 0.5466792583465576, -0.9124762415885925, -0.2867825925350189, 0.6102771759033203, 0.94245845079422, 0.5391791462898254, 0.6875285506248474, 1.1377830505371094, -0.4545353651046753, -0.4848543703556061, -0.48632174730300903, 0.9769149422645569, -0.40214279294013977, -0.7078778147697449, -0.07912635803222656, 0.07979979366064072, -0.3853691518306732, -0.8642873764038086, 0.4385763704776764, 0.5180678367614746, -0.4314514994621277, -0.4345359802246094, -1.0999653339385986, -0.0474461205303669, -0.8778736591339111, -0.6053842306137085, -0.6695656776428223, 0.6985814571380615, 0.028900405392050743, -0.5094020962715149, -0.2140420526266098, -0.9747379422187805, 3.5822231769561768, 0.3427112400531769, 0.9777442812919617, 0.08158939331769943, 1.041500449180603, 1.8263189792633057, 0.5922962427139282, 0.3010428547859192, -0.04436859115958214, -0.8792985677719116, 0.06906511634588242, -0.6630662679672241, 0.616479218006134, 0.6739351749420166, -0.15869104862213135, 1.1615214347839355, -0.4391363561153412, -0.10623390227556229, 0.5808461308479309, -1.4216582775115967, -1.0520687103271484, 0.1145118996500969, 0.42858123779296875, 0.9033954739570618, 0.3184102475643158, 0.4746527075767517, 0.5798823237419128, -1.2860640287399292, -0.09270413964986801, -0.5235591530799866, 0.2219018042087555, -0.5637973546981812, 0.9574240446090698, -0.030475154519081116, 0.14224189519882202, 0.8098365068435669, 0.37616515159606934, 0.044196583330631256, -0.014152522198855877, -0.2961471676826477, 0.04171978682279587, -0.5002655982971191, 0.5522216558456421, -0.548897385597229, -0.12452234327793121, 0.8354419469833374, -0.5054560899734497, 0.3904758393764496, 0.8334362506866455, -0.16559620201587677, 0.9190464019775391, -0.7961034178733826, 0.40015751123428345, -0.02543184533715248, -0.5625435709953308, -0.12289844453334808, 0.033525992184877396, -0.5849841237068176, -0.5669596791267395, 0.061335671693086624, 0.2993233799934387, 0.40176066756248474, -0.3430889844894409, 0.2788175344467163, -1.106833577156067, -0.2430945485830307, -0.03726760670542717, 0.7200159430503845, -0.47153913974761963, -0.3804258406162262, -0.5604486465454102, -0.15018439292907715, 0.21956034004688263, -0.5709713101387024, 0.3098151981830597, 0.932934582233429, -0.597590982913971, 0.8235334753990173, 0.34403350949287415, -0.6435809135437012, 0.013282455503940582, -0.1415560394525528, 0.28792044520378113, -0.44602152705192566, 0.487233430147171, 0.4160228371620178, -0.4536564350128174, -0.7125885486602783, -0.5715028047561646, 0.33500227332115173, 0.9613651633262634, 1.2773693799972534, -0.3740025758743286, 0.2984887957572937, 0.31901815533638]]}"
     ]
    }
   ],
   "source": [
    "%%bash -s $APP_NAME\n",
    "\n",
    "APP_NAME=$1\n",
    "\n",
    "cat > ./${APP_NAME}_predictor/instances.json <<END\n",
    "{ \n",
    "   \"instances\": [\n",
    "     { \n",
    "       \"data\": {\n",
    "         \"b64\": \"$(echo 'I am creating an endpoint using TorchServe and HF transformers' | base64 --wrap=0)\"\n",
    "       }\n",
    "     }\n",
    "   ]\n",
    "}\n",
    "END\n",
    "\n",
    "curl -s -X POST \\\n",
    "  -H \"Content-Type: application/json; charset=utf-8\" \\\n",
    "  -d @./${APP_NAME}_predictor/instances.json \\\n",
    "  http://localhost:7080/predictions/$APP_NAME/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1943b33-a4e5-470a-85ed-0a462277fd74",
   "metadata": {},
   "source": [
    "3. Run load test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9d0b6b10-568c-4ea8-9efa-875cf8971566",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = pipe.tokenizer.max_len_single_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "40a8a1fc-ecc7-413b-9fb4-bee61a91ec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s $APP_NAME $MAX_SEQ_LEN\n",
    "\n",
    "APP_NAME=$1\n",
    "MAX_SEQ_LEN=$2\n",
    "\n",
    "cat > ./locustfile.py << END\n",
    "\n",
    "import json\n",
    "from base64 import b64encode\n",
    "from locust import HttpUser, task\n",
    "\n",
    "\n",
    "class BulkEncodeUser(HttpUser):\n",
    "    \n",
    "    @task\n",
    "    def encode_text(self):\n",
    "        test_input = \"hello \" * $MAX_SEQ_LEN\n",
    "        instances = {\n",
    "            \"instances\": [\n",
    "                {\"data\": {\"b64\": b64encode(f\"{test_input}\".encode()).decode(\"utf-8\")}}\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        self.client.post(\n",
    "            \"/predictions/${APP_NAME}/\",\n",
    "            headers={\"Content-Type\": \"application/json; charset=utf-8\"},\n",
    "            json=instances,\n",
    "        )\n",
    "END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "39041efc-1ec1-42ae-9cdf-f181da7d581a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_TEST_RESULTS_DIR = \"./load-test-results\"\n",
    "os.makedirs(LOAD_TEST_RESULTS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f42871a-4418-42d2-b0c3-e4170e0f47ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "!locust \\\n",
    "    --headless \\\n",
    "    --users 100 \\\n",
    "    --spawn-rate 100 \\\n",
    "    --iterations 1_000 \\\n",
    "    --host http://localhost:7080 \\\n",
    "    --csv=$LOAD_TEST_RESULTS_DIR/results \\\n",
    "    # --only-summary "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8128633d-11fc-43f7-afb2-5016d0a436c2",
   "metadata": {},
   "source": [
    "Notes on BGE-large:\n",
    "- Without batching, we're at 1min 11sec to run 1000 requests at 81% GPU utilization on both GPU's\n",
    "- With batchsize=8, we're at 1min 8sec to run 1000 requests a 60-80% GPU utilization on both GPU's\n",
    "- With batchsize=32, we're at 1min 11sec to run 1000 requests a 60-80% GPU utilization on both GPU's\n",
    "\n",
    "Hmmmm... need to look into this\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f906fc1",
   "metadata": {},
   "source": [
    "3. Stop the container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726bd349",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker stop local_sbert_embedder_optimum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24c5d0c",
   "metadata": {},
   "source": [
    "### Push image Container Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2611ed58",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker push $CUSTOM_PREDICTOR_IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e9565b",
   "metadata": {},
   "source": [
    "### Create model and endpoint to VertexAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba10b3e0",
   "metadata": {},
   "source": [
    "We create a model resource on Vertex AI and deploy the model to a Vertex AI Endpoints. You must deploy a model to an endpoint before using the model. The deployed model runs the custom container image to serve predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b99644",
   "metadata": {},
   "source": [
    "**Initialize the Vertex AI SDK for Python**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f00242",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e418380",
   "metadata": {},
   "source": [
    "**Create a Model resource with custom serving container**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170d5ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = 1\n",
    "model_display_name = f\"{APP_NAME}-v{VERSION}\"\n",
    "model_description = \"PyTorch based sentence transformers embedder with custom container\"\n",
    "\n",
    "MODEL_NAME = APP_NAME\n",
    "health_route = \"/ping\"\n",
    "predict_route = f\"/predictions/{MODEL_NAME}\"\n",
    "serving_container_ports = [7080]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eab0c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = aiplatform.Model.upload(\n",
    "    display_name=model_display_name,\n",
    "    description=model_description,\n",
    "    serving_container_image_uri=CUSTOM_PREDICTOR_IMAGE_URI,\n",
    "    serving_container_predict_route=predict_route,\n",
    "    serving_container_health_route=health_route,\n",
    "    serving_container_ports=serving_container_ports,\n",
    ")\n",
    "\n",
    "model.wait()\n",
    "\n",
    "print(model.display_name)\n",
    "print(model.resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7343cea4",
   "metadata": {},
   "source": [
    "**Create an Endpoint for Model with Custom Container**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3863a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_display_name = f\"{APP_NAME}-endpoint\"\n",
    "endpoint = aiplatform.Endpoint.create(display_name=endpoint_display_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea432646",
   "metadata": {},
   "source": [
    "**Deploy the Model to Endpoint**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b772534",
   "metadata": {},
   "source": [
    "See more on the [documentation](https://cloud.google.com/vertex-ai/docs/predictions/deploy-model-api)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52b599d",
   "metadata": {},
   "source": [
    "To select the right machine type according to your budget select go to [Google Cloud Pricing Calculator](https://cloud.google.com/products/calculator) and [Finding the ideal machine type](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#finding_the_ideal_machine_type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecad0cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_percentage = 100\n",
    "machine_type = \"n1-standard-8\"\n",
    "deployed_model_display_name = model_display_name\n",
    "min_replica_count = 1\n",
    "max_replica_count = 3\n",
    "sync = True\n",
    "\n",
    "model.deploy(\n",
    "    endpoint=endpoint,\n",
    "    deployed_model_display_name=deployed_model_display_name,\n",
    "    machine_type=machine_type,\n",
    "    traffic_percentage=traffic_percentage,\n",
    "    min_replica_count=min_replica_count,\n",
    "    max_replica_count=max_replica_count,\n",
    "    sync=sync,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1add143",
   "metadata": {},
   "source": [
    "### Invoking the Endpoint with deployed Model using Vertex AI SDK to make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49af35f0",
   "metadata": {},
   "source": [
    "**Get the endpoint id**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a405988",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_display_name = f\"{APP_NAME}-endpoint\"\n",
    "filter = f'display_name=\"{endpoint_display_name}\"'\n",
    "\n",
    "for endpoint_info in aiplatform.Endpoint.list(filter=filter):\n",
    "    print(\n",
    "        f\"Endpoint display name = {endpoint_info.display_name} resource id ={endpoint_info.resource_name} \"\n",
    "    )\n",
    "\n",
    "endpoint = aiplatform.Endpoint(endpoint_info.resource_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c937f2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a59ee57",
   "metadata": {},
   "source": [
    "**Formatting input for online prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cf7c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_instances = [\n",
    "    b\"This is an example of model deployment using a sentence transformers model and optimum\",\n",
    "]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f260939",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer(test_instances[0])[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0268a7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a19a344",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"=\" * 100)\n",
    "for instance in test_instances:\n",
    "    print(f\"Input text: \\n\\t{instance.decode('utf-8')}\\n\")\n",
    "    b64_encoded = base64.b64encode(instance)\n",
    "    test_instance = [{\"data\": {\"b64\": f\"{str(b64_encoded.decode('utf-8'))}\"}}]\n",
    "    print(f\"Formatted input: \\n{json.dumps(test_instance, indent=4)}\\n\")\n",
    "    prediction = endpoint.predict(instances=test_instance)\n",
    "    #print(f\"Prediction response: \\n\\t{prediction}\")\n",
    "    print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1a6d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "prediction = endpoint.predict(instances=test_instance)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.2-0.m110",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.2-0:m110"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
