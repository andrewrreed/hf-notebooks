{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc6e5f2b",
   "metadata": {},
   "source": [
    "# Deploying a Hugging Face model to Google Vertex AI for Bulk Embedding Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dbbda7",
   "metadata": {},
   "source": [
    "Inspired by the [GCP tutorial]( https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/community-content/pytorch_text_classification_using_vertex_sdk_and_gcloud/pytorch-text-classification-vertex-ai-train-tune-deploy.ipynb) we will deploy a `sentence-transformers` model on a [Vertex AI](https://cloud.google.com/vertex-ai/docs/predictions/deploy-model-api) endpoint. We will use [TorchServe](https://pytorch.org/serve/) to serve a Hugging Face model available on the [Hub](hf.co). To accelerate inference we will also use features from the `optimum` [library](https://github.com/huggingface/optimum) to apply graph optimization and/or quantization to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222da492-d787-42dd-bb80-f59c3dc97bf7",
   "metadata": {},
   "source": [
    "**Resources**:\n",
    "   - On batching:\n",
    "        - https://huggingface.co/docs/transformers/main_classes/pipelines#pipeline-batching\n",
    "   - On TorchServe:\n",
    "        - https://github.com/pytorch/serve/tree/master/examples/Huggingface_Transformers\n",
    "   - On Locust:\n",
    "        - https://medium.com/@tferreiraw/performing-load-tests-with-python-locust-io-62de7d91eebd\n",
    "        - https://medium.com/@ashmi_banerjee/3-step-tutorial-to-performance-test-ml-serving-apis-using-locust-and-fastapi-40e6cc580adc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854097a7",
   "metadata": {},
   "source": [
    "### Set up your local development environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0c5ae2",
   "metadata": {},
   "source": [
    "1. Follow the Google Cloud guide to [setting up a Python development environment](https://cloud.google.com/python/docs/setup) \n",
    "2. [Install and initialize the Cloud SDK.](https://cloud.google.com/sdk/docs/) \n",
    "3. Create a virtual environment (virtualenv, pyenv) with Python 3 (<3.9) and activate the environment\n",
    "4. Launch jupyter notebook from this environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7626c33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://cloud.google.com/products/calculator/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41229c4d",
   "metadata": {},
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e0237e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip -q install --upgrade google-cloud-aiplatform #Vertex AI sdk\n",
    "!pip -q install --upgrade transformers, datasets, locust, locust-plugins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b7143d",
   "metadata": {},
   "source": [
    "### Set up your Google Cloud project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445b5bee",
   "metadata": {},
   "source": [
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager)\n",
    "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project)\n",
    "1. Enable following APIs in your project required for running the tutorial\n",
    "    - [Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com)\n",
    "    - [Cloud Storage API](https://console.cloud.google.com/flows/enableapi?apiid=storage.googleapis.com)\n",
    "    - [Container Registry API](https://console.cloud.google.com/flows/enableapi?apiid=containerregistry.googleapis.com)\n",
    "    - [Cloud Build API](https://console.cloud.google.com/flows/enableapi?apiid=cloudbuild.googleapis.com)\n",
    "   \n",
    "1. Enter your project ID in the cell below. Then run the cell to make sure the Cloud SDK uses the right project for all the commands in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bc4202",
   "metadata": {},
   "source": [
    "### Authenticate to gcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0bcad2",
   "metadata": {},
   "source": [
    " 1. In the Cloud Console, go to the [**Create service account key** page](https://console.cloud.google.com/apis/credentials/serviceaccountkey).,\n",
    " 2. Click **Create service account**.,\n",
    " 3. In the **Service account name** field, enter a name, and click **Create**,\n",
    " 4. In the **Grant this service account access to project** section, click the **Role** drop-down list. Type \\\"Vertex AI\\\" into the filter box, and select **Vertex AI Administrator**. Type \\\"Storage Object Admin\\\" into the filter box, and select **Storage Object Admin**.\n",
    " 5. Click *Create*. A JSON file that contains your key downloads to your local environment.\n",
    " 6. Enter the path to your service account key as the `GOOGLE_APPLICATION_CREDENTIALS` variable in the cell below and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee9cbff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env GOOGLE_APPLICATION_CREDENTIALS ./keys/huggingface-ml-e974975230cc.json #change to your service account key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a4d2a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID:  huggingface-ml\n"
     ]
    }
   ],
   "source": [
    "# Get your Google Cloud project ID using google.auth\n",
    "import google.auth\n",
    "\n",
    "_, PROJECT_ID = google.auth.default()\n",
    "print(\"Project ID: \", PROJECT_ID)\n",
    "\n",
    "#Or set it yourself manually\n",
    "# PROJECT_ID = \"huggingface-ml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01857fb5",
   "metadata": {},
   "source": [
    "### Create a cloud storage bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cb4c6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"gs://andrew-reed-bucket\"  # <---CHANGE THIS TO YOUR BUCKET\n",
    "REGION = \"us-east4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b366365",
   "metadata": {},
   "source": [
    "**If the bucket doesn't exist, run the following:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9c1fb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! gsutil mb -l $REGION $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd343f3",
   "metadata": {},
   "source": [
    "Access the content of the bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20adc9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827daccf",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d25c6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import base64\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import transformers\n",
    "\n",
    "import google.auth\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import gapic as aip\n",
    "from google.protobuf.json_format import MessageToDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5e61db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook runtime: GPU\n",
      "PyTorch version : 2.0.1+cu118\n",
      "Transformers version : 4.32.1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Notebook runtime: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(f\"PyTorch version : {torch.__version__}\")\n",
    "print(f\"Transformers version : {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4818dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_NAME = \"test_bge_embedder\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed45919e",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841c6e37",
   "metadata": {},
   "source": [
    "#### *Overview*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e05d7bb",
   "metadata": {},
   "source": [
    "Deploying a PyTorch model on [Vertex AI Predictions](https://cloud.google.com/vertex-ai/docs/predictions/getting-predictions) requires to use a custom container that serves online predictions. You will deploy a container running [PyTorch's TorchServe](https://pytorch.org/serve/) tool in order to serve predictions from a fine-tuned sentence transformer model `msmarco-distilbert-base-tas-b` available in [Hugging Face Transformers](https://huggingface.co/sentence-transformers/msmarco-distilbert-base-tas-b). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b69d42",
   "metadata": {},
   "source": [
    "Essentially, to deploy a PyTorch model on Vertex AI Predictions following are the steps:\n",
    "1. Package the trained model artifacts including [default](https://pytorch.org/serve/#default-handlers) or [custom](https://pytorch.org/serve/custom_service.html) handlers by creating an archive file using [Torch model archiver](https://github.com/pytorch/serve/tree/master/model-archiver),\n",
    "2. Build a [custom container](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements) compatible with Vertex AI Predictions to serve the model using Torchserve\n",
    "3. Upload the model with custom container image to serve predictions as a Vertex AI Model resource,\n",
    "4. Create a Vertex AI Endpoint and [deploy the model](https://cloud.google.com/vertex-ai/docs/predictions/deploy-model-api) resource"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035c60c5",
   "metadata": {},
   "source": [
    "### Save model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3372ad06-ee72-46fd-ae1b-e3195ab505c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = \"./\"+APP_NAME+\"_predictor\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0825e6ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_id = \"BAAI/bge-small-en\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModel.from_pretrained(model_id)\n",
    "\n",
    "pt_save_directory = os.path.join(MODEL_DIR, \"model\")\n",
    "\n",
    "tokenizer.save_pretrained(pt_save_directory)\n",
    "model.save_pretrained(pt_save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65cd329a-1385-451c-a7e4-7c0228f642df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def cls_pooling(pipeline_output):\n",
    "    \"\"\"\n",
    "    Return the [CLS] token embedding\n",
    "    \"\"\"\n",
    "    return [_h[0][0] for _h in pipeline_output]\n",
    "        \n",
    "        \n",
    "tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)\n",
    "model = AutoModel.from_pretrained(pt_save_directory)\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "pipe = pipeline(\"feature-extraction\", model=model, tokenizer=tokenizer, batch_size=BATCH_SIZE)\n",
    "\n",
    "input_texts = [\"I like the new ORT pipeline\", \"blah blah blah\", \"Hello, I'm andrew\"]\n",
    "embeddings = cls_pooling(pipe(input_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17a81e9d-c0da-466d-ab51-6a700247e746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 384)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings), len(embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d1b219",
   "metadata": {},
   "source": [
    "### Create a custom model handler "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2eb813",
   "metadata": {},
   "source": [
    "Please refer to the [TorchServe documentation](https://pytorch.org/serve/custom_service.html) for defining a custom handler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8d0f6b68-9e9f-4061-a89e-f66025c1569a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use int for value, False to disable batching\n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9de29252",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s $APP_NAME $BATCH_SIZE\n",
    "\n",
    "# %%writefile test_bge_embedder_predictor/custom_handler.py\n",
    "APP_NAME=$1\n",
    "BATCH_SIZE=$2\n",
    "\n",
    "cat << EOF > ./${APP_NAME}_predictor/custom_handler.py\n",
    "\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, pipeline\n",
    "\n",
    "from ts.torch_handler.base_handler import BaseHandler\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class SentenceTransformersHandler(BaseHandler):\n",
    "    \"\"\"\n",
    "    The handler takes an input string and returns the embedding \n",
    "    based on the serialized transformers checkpoint.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(SentenceTransformersHandler, self).__init__()\n",
    "        self.initialized = False\n",
    "        self.batch_size = $BATCH_SIZE\n",
    "\n",
    "    def initialize(self, ctx):\n",
    "        \"\"\" Loads the model.onnx file and initialized the model object.\n",
    "        Instantiates Tokenizer for preprocessor to use and a feature extraction pipeline\n",
    "        \"\"\"\n",
    "        self.manifest = ctx.manifest\n",
    "\n",
    "        properties = ctx.system_properties\n",
    "        model_dir = properties.get(\"model_dir\")\n",
    "        self.device = torch.device(\"cuda:\" + str(properties.get(\"gpu_id\")) if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Read model serialize/pt file\n",
    "        serialized_file = self.manifest[\"model\"][\"serializedFile\"]\n",
    "        model_pt_path = os.path.join(model_dir, serialized_file)\n",
    "        if not os.path.isfile(model_pt_path):\n",
    "            raise RuntimeError(\"Missing the model.onnx or pytorch_model.bin file\")\n",
    "        \n",
    "        # Load model\n",
    "        self.model = AutoModel.from_pretrained(model_dir)\n",
    "        logger.debug('Transformer model from path {0} loaded successfully'.format(model_dir))\n",
    "        logger.info(f\"model_pt_path: {model_pt_path}\")\n",
    "        logger.info(f\"model_dir: {model_dir}\")\n",
    "        \n",
    "        # Ensure to use the same tokenizer used during training\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_dir, model_max_length=512)\n",
    "        \n",
    "        # Create an optimum pipeline\n",
    "        # Use BetterTransformer for fused kernel + sparsity optimizations\n",
    "        # https://medium.com/pytorch/bettertransformer-out-of-the-box-performance-for-huggingface-transformers-3fbe27d50ab2\n",
    "        if self.batch_size:\n",
    "            logger.info(f\"----Loading pipeline with batch_size={self.batch_size}\")\n",
    "            self.pipeline = pipeline(\"feature-extraction\",\n",
    "                                     model=self.model, \n",
    "                                     tokenizer=self.tokenizer, \n",
    "                                     device=self.device, \n",
    "                                     accelerator=\"bettertransformer\",\n",
    "                                     batch_size=self.batch_size)\n",
    "        else:\n",
    "            logger.info(f\"----Loading pipeline without batching\")\n",
    "            self.pipeline = pipeline(\"feature-extraction\",\n",
    "                                     model=self.model, \n",
    "                                     tokenizer=self.tokenizer, \n",
    "                                     device=self.device, \n",
    "                                     accelerator=\"bettertransformer\")\n",
    "\n",
    "        self.initialized = True\n",
    "\n",
    "    def preprocess(self, requests):\n",
    "        \"\"\" Preprocessing input request by tokenizing\n",
    "            Extend with your own preprocessing steps as needed\n",
    "            \n",
    "            [{'data': b'I am creating an endpoint using TorchServe and HF transformers\\n'}]\n",
    "        \"\"\"\n",
    "        # print(f'{\"---\"*20}')\n",
    "        # print(requests)\n",
    "        # print(type(requests))\n",
    "        # print(len(requests))\n",
    "        # print(f'{\"---\"*20}')\n",
    "        \n",
    "        input_texts = []\n",
    "        for idx, request in enumerate(requests):\n",
    "            text = request.get(\"data\")\n",
    "            if text is None:\n",
    "                text = request.get(\"body\")\n",
    "                \n",
    "            text = text.decode('utf-8')\n",
    "            input_texts.append(text)\n",
    "        logger.info(\"Received text: '%s'\", input_texts)\n",
    "        \n",
    "        return input_texts\n",
    "\n",
    "    def inference(self, input_texts):\n",
    "        \"\"\" Predict the class of a text using a trained transformer model.\n",
    "        \"\"\"\n",
    "        \n",
    "        def cls_pooling(pipeline_output):\n",
    "            \"\"\"\n",
    "            Return the [CLS] token embedding\n",
    "            \"\"\"\n",
    "            return [_h[0][0] for _h in pipeline_output]\n",
    "        \n",
    "        embeddings = cls_pooling(self.pipeline(input_texts))\n",
    "\n",
    "        logger.info(f\"Model embedded: {len(embeddings)}\")\n",
    "        return embeddings\n",
    "\n",
    "    def postprocess(self, inference_output):\n",
    "        return inference_output\n",
    "    \n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f44f516",
   "metadata": {},
   "source": [
    "### Create custom container image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fb4a9a",
   "metadata": {},
   "source": [
    "**Create a Dockerfile with TorchServe as base image**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbb394c",
   "metadata": {},
   "source": [
    "**NB**: to define the right Torchserve parameters such as `workers` please consult (https://github.com/pytorch/serve/blob/master/docs/performance_guide.md) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ca79194d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./test_bge_embedder_predictor/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%bash -s $APP_NAME $BATCH_SIZE\n",
    "\n",
    "APP_NAME=$1\n",
    "BATCH_SIZE=$2\n",
    "\n",
    "cat << EOF > ./${APP_NAME}_predictor/Dockerfile\n",
    "\n",
    "FROM pytorch/torchserve:latest-gpu\n",
    "\n",
    "# install dependencies\n",
    "RUN python3 -m pip install --upgrade pip\n",
    "RUN pip3 install transformers\n",
    "\n",
    "USER model-server\n",
    "\n",
    "# copy model artifacts, custom handler and other dependencies\n",
    "COPY custom_handler.py /home/model-server/\n",
    "COPY ./model/ / /home/model-server/\n",
    "\n",
    "# create torchserve configuration file\n",
    "USER root\n",
    "RUN printf \"\\nservice_envelope=json\" >> /home/model-server/config.properties\n",
    "RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home/model-server/config.properties\n",
    "RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /home/model-server/config.properties\n",
    "RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /home/model-server/config.properties\n",
    "RUN if [ \"$BATCH_SIZE\" = False ]; then \\\n",
    "        : \\\n",
    "        else \\\n",
    "        printf '\\nmodels={\\\n",
    "          \"$APP_NAME\": {\\\n",
    "            \"1.0\": {\\\n",
    "                \"defaultVersion\": true,\\\n",
    "                \"marName\": \"$APP_NAME.mar\",\\\n",
    "                \"minWorkers\": 1,\\\n",
    "                \"maxWorkers\": 8,\\\n",
    "                \"batchSize\": \"$BATCH_SIZE\",\\\n",
    "                \"maxBatchDelay\": 10000,\\\n",
    "                \"responseTimeout\": 20000\\\n",
    "            }\\\n",
    "          }}' >> /home/model-server/config.properties; \\\n",
    "    fi\n",
    "    \n",
    "# expose health and prediction listener ports from the image\n",
    "EXPOSE 7080\n",
    "EXPOSE 7081\n",
    "\n",
    "# create model archive file packaging model artifacts and dependencies\n",
    "RUN torch-model-archiver -f \\\n",
    "  --model-name=$APP_NAME \\\n",
    "  --version=1.0 \\\n",
    "  --serialized-file=/home/model-server/pytorch_model.bin \\\n",
    "  --handler=/home/model-server/custom_handler.py \\\n",
    "  --extra-files \"/home/model-server/config.json,/home/model-server/tokenizer.json,/home/model-server/tokenizer_config.json,/home/model-server/special_tokens_map.json,/home/model-server/vocab.txt\" \\\n",
    "  --export-path=/home/model-server/model-store\n",
    "\n",
    "# run Torchserve HTTP serve to respond to prediction requests\n",
    "CMD [\"torchserve\", \\\n",
    "     \"--start\", \\\n",
    "     \"--ts-config=/home/model-server/config.properties\", \\\n",
    "     \"--models\", \\\n",
    "     \"$APP_NAME=$APP_NAME.mar\", \\\n",
    "     \"--model-store\", \\\n",
    "     \"/home/model-server/model-store\"]\n",
    "\n",
    "EOF\n",
    "\n",
    "echo \"Writing ./${APP_NAME}_predictor/Dockerfile\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "cc89ea12-0e5e-4308-863c-5aae33058f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_DIR = APP_NAME+\"_predictor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "98567898-71ce-4d2b-9f86-d9565973c1b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('test_bge_embedder', 'test_bge_embedder_predictor')"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "APP_NAME, APP_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca594d5b",
   "metadata": {},
   "source": [
    "**Build container**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e2b160e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUSTOM_PREDICTOR_IMAGE_URI = gcr.io/huggingface-ml/pytorch_predict_test_bge_embedder_256\n"
     ]
    }
   ],
   "source": [
    "CUSTOM_PREDICTOR_IMAGE_URI = f\"gcr.io/{PROJECT_ID}/pytorch_predict_{APP_NAME}_{BATCH_SIZE}\"\n",
    "print(f\"CUSTOM_PREDICTOR_IMAGE_URI = {CUSTOM_PREDICTOR_IMAGE_URI}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "7d09ce47",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  134.5MB\n",
      "Step 1/16 : FROM pytorch/torchserve:latest-gpu\n",
      " ---> 04eef250c14e\n",
      "Step 2/16 : RUN python3 -m pip install --upgrade pip\n",
      " ---> Using cache\n",
      " ---> 2f06486ed62f\n",
      "Step 3/16 : RUN pip3 install transformers\n",
      " ---> Using cache\n",
      " ---> 4ed472215157\n",
      "Step 4/16 : USER model-server\n",
      " ---> Using cache\n",
      " ---> 4fa0cd41a25e\n",
      "Step 5/16 : COPY custom_handler.py /home/model-server/\n",
      " ---> Using cache\n",
      " ---> 1b9ecff8e2d6\n",
      "Step 6/16 : COPY ./model/ / /home/model-server/\n",
      " ---> 0f96520e10a9\n",
      "Step 7/16 : USER root\n",
      " ---> Running in 2db035d6ef4c\n",
      "Removing intermediate container 2db035d6ef4c\n",
      " ---> ade06f5faf45\n",
      "Step 8/16 : RUN printf \"\\nservice_envelope=json\" >> /home/model-server/config.properties\n",
      " ---> Running in f134d2739a8a\n",
      "Removing intermediate container f134d2739a8a\n",
      " ---> b5dc7c5e1841\n",
      "Step 9/16 : RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home/model-server/config.properties\n",
      " ---> Running in 9a0311739d66\n",
      "Removing intermediate container 9a0311739d66\n",
      " ---> 165e98b2eb15\n",
      "Step 10/16 : RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /home/model-server/config.properties\n",
      " ---> Running in cc7240a9202e\n",
      "Removing intermediate container cc7240a9202e\n",
      " ---> f8a3561a9c3a\n",
      "Step 11/16 : RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /home/model-server/config.properties\n",
      " ---> Running in 286881802b8a\n",
      "Removing intermediate container 286881802b8a\n",
      " ---> 302c48cc640e\n",
      "Step 12/16 : RUN if [ \"256\" = False ]; then         :         else         printf '\\nmodels={          \"test_bge_embedder\": {            \"1.0\": {                \"defaultVersion\": true,                \"marName\": \"test_bge_embedder.mar\",                \"minWorkers\": 1,                \"maxWorkers\": 8,                \"batchSize\": \"256\",                \"maxBatchDelay\": 10000,                \"responseTimeout\": 20000            }          }}' >> /home/model-server/config.properties;     fi\n",
      " ---> Running in 3fdcd193bb85\n",
      "Removing intermediate container 3fdcd193bb85\n",
      " ---> 032f9ebfc42c\n",
      "Step 13/16 : EXPOSE 7080\n",
      " ---> Running in 04d1ea3abb91\n",
      "Removing intermediate container 04d1ea3abb91\n",
      " ---> 632502f9b94f\n",
      "Step 14/16 : EXPOSE 7081\n",
      " ---> Running in c288358809a5\n",
      "Removing intermediate container c288358809a5\n",
      " ---> ca6ffd0ac7ba\n",
      "Step 15/16 : RUN torch-model-archiver -f   --model-name=test_bge_embedder   --version=1.0   --serialized-file=/home/model-server/pytorch_model.bin   --handler=/home/model-server/custom_handler.py   --extra-files \"/home/model-server/config.json,/home/model-server/tokenizer.json,/home/model-server/tokenizer_config.json,/home/model-server/special_tokens_map.json,/home/model-server/vocab.txt\"   --export-path=/home/model-server/model-store\n",
      " ---> Running in affce2c6a2f9\n",
      "Removing intermediate container affce2c6a2f9\n",
      " ---> 1cef3ff41846\n",
      "Step 16/16 : CMD [\"torchserve\",      \"--start\",      \"--ts-config=/home/model-server/config.properties\",      \"--models\",      \"test_bge_embedder=test_bge_embedder.mar\",      \"--model-store\",      \"/home/model-server/model-store\"]\n",
      " ---> Running in 959bd0ac1ba9\n",
      "Removing intermediate container 959bd0ac1ba9\n",
      " ---> a34d7c70a65b\n",
      "Successfully built a34d7c70a65b\n",
      "Successfully tagged gcr.io/huggingface-ml/pytorch_predict_test_bge_embedder_256:latest\n"
     ]
    }
   ],
   "source": [
    "!docker build \\\n",
    "  --tag=$CUSTOM_PREDICTOR_IMAGE_URI \\\n",
    "  ./$APP_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97df6271",
   "metadata": {},
   "source": [
    "**Test API locally**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7116e868-91d2-4f29-b614-b43eb3474735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_bge_embedder-256\n"
     ]
    }
   ],
   "source": [
    "!echo $APP_NAME-$BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c284be67-6783-4237-bc08-7b90f73dcbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6c0e75d4f25d976dad028c6da363f0e844582250b8b9fcb92272d8f9509a1849\n"
     ]
    }
   ],
   "source": [
    "!docker run -td --rm -p 7080:7080 --gpus all --name=$APP_NAME-$BATCH_SIZE $CUSTOM_PREDICTOR_IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "10700a96-9583-4127-85d9-270640cc4053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61c620e7db2d97a2ca92adaf17cd7b5f0f2d3f5e0eee14840f17fbcf2893434f\n"
     ]
    }
   ],
   "source": [
    "!docker run -td --rm -p 7080:7080 --gpus all --name=test_bge_embedder gcr.io/huggingface-ml/pytorch_predict_test_bge_embedder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4816b243",
   "metadata": {},
   "source": [
    "1. Health check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f66cc11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"status\": \"Healthy\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!curl http://localhost:7080/ping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd925c3f-d3a4-44f4-b52e-62a06de2f571",
   "metadata": {},
   "source": [
    "2. Send request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "2dcbddc7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"predictions\": [[-0.6905803084373474, 0.009051576256752014, 0.30402737855911255, -0.3663185238838196, -0.07281982898712158, 0.5289543271064758, -0.35053467750549316, 0.05200103297829628, -0.20036430656909943, 0.19550664722919464, 0.07047154009342194, -0.9721583724021912, 0.09480097144842148, 0.32743242383003235, -0.07829789817333221, 0.25278550386428833, -0.24017898738384247, 0.42118600010871887, -0.7957032322883606, 0.09498923271894455, 0.6807821393013, -0.4031156897544861, -0.10545552521944046, -0.22044122219085693, -0.2215351164340973, 0.35518744587898254, -0.21126778423786163, -0.18924182653427124, -0.31485188007354736, -1.508244276046753, 0.051057118922472, -0.4146045744419098, -0.031520672142505646, -0.1402014195919037, -0.15924163162708282, -0.3663807213306427, -0.2124873250722885, 0.09935609251260757, -0.26911237835884094, 0.23719154298305511, 0.3093794584274292, -0.13422630727291107, -0.16769321262836456, -0.38768434524536133, -0.3415384888648987, -0.983018696308136, -0.1300000697374344, 0.035219643265008926, 0.2488921731710434, -0.05626659095287323, 0.16235129535198212, -0.17184293270111084, 0.2743591368198395, 0.14688877761363983, 0.16204729676246643, 0.5731976628303528, 0.5477639436721802, 0.48827478289604187, 0.2619783282279968, -0.12520566582679749, 0.3408348560333252, 0.3800036907196045, -1.8990272283554077, 0.3012057840824127, -0.03124055825173855, 0.24463579058647156, 0.09156738221645355, -0.38220861554145813, 0.25245603919029236, 0.18909262120723724, -0.3024997115135193, -0.1510433703660965, 0.03441021218895912, 0.5986877083778381, -0.13345350325107574, -0.25075802206993103, -0.09011444449424744, 0.21275849640369415, 0.4769946038722992, -0.2007867395877838, -0.34030336141586304, -0.23320145905017853, 0.03756546974182129, 0.2565811574459076, -0.15849576890468597, -0.1134718582034111, 0.4837566912174225, -0.13046951591968536, 0.4448842406272888, 0.21901051700115204, -0.06748619675636292, -0.31399351358413696, -0.5044843554496765, 0.36631399393081665, -0.3715570271015167, -0.5174586772918701, 0.33711832761764526, 0.04443900287151337, -0.5753383636474609, 4.85839319229126, -0.5534889698028564, -0.24287796020507812, 0.1997729241847992, -0.1246413141489029, -0.049446333199739456, 0.08609776943922043, -0.17474216222763062, -0.31267306208610535, -0.2676825225353241, -0.06577201932668686, -0.07566565275192261, -0.36128535866737366, 0.267841100692749, -0.396139919757843, 0.2703481912612915, 0.18481457233428955, -0.03815140575170517, 0.3631405532360077, -0.39869099855422974, 0.12102761119604111, -0.021378876641392708, 0.5012426376342773, 0.14377553761005402, 0.12918484210968018, -0.10920320451259613, -0.15826338529586792, 0.4990377128124237, 0.7614766955375671, 0.07207412272691727, 0.5373900532722473, 0.20268014073371887, 0.5008494853973389, -0.5996555089950562, -0.45111551880836487, 0.1431288719177246, 0.4331156313419342, 0.08071652799844742, 0.07313856482505798, 0.263150691986084, 0.20631839334964752, -0.2714863121509552, -0.33874937891960144, -0.3182941675186157, -0.6540637612342834, 0.2297203093767166, 0.624466598033905, -0.4774194061756134, 0.12800592184066772, -0.37394028902053833, -0.14333105087280273, -0.1394859403371811, 0.29510799050331116, -0.05422031879425049, -0.17905154824256897, -0.10885220021009445, 0.5635253190994263, 0.0684923380613327, -0.08984469622373581, -0.3183024227619171, 0.23911800980567932, -0.027304287999868393, -0.47829630970954895, 0.032797668129205704, 0.35001978278160095, -0.05388081073760986, -0.7105339169502258, 0.21091784536838531, 0.04772459343075752, -0.1595621407032013, -0.029795365408062935, -0.09053075313568115, 0.3333447575569153, 0.18921343982219696, -0.21328496932983398, 0.8105850219726562, 0.02982020564377308, -0.5322548151016235, -0.2121349275112152, 0.1565578728914261, 0.009341631084680557, 0.1951020061969757, -0.14369302988052368, -0.1962207853794098, -0.002261942019686103, 0.013481528498232365, -0.701908528804779, -0.1908990442752838, -0.5176093578338623, -0.24720850586891174, 0.1676502376794815, -0.7213609218597412, 0.0983167216181755, -0.08080002665519714, -0.029564663767814636, 0.1097061038017273, -0.2364809364080429, -0.06629902869462967, 0.007960138842463493, -0.1249711662530899, -0.10830879211425781, 0.6769337058067322, 0.2936606705188751, -0.3227729797363281, 0.30935847759246826, -0.38060006499290466, 0.10767599940299988, -0.37044715881347656, -0.3428499698638916, 0.16145101189613342, -0.2935033440589905, -0.11119119077920914, -0.06704526394605637, 0.6731573939323425, 0.08023416996002197, -0.2549784183502197, -0.13448160886764526, 0.08535367250442505, 0.24143244326114655, 0.01918051950633526, 0.41071459650993347, -0.060635264962911606, -0.025668276473879814, -0.4048178493976593, -2.558314800262451, -0.17202933132648468, -0.08912386000156403, -0.6787291169166565, 0.20555342733860016, -0.4281938076019287, 0.43874144554138184, -0.18657420575618744, -0.22579039633274078, 0.3259444832801819, 1.3430190086364746, -0.21821030974388123, -0.06889787316322327, -0.3366176187992096, -0.032271627336740494, 0.12820865213871002, 0.18707704544067383, -0.12301583588123322, -0.002961753634735942, 0.13205347955226898, -0.03949243575334549, -0.4613759219646454, 0.4856717586517334, -0.47162678837776184, -0.0853767916560173, -0.062050655484199524, 1.856377124786377, 0.06670694053173065, 0.4016408920288086, -0.25009027123451233, 0.6795856952667236, 0.250704824924469, -0.08032155781984329, -0.5684418678283691, 0.4108579754829407, 0.03967397287487984, 0.4691069722175598, 0.18129217624664307, 0.20706821978092194, -0.18584999442100525, -0.050794437527656555, 0.1672181934118271, 0.24933353066444397, -0.6359446048736572, 0.3700462281703949, -0.3351902365684509, -0.5145006775856018, 0.07919830083847046, -0.3252407908439636, 0.1324295550584793, 0.6383417844772339, -0.36535516381263733, 0.3574879765510559, 0.16224326193332672, 0.20130552351474762, -0.46445223689079285, -0.6480576395988464, -0.18148444592952728, -0.11523744463920593, 0.2541026175022125, -0.01847172901034355, 0.06282106041908264, -0.08921589702367783, -0.47750210762023926, 0.4961772561073303, 0.1701248437166214, 0.26604729890823364, -0.21806463599205017, 0.6039350628852844, -0.3781910538673401, -0.32324931025505066, 0.18124523758888245, 0.1831442415714264, 0.4663654565811157, 0.34507885575294495, 0.32374289631843567, 0.09636509418487549, -0.18869324028491974, -0.04689910635352135, 0.056211210787296295, 0.16690850257873535, -0.27008703351020813, 0.44399362802505493, 0.4042748808860779, 0.15273161232471466, 0.16504937410354614, 0.15878860652446747, -0.1418834924697876, 0.3344329595565796, 0.09912878274917603, -0.20760078728199005, 0.05571828410029411, -0.11689386516809464, 0.03548687696456909, 0.2459520399570465, 0.012280764058232307, -3.0521421432495117, 0.06671539694070816, 0.2481868863105774, 0.20496705174446106, -0.6141988635063171, -0.13891328871250153, 0.09782564640045166, -0.00029758509481325746, -0.6135315895080566, -0.07241324335336685, -0.35925525426864624, 0.3040763735771179, 0.21387378871440887, -0.050022732466459274, 0.2849418520927429, 0.19620037078857422, 0.2702431082725525, -0.1906331330537796, 0.13882660865783691, -0.5899300575256348, 0.19101271033287048, 0.08807346969842911, 2.2126834392547607, -0.11752588301897049, 0.2937745451927185, 0.0012090830132365227, -0.1471361666917801, 0.08150229603052139, 0.2858717739582062, 0.26981693506240845, 0.1498923897743225, 0.31326547265052795, 0.7007961869239807, -0.04671529680490494, 0.3544251024723053, 0.24455270171165466, 0.20149008929729462, 0.019302302971482277, 0.14580614864826202, 0.003431464545428753, 0.23800280690193176, 0.3354092836380005, 0.1066192239522934, -0.041827306151390076, 0.602459192276001, -0.447360098361969, -0.26155996322631836, -0.1297263503074646, 0.2946590185165405, -0.23923297226428986, -0.2766574025154114, 0.22572000324726105, -0.23151253163814545, 0.2573625147342682, -0.010167974047362804, 0.027061456814408302, -0.09208062291145325, -0.02741057425737381, -0.3351157605648041, -0.17219172418117523, 0.4204079508781433, -0.2659534513950348, 0.35753539204597473, -0.3048906922340393, 0.16238227486610413]]}"
     ]
    }
   ],
   "source": [
    "%%bash -s $APP_NAME\n",
    "\n",
    "APP_NAME=$1\n",
    "\n",
    "cat > ./${APP_NAME}_predictor/instances.json <<END\n",
    "{ \n",
    "   \"instances\": [\n",
    "     { \n",
    "       \"data\": {\n",
    "         \"b64\": \"$(echo 'I am creating an endpoint using TorchServe and HF transformers' | base64 --wrap=0)\"\n",
    "       }\n",
    "     }\n",
    "   ]\n",
    "}\n",
    "END\n",
    "\n",
    "curl -s -X POST \\\n",
    "  -H \"Content-Type: application/json; charset=utf-8\" \\\n",
    "  -d @./${APP_NAME}_predictor/instances.json \\\n",
    "  http://localhost:7080/predictions/$APP_NAME/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d33626d6-a3b6-4ef9-80e2-e1fe7f1f06a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test_bge_embedder'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "APP_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "4cc12003-cf66-4ee4-ba3a-13e466ebe262",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3846443997.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[123], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    http://localhost:7080/predictions/$APP_NAME/\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "http://localhost:7080/predictions/test_bge_embedder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1943b33-a4e5-470a-85ed-0a462277fd74",
   "metadata": {},
   "source": [
    "3. Run load test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9d0b6b10-568c-4ea8-9efa-875cf8971566",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = pipe.tokenizer.max_len_single_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "40a8a1fc-ecc7-413b-9fb4-bee61a91ec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s $APP_NAME $MAX_SEQ_LEN\n",
    "\n",
    "APP_NAME=$1\n",
    "MAX_SEQ_LEN=$2\n",
    "\n",
    "cat > ./locustfile.py << END\n",
    "\n",
    "import json\n",
    "from base64 import b64encode\n",
    "from locust import HttpUser, task\n",
    "\n",
    "\n",
    "class BulkEncodeUser(HttpUser):\n",
    "    \n",
    "    @task\n",
    "    def encode_text(self):\n",
    "        test_input = \"hello \" * $MAX_SEQ_LEN\n",
    "        instances = {\n",
    "            \"instances\": [\n",
    "                {\"data\": {\"b64\": b64encode(f\"{test_input}\".encode()).decode(\"utf-8\")}}\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        self.client.post(\n",
    "            \"/predictions/${APP_NAME}/\",\n",
    "            headers={\"Content-Type\": \"application/json; charset=utf-8\"},\n",
    "            json=instances,\n",
    "        )\n",
    "END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "39041efc-1ec1-42ae-9cdf-f181da7d581a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_TEST_RESULTS_DIR = \"./load-test-results\"\n",
    "os.makedirs(LOAD_TEST_RESULTS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8f42871a-4418-42d2-b0c3-e4170e0f47ec",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-01 12:19:07,142] andrew-reed-dev/WARNING/locust.main: System open file limit '4096' is below minimum setting '10000'.\n",
      "It's not high enough for load testing, and the OS didn't allow locust to increase it by itself.\n",
      "See https://github.com/locustio/locust/wiki/Installation#increasing-maximum-number-of-open-files-limit for more info.\n",
      "[2023-09-01 12:19:07,142] andrew-reed-dev/INFO/locust.main: No run time limit set, use CTRL+C to interrupt\n",
      "[2023-09-01 12:19:07,142] andrew-reed-dev/INFO/locust.main: Starting Locust 2.16.1\n",
      "[2023-09-01 12:19:07,143] andrew-reed-dev/INFO/locust.runners: Ramping to 100 users at a rate of 100.00 per second\n",
      "[2023-09-01 12:19:07,153] andrew-reed-dev/INFO/locust.runners: All users spawned: {\"BulkEncodeUser\": 100} (100 total users)\n",
      "[2023-09-01 12:19:21,994] andrew-reed-dev/INFO/root: Iteration limit reached (1000), stopping Users at the start of their next task run\n",
      "[2023-09-01 12:19:23,474] andrew-reed-dev/INFO/root: Last user stopped, quitting runner\n",
      "[2023-09-01 12:19:23,574] andrew-reed-dev/INFO/locust.main: Shutting down (exit code 0)\n",
      "Type     Name  # reqs      # fails |    Avg     Min     Max    Med |   req/s  failures/s\n",
      "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
      "POST     /predictions/test_bge_embedder/    1000     0(0.00%) |   1556     144    2372   1500 |   61.24        0.00\n",
      "--------||-------|-------------|-------|-------|-------|-------|--------|-----------\n",
      "         Aggregated    1000     0(0.00%) |   1556     144    2372   1500 |   61.24        0.00\n",
      "\n",
      "Response time percentiles (approximated)\n",
      "Type     Name      50%    66%    75%    80%    90%    95%    98%    99%  99.9% 99.99%   100% # reqs\n",
      "--------||--------|------|------|------|------|------|------|------|------|------|------|------\n",
      "POST     /predictions/test_bge_embedder/     1500   1600   1600   1600   1700   1800   2100   2200   2400   2400   2400   1000\n",
      "--------||--------|------|------|------|------|------|------|------|------|------|------|------\n",
      "         Aggregated     1500   1600   1600   1600   1700   1800   2100   2200   2400   2400   2400   1000\n",
      "\n",
      "CPU times: user 348 ms, sys: 94.4 ms, total: 443 ms\n",
      "Wall time: 17.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!locust \\\n",
    "    --headless \\\n",
    "    --users 100 \\\n",
    "    --spawn-rate 100 \\\n",
    "    --iterations 1_000 \\\n",
    "    --host http://localhost:7080 \\\n",
    "    --csv=$LOAD_TEST_RESULTS_DIR/results \\\n",
    "    --only-summary "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bf7a80-ef9f-456c-8763-0760753e6ebb",
   "metadata": {},
   "source": [
    "**To access Locust UI from local, forward the port:**\n",
    "\n",
    "gcloud compute ssh --project huggingface-ml --zone us-east4-a andrew-reed-dev -- -L 8080:localhost:8089"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0662b0be-347e-48e7-9441-6f3d73397032",
   "metadata": {},
   "source": [
    "locust \\\n",
    "    --users 1000 \\\n",
    "    --spawn-rate 1000 \\\n",
    "    --iterations 10_000 \\\n",
    "    --host http://localhost:7080 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8128633d-11fc-43f7-afb2-5016d0a436c2",
   "metadata": {},
   "source": [
    "Notes on BGE-large:\n",
    "- Without batching, we're at 1min 11sec to run 1000 requests at 81% GPU utilization on both GPU's\n",
    "- With batchsize=8, we're at 1min 8sec to run 1000 requests a 60-80% GPU utilization on both GPU's\n",
    "- With batchsize=32, we're at 1min 11sec to run 1000 requests a 60-80% GPU utilization on both GPU's\n",
    "\n",
    "Open questions:\n",
    "- I think test results are differentiated because the limiting factor is throughput of requests coming in, not being processed...\n",
    "- How to reliably get overall runtime\n",
    "- How to reliably get max GPU mem utilization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f906fc1",
   "metadata": {},
   "source": [
    "3. Stop the container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726bd349",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker stop local_sbert_embedder_optimum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24c5d0c",
   "metadata": {},
   "source": [
    "### Push image Container Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2611ed58",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker push $CUSTOM_PREDICTOR_IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e9565b",
   "metadata": {},
   "source": [
    "### Create model and endpoint to VertexAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba10b3e0",
   "metadata": {},
   "source": [
    "We create a model resource on Vertex AI and deploy the model to a Vertex AI Endpoints. You must deploy a model to an endpoint before using the model. The deployed model runs the custom container image to serve predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b99644",
   "metadata": {},
   "source": [
    "**Initialize the Vertex AI SDK for Python**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f00242",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e418380",
   "metadata": {},
   "source": [
    "**Create a Model resource with custom serving container**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170d5ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = 1\n",
    "model_display_name = f\"{APP_NAME}-v{VERSION}\"\n",
    "model_description = \"PyTorch based sentence transformers embedder with custom container\"\n",
    "\n",
    "MODEL_NAME = APP_NAME\n",
    "health_route = \"/ping\"\n",
    "predict_route = f\"/predictions/{MODEL_NAME}\"\n",
    "serving_container_ports = [7080]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eab0c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = aiplatform.Model.upload(\n",
    "    display_name=model_display_name,\n",
    "    description=model_description,\n",
    "    serving_container_image_uri=CUSTOM_PREDICTOR_IMAGE_URI,\n",
    "    serving_container_predict_route=predict_route,\n",
    "    serving_container_health_route=health_route,\n",
    "    serving_container_ports=serving_container_ports,\n",
    ")\n",
    "\n",
    "model.wait()\n",
    "\n",
    "print(model.display_name)\n",
    "print(model.resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7343cea4",
   "metadata": {},
   "source": [
    "**Create an Endpoint for Model with Custom Container**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3863a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_display_name = f\"{APP_NAME}-endpoint\"\n",
    "endpoint = aiplatform.Endpoint.create(display_name=endpoint_display_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea432646",
   "metadata": {},
   "source": [
    "**Deploy the Model to Endpoint**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b772534",
   "metadata": {},
   "source": [
    "See more on the [documentation](https://cloud.google.com/vertex-ai/docs/predictions/deploy-model-api)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52b599d",
   "metadata": {},
   "source": [
    "To select the right machine type according to your budget select go to [Google Cloud Pricing Calculator](https://cloud.google.com/products/calculator) and [Finding the ideal machine type](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#finding_the_ideal_machine_type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecad0cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_percentage = 100\n",
    "machine_type = \"n1-standard-8\"\n",
    "deployed_model_display_name = model_display_name\n",
    "min_replica_count = 1\n",
    "max_replica_count = 3\n",
    "sync = True\n",
    "\n",
    "model.deploy(\n",
    "    endpoint=endpoint,\n",
    "    deployed_model_display_name=deployed_model_display_name,\n",
    "    machine_type=machine_type,\n",
    "    traffic_percentage=traffic_percentage,\n",
    "    min_replica_count=min_replica_count,\n",
    "    max_replica_count=max_replica_count,\n",
    "    sync=sync,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1add143",
   "metadata": {},
   "source": [
    "### Invoking the Endpoint with deployed Model using Vertex AI SDK to make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49af35f0",
   "metadata": {},
   "source": [
    "**Get the endpoint id**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a405988",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_display_name = f\"{APP_NAME}-endpoint\"\n",
    "filter = f'display_name=\"{endpoint_display_name}\"'\n",
    "\n",
    "for endpoint_info in aiplatform.Endpoint.list(filter=filter):\n",
    "    print(\n",
    "        f\"Endpoint display name = {endpoint_info.display_name} resource id ={endpoint_info.resource_name} \"\n",
    "    )\n",
    "\n",
    "endpoint = aiplatform.Endpoint(endpoint_info.resource_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c937f2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a59ee57",
   "metadata": {},
   "source": [
    "**Formatting input for online prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cf7c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_instances = [\n",
    "    b\"This is an example of model deployment using a sentence transformers model and optimum\",\n",
    "]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f260939",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer(test_instances[0])[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0268a7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a19a344",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"=\" * 100)\n",
    "for instance in test_instances:\n",
    "    print(f\"Input text: \\n\\t{instance.decode('utf-8')}\\n\")\n",
    "    b64_encoded = base64.b64encode(instance)\n",
    "    test_instance = [{\"data\": {\"b64\": f\"{str(b64_encoded.decode('utf-8'))}\"}}]\n",
    "    print(f\"Formatted input: \\n{json.dumps(test_instance, indent=4)}\\n\")\n",
    "    prediction = endpoint.predict(instances=test_instance)\n",
    "    #print(f\"Prediction response: \\n\\t{prediction}\")\n",
    "    print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1a6d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "prediction = endpoint.predict(instances=test_instance)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.2-0.m110",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.2-0:m110"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
