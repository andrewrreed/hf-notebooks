{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc6e5f2b",
   "metadata": {},
   "source": [
    "# Deploying a Hugging Face model to Google Vertex AI for Bulk Embedding Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dbbda7",
   "metadata": {},
   "source": [
    "Inspired by the [GCP tutorial]( https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/community-content/pytorch_text_classification_using_vertex_sdk_and_gcloud/pytorch-text-classification-vertex-ai-train-tune-deploy.ipynb) we will deploy a `sentence-transformers` model on a [Vertex AI](https://cloud.google.com/vertex-ai/docs/predictions/deploy-model-api) endpoint. We will use [TorchServe](https://pytorch.org/serve/) to serve a Hugging Face model available on the [Hub](hf.co). To accelerate inference we will also use features from the `optimum` [library](https://github.com/huggingface/optimum) to apply graph optimization and/or quantization to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222da492-d787-42dd-bb80-f59c3dc97bf7",
   "metadata": {},
   "source": [
    "**Resources**:\n",
    "   - On batching:\n",
    "        - https://huggingface.co/docs/transformers/main_classes/pipelines#pipeline-batching\n",
    "   - On TorchServe:\n",
    "        - https://github.com/pytorch/serve/tree/master/examples/Huggingface_Transformers\n",
    "   - On Locust:\n",
    "        - https://medium.com/@tferreiraw/performing-load-tests-with-python-locust-io-62de7d91eebd\n",
    "        - https://medium.com/@ashmi_banerjee/3-step-tutorial-to-performance-test-ml-serving-apis-using-locust-and-fastapi-40e6cc580adc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854097a7",
   "metadata": {},
   "source": [
    "### Set up your local development environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0c5ae2",
   "metadata": {},
   "source": [
    "1. Follow the Google Cloud guide to [setting up a Python development environment](https://cloud.google.com/python/docs/setup) \n",
    "2. [Install and initialize the Cloud SDK.](https://cloud.google.com/sdk/docs/) \n",
    "3. Create a virtual environment (virtualenv, pyenv) with Python 3 (<3.9) and activate the environment\n",
    "4. Launch jupyter notebook from this environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7626c33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://cloud.google.com/products/calculator/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41229c4d",
   "metadata": {},
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e0237e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip -q install --upgrade google-cloud-aiplatform #Vertex AI sdk\n",
    "!pip -q install --upgrade transformers, datasets, locust, locust-plugins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b7143d",
   "metadata": {},
   "source": [
    "### Set up your Google Cloud project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445b5bee",
   "metadata": {},
   "source": [
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager)\n",
    "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project)\n",
    "1. Enable following APIs in your project required for running the tutorial\n",
    "    - [Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com)\n",
    "    - [Cloud Storage API](https://console.cloud.google.com/flows/enableapi?apiid=storage.googleapis.com)\n",
    "    - [Container Registry API](https://console.cloud.google.com/flows/enableapi?apiid=containerregistry.googleapis.com)\n",
    "    - [Cloud Build API](https://console.cloud.google.com/flows/enableapi?apiid=cloudbuild.googleapis.com)\n",
    "   \n",
    "1. Enter your project ID in the cell below. Then run the cell to make sure the Cloud SDK uses the right project for all the commands in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bc4202",
   "metadata": {},
   "source": [
    "### Authenticate to gcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0bcad2",
   "metadata": {},
   "source": [
    " 1. In the Cloud Console, go to the [**Create service account key** page](https://console.cloud.google.com/apis/credentials/serviceaccountkey).,\n",
    " 2. Click **Create service account**.,\n",
    " 3. In the **Service account name** field, enter a name, and click **Create**,\n",
    " 4. In the **Grant this service account access to project** section, click the **Role** drop-down list. Type \\\"Vertex AI\\\" into the filter box, and select **Vertex AI Administrator**. Type \\\"Storage Object Admin\\\" into the filter box, and select **Storage Object Admin**.\n",
    " 5. Click *Create*. A JSON file that contains your key downloads to your local environment.\n",
    " 6. Enter the path to your service account key as the `GOOGLE_APPLICATION_CREDENTIALS` variable in the cell below and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee9cbff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env GOOGLE_APPLICATION_CREDENTIALS ./keys/huggingface-ml-e974975230cc.json #change to your service account key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a4d2a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID:  huggingface-ml\n"
     ]
    }
   ],
   "source": [
    "# Get your Google Cloud project ID using google.auth\n",
    "import google.auth\n",
    "\n",
    "_, PROJECT_ID = google.auth.default()\n",
    "print(\"Project ID: \", PROJECT_ID)\n",
    "\n",
    "#Or set it yourself manually\n",
    "# PROJECT_ID = \"huggingface-ml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01857fb5",
   "metadata": {},
   "source": [
    "### Create a cloud storage bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cb4c6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"gs://andrew-reed-bucket\"  # <---CHANGE THIS TO YOUR BUCKET\n",
    "REGION = \"us-east4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b366365",
   "metadata": {},
   "source": [
    "**If the bucket doesn't exist, run the following:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9c1fb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! gsutil mb -l $REGION $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd343f3",
   "metadata": {},
   "source": [
    "Access the content of the bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20adc9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827daccf",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d25c6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import base64\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import transformers\n",
    "\n",
    "import google.auth\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import gapic as aip\n",
    "from google.protobuf.json_format import MessageToDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa5e61db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook runtime: GPU\n",
      "PyTorch version : 2.0.1+cu118\n",
      "Transformers version : 4.32.1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Notebook runtime: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(f\"PyTorch version : {torch.__version__}\")\n",
    "print(f\"Transformers version : {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4818dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_NAME = \"test_bge_embedder\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed45919e",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841c6e37",
   "metadata": {},
   "source": [
    "#### *Overview*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e05d7bb",
   "metadata": {},
   "source": [
    "Deploying a PyTorch model on [Vertex AI Predictions](https://cloud.google.com/vertex-ai/docs/predictions/getting-predictions) requires to use a custom container that serves online predictions. You will deploy a container running [PyTorch's TorchServe](https://pytorch.org/serve/) tool in order to serve predictions from a fine-tuned sentence transformer model `msmarco-distilbert-base-tas-b` available in [Hugging Face Transformers](https://huggingface.co/sentence-transformers/msmarco-distilbert-base-tas-b). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b69d42",
   "metadata": {},
   "source": [
    "Essentially, to deploy a PyTorch model on Vertex AI Predictions following are the steps:\n",
    "1. Package the trained model artifacts including [default](https://pytorch.org/serve/#default-handlers) or [custom](https://pytorch.org/serve/custom_service.html) handlers by creating an archive file using [Torch model archiver](https://github.com/pytorch/serve/tree/master/model-archiver),\n",
    "2. Build a [custom container](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements) compatible with Vertex AI Predictions to serve the model using Torchserve\n",
    "3. Upload the model with custom container image to serve predictions as a Vertex AI Model resource,\n",
    "4. Create a Vertex AI Endpoint and [deploy the model](https://cloud.google.com/vertex-ai/docs/predictions/deploy-model-api) resource"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035c60c5",
   "metadata": {},
   "source": [
    "### Save model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3372ad06-ee72-46fd-ae1b-e3195ab505c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = \"./\"+APP_NAME+\"_predictor\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0825e6ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_id = \"BAAI/bge-small-en\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModel.from_pretrained(model_id)\n",
    "\n",
    "pt_save_directory = os.path.join(MODEL_DIR, \"model\")\n",
    "\n",
    "tokenizer.save_pretrained(pt_save_directory)\n",
    "model.save_pretrained(pt_save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65cd329a-1385-451c-a7e4-7c0228f642df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def cls_pooling(pipeline_output):\n",
    "    \"\"\"\n",
    "    Return the [CLS] token embedding\n",
    "    \"\"\"\n",
    "    return [_h[0][0] for _h in pipeline_output]\n",
    "        \n",
    "        \n",
    "tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)\n",
    "model = AutoModel.from_pretrained(pt_save_directory)\n",
    "\n",
    "pipe = pipeline(\"feature-extraction\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "input_texts = [\"I like the new ORT pipeline\", \"blah blah blah\", \"Hello, I'm andrew\"]\n",
    "embeddings = cls_pooling(pipe(input_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17a81e9d-c0da-466d-ab51-6a700247e746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 384)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings), len(embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d1b219",
   "metadata": {},
   "source": [
    "### Create a custom model handler "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2eb813",
   "metadata": {},
   "source": [
    "Please refer to the [TorchServe documentation](https://pytorch.org/serve/custom_service.html) for defining a custom handler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "7b1be603-2c2c-4832-94f9-17fa99f14348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use int for value, False to disable batching\n",
    "BATCH_SIZE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "9de29252",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s $APP_NAME $BATCH_SIZE\n",
    "\n",
    "APP_NAME=$1\n",
    "BATCH_SIZE=$2\n",
    "\n",
    "cat << EOF > ./${APP_NAME}_predictor/custom_handler.py\n",
    "\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, pipeline\n",
    "\n",
    "from ts.torch_handler.base_handler import BaseHandler\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class SentenceTransformersHandler(BaseHandler):\n",
    "    \"\"\"\n",
    "    The handler takes an input string and returns the embedding \n",
    "    based on the serialized transformers checkpoint.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(SentenceTransformersHandler, self).__init__()\n",
    "        self.initialized = False\n",
    "        self.batch_size = $BATCH_SIZE\n",
    "\n",
    "    def initialize(self, ctx):\n",
    "        \"\"\" Loads the model.onnx file and initialized the model object.\n",
    "        Instantiates Tokenizer for preprocessor to use and a feature extraction pipeline\n",
    "        \"\"\"\n",
    "        self.manifest = ctx.manifest\n",
    "\n",
    "        properties = ctx.system_properties\n",
    "        model_dir = properties.get(\"model_dir\")\n",
    "        self.device = torch.device(\"cuda:\" + str(properties.get(\"gpu_id\")) if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Read model serialize/pt file\n",
    "        serialized_file = self.manifest[\"model\"][\"serializedFile\"]\n",
    "        model_pt_path = os.path.join(model_dir, serialized_file)\n",
    "        if not os.path.isfile(model_pt_path):\n",
    "            raise RuntimeError(\"Missing the model.onnx or pytorch_model.bin file\")\n",
    "        \n",
    "        # Load model\n",
    "        self.model = AutoModel.from_pretrained(model_dir)\n",
    "        logger.debug('Transformer model from path {0} loaded successfully'.format(model_dir))\n",
    "        logger.info(f\"model_pt_path: {model_pt_path}\")\n",
    "        logger.info(f\"model_dir: {model_dir}\")\n",
    "        \n",
    "        # Cast to fp16 for inference speedup\n",
    "        self.model.half()\n",
    "        \n",
    "        # Ensure to use the same tokenizer used during training\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_dir, model_max_length=512)\n",
    "        \n",
    "        # Create an optimum pipeline\n",
    "        # Use BetterTransformer for fused kernel + sparsity optimizations\n",
    "        # https://medium.com/pytorch/bettertransformer-out-of-the-box-performance-for-huggingface-transformers-3fbe27d50ab2\n",
    "        if self.batch_size:\n",
    "            logger.info(f\"----Loading pipeline with batch_size={self.batch_size}\")\n",
    "            self.pipeline = pipeline(\"feature-extraction\",\n",
    "                                     model=self.model, \n",
    "                                     tokenizer=self.tokenizer, \n",
    "                                     device=self.device, \n",
    "                                     accelerator=\"bettertransformer\",\n",
    "                                     batch_size=self.batch_size)\n",
    "        else:\n",
    "            logger.info(f\"----Loading pipeline without batching\")\n",
    "            self.pipeline = pipeline(\"feature-extraction\",\n",
    "                                     model=self.model, \n",
    "                                     tokenizer=self.tokenizer, \n",
    "                                     device=self.device, \n",
    "                                     accelerator=\"bettertransformer\")\n",
    "\n",
    "        self.initialized = True\n",
    "\n",
    "    def preprocess(self, requests):\n",
    "        \"\"\" Preprocessing input request by tokenizing\n",
    "            Extend with your own preprocessing steps as needed\n",
    "            \n",
    "            [{'data': b'I am creating an endpoint using TorchServe and HF transformers\\n'}]\n",
    "        \"\"\"\n",
    "        # print(f'{\"---\"*20}')\n",
    "        # print(requests)\n",
    "        # print(type(requests))\n",
    "        # print(len(requests))\n",
    "        # print(f'{\"---\"*20}')\n",
    "        \n",
    "        input_texts = []\n",
    "        for idx, request in enumerate(requests):\n",
    "            text = request.get(\"data\")\n",
    "            if text is None:\n",
    "                text = request.get(\"body\")\n",
    "                \n",
    "            text = text.decode('utf-8')\n",
    "            input_texts.append(text)\n",
    "        logger.info(\"Received text: '%s'\", input_texts)\n",
    "        \n",
    "        return input_texts\n",
    "\n",
    "    def inference(self, input_texts):\n",
    "        \"\"\" Predict the class of a text using a trained transformer model.\n",
    "        \"\"\"\n",
    "        \n",
    "        def cls_pooling(pipeline_output):\n",
    "            \"\"\"\n",
    "            Return the [CLS] token embedding\n",
    "            \"\"\"\n",
    "            return [_h[0][0] for _h in pipeline_output]\n",
    "        \n",
    "        embeddings = cls_pooling(self.pipeline(input_texts))\n",
    "\n",
    "        logger.info(f\"Model embedded: {len(embeddings)}\")\n",
    "        return embeddings\n",
    "\n",
    "    def postprocess(self, inference_output):\n",
    "        return inference_output\n",
    "    \n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f44f516",
   "metadata": {},
   "source": [
    "### Create custom container image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fb4a9a",
   "metadata": {},
   "source": [
    "**Create a Dockerfile with TorchServe as base image**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbb394c",
   "metadata": {},
   "source": [
    "**NB**: to define the right Torchserve parameters such as `workers` please consult (https://github.com/pytorch/serve/blob/master/docs/performance_guide.md) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "ca79194d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./test_bge_embedder_predictor/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%bash -s $APP_NAME\n",
    "\n",
    "APP_NAME=$1\n",
    "\n",
    "cat << EOF > ./${APP_NAME}_predictor/Dockerfile\n",
    "\n",
    "FROM pytorch/torchserve:latest-gpu\n",
    "\n",
    "# install dependencies\n",
    "RUN python3 -m pip install --upgrade pip\n",
    "RUN pip3 install transformers\n",
    "\n",
    "USER model-server\n",
    "\n",
    "# copy model artifacts, custom handler and other dependencies\n",
    "COPY custom_handler.py /home/model-server/\n",
    "COPY ./model/ / /home/model-server/\n",
    "\n",
    "# create torchserve configuration file\n",
    "USER root\n",
    "RUN printf \"\\nservice_envelope=json\" >> /home/model-server/config.properties\n",
    "RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home/model-server/config.properties\n",
    "RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /home/model-server/config.properties\n",
    "RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /home/model-server/config.properties\n",
    "\n",
    "    \n",
    "# expose health and prediction listener ports from the image\n",
    "EXPOSE 7080\n",
    "EXPOSE 7081\n",
    "\n",
    "# create model archive file packaging model artifacts and dependencies\n",
    "RUN torch-model-archiver -f \\\n",
    "  --model-name=$APP_NAME \\\n",
    "  --version=1.0 \\\n",
    "  --serialized-file=/home/model-server/pytorch_model.bin \\\n",
    "  --handler=/home/model-server/custom_handler.py \\\n",
    "  --extra-files \"/home/model-server/config.json,/home/model-server/tokenizer.json,/home/model-server/tokenizer_config.json,/home/model-server/special_tokens_map.json,/home/model-server/vocab.txt\" \\\n",
    "  --export-path=/home/model-server/model-store\n",
    "\n",
    "# run Torchserve HTTP serve to respond to prediction requests\n",
    "# note that we don't register a model upon startup...\n",
    "# we'll use the Management API for this later on\n",
    "CMD [\"torchserve\", \\\n",
    "     \"--start\", \\\n",
    "     \"--ts-config=/home/model-server/config.properties\", \\\n",
    "     \"--model-store\", \\\n",
    "     \"/home/model-server/model-store\"]\n",
    "\n",
    "EOF\n",
    "\n",
    "echo \"Writing ./${APP_NAME}_predictor/Dockerfile\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "cc89ea12-0e5e-4308-863c-5aae33058f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_DIR = APP_NAME+\"_predictor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "98567898-71ce-4d2b-9f86-d9565973c1b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('test_bge_embedder', 'test_bge_embedder_predictor')"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "APP_NAME, APP_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca594d5b",
   "metadata": {},
   "source": [
    "**Build container**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "e2b160e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUSTOM_PREDICTOR_IMAGE_URI = gcr.io/huggingface-ml/pytorch_predict_test_bge_embedder_8\n"
     ]
    }
   ],
   "source": [
    "CUSTOM_PREDICTOR_IMAGE_URI = f\"gcr.io/{PROJECT_ID}/pytorch_predict_{APP_NAME}_{BATCH_SIZE}\"\n",
    "print(f\"CUSTOM_PREDICTOR_IMAGE_URI = {CUSTOM_PREDICTOR_IMAGE_URI}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "7d09ce47",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  134.5MB\n",
      "Step 1/15 : FROM pytorch/torchserve:latest-gpu\n",
      " ---> 04eef250c14e\n",
      "Step 2/15 : RUN python3 -m pip install --upgrade pip\n",
      " ---> Using cache\n",
      " ---> 2f06486ed62f\n",
      "Step 3/15 : RUN pip3 install transformers\n",
      " ---> Using cache\n",
      " ---> 4ed472215157\n",
      "Step 4/15 : USER model-server\n",
      " ---> Using cache\n",
      " ---> 4fa0cd41a25e\n",
      "Step 5/15 : COPY custom_handler.py /home/model-server/\n",
      " ---> 7051889b9865\n",
      "Step 6/15 : COPY ./model/ / /home/model-server/\n",
      " ---> cc0a1a489d16\n",
      "Step 7/15 : USER root\n",
      " ---> Running in 01044748924c\n",
      "Removing intermediate container 01044748924c\n",
      " ---> 5d5b3705c81c\n",
      "Step 8/15 : RUN printf \"\\nservice_envelope=json\" >> /home/model-server/config.properties\n",
      " ---> Running in dce9a937d9b4\n",
      "Removing intermediate container dce9a937d9b4\n",
      " ---> 6f88045f5a78\n",
      "Step 9/15 : RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home/model-server/config.properties\n",
      " ---> Running in 9ddd88ca05ee\n",
      "Removing intermediate container 9ddd88ca05ee\n",
      " ---> 33171b414806\n",
      "Step 10/15 : RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /home/model-server/config.properties\n",
      " ---> Running in 45affa1fb654\n",
      "Removing intermediate container 45affa1fb654\n",
      " ---> e1fa90b5a4f3\n",
      "Step 11/15 : RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /home/model-server/config.properties\n",
      " ---> Running in 2762302ad7b8\n",
      "Removing intermediate container 2762302ad7b8\n",
      " ---> a42c792d4b5f\n",
      "Step 12/15 : EXPOSE 7080\n",
      " ---> Running in c19854d9541f\n",
      "Removing intermediate container c19854d9541f\n",
      " ---> 82f0ffd3f318\n",
      "Step 13/15 : EXPOSE 7081\n",
      " ---> Running in 7fd6569ff4ce\n",
      "Removing intermediate container 7fd6569ff4ce\n",
      " ---> a142b242925f\n",
      "Step 14/15 : RUN torch-model-archiver -f   --model-name=test_bge_embedder   --version=1.0   --serialized-file=/home/model-server/pytorch_model.bin   --handler=/home/model-server/custom_handler.py   --extra-files \"/home/model-server/config.json,/home/model-server/tokenizer.json,/home/model-server/tokenizer_config.json,/home/model-server/special_tokens_map.json,/home/model-server/vocab.txt\"   --export-path=/home/model-server/model-store\n",
      " ---> Running in 9c1d7f401ee3\n",
      "Removing intermediate container 9c1d7f401ee3\n",
      " ---> 6754a52cdf66\n",
      "Step 15/15 : CMD [\"torchserve\",      \"--start\",      \"--ts-config=/home/model-server/config.properties\",      \"--model-store\",      \"/home/model-server/model-store\"]\n",
      " ---> Running in ff8c888e6284\n",
      "Removing intermediate container ff8c888e6284\n",
      " ---> 7a58c14d7b95\n",
      "Successfully built 7a58c14d7b95\n",
      "Successfully tagged gcr.io/huggingface-ml/pytorch_predict_test_bge_embedder_8:latest\n"
     ]
    }
   ],
   "source": [
    "!docker build \\\n",
    "  --tag=$CUSTOM_PREDICTOR_IMAGE_URI \\\n",
    "  ./$APP_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97df6271",
   "metadata": {},
   "source": [
    "**Test API locally**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "c284be67-6783-4237-bc08-7b90f73dcbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "484840d7256f49e5e0cb44467e6208ab7b85d3fe37e115e9882e3fce8b4d51ff\n"
     ]
    }
   ],
   "source": [
    "!docker run -td --rm -p 7080:7080 -p 7081:7081 --gpus all --name=$APP_NAME $CUSTOM_PREDICTOR_IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43497c8-8ed4-4dde-aa03-001e92e5c9e3",
   "metadata": {},
   "source": [
    "**Use TorchServe's Management API to register our model**\n",
    "\n",
    "Here we can optionally choose to deploy the model for batched inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "4ccfe1a5-2123-4008-bbfc-c3f1eb7e7b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"status\": \"Model \\\"test_bge_embedder\\\" Version: 1.0 registered with 2 initial workers\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Without batched inference\n",
    "!curl -X POST \"localhost:7081/models?url=test_bge_embedder.mar&initial_workers=2&model_name=test_bge_embedder\"\n",
    "\n",
    "# With batched inference\n",
    "# !curl -X POST \"localhost:7081/models?url=test_bge_embedder.mar&batch_size=$BATCH_SIZE&max_batch_delay=5000&initial_workers=2&model_name=test_bge_embedder\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4816b243",
   "metadata": {},
   "source": [
    "1. Health check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "f66cc11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"status\": \"Healthy\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!curl http://localhost:7080/ping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd38c546-a827-4896-b87b-3df0f719adff",
   "metadata": {},
   "source": [
    "2. Check model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "bee4a8ce-abae-4395-a075-da519a6fbef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"modelName\": \"test_bge_embedder\",\n",
      "    \"modelVersion\": \"1.0\",\n",
      "    \"modelUrl\": \"test_bge_embedder.mar\",\n",
      "    \"runtime\": \"python\",\n",
      "    \"minWorkers\": 2,\n",
      "    \"maxWorkers\": 2,\n",
      "    \"batchSize\": 8,\n",
      "    \"maxBatchDelay\": 5000,\n",
      "    \"loadedAtStartup\": false,\n",
      "    \"workers\": [\n",
      "      {\n",
      "        \"id\": \"9000\",\n",
      "        \"startTime\": \"2023-09-05T21:14:33.152Z\",\n",
      "        \"status\": \"READY\",\n",
      "        \"memoryUsage\": 0,\n",
      "        \"pid\": 94,\n",
      "        \"gpu\": true,\n",
      "        \"gpuUsage\": \"gpuId::1 utilization.gpu [%]::0 % utilization.memory [%]::0 % memory.used [MiB]::678 MiB\"\n",
      "      },\n",
      "      {\n",
      "        \"id\": \"9001\",\n",
      "        \"startTime\": \"2023-09-05T21:14:33.153Z\",\n",
      "        \"status\": \"READY\",\n",
      "        \"memoryUsage\": 0,\n",
      "        \"pid\": 95,\n",
      "        \"gpu\": true,\n",
      "        \"gpuUsage\": \"gpuId::0 utilization.gpu [%]::0 % utilization.memory [%]::0 % memory.used [MiB]::678 MiB\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "!curl http://localhost:7081/models/test_bge_embedder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd925c3f-d3a4-44f4-b52e-62a06de2f571",
   "metadata": {},
   "source": [
    "2. Send request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "87129b45-fa88-475e-98bd-c99d73c40efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = \"hello \" * 510"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "2dcbddc7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"predictions\": [[-0.147705078125, 0.21142578125, 0.2052001953125, -0.60595703125, 0.2403564453125, 0.337158203125, 0.370361328125, 0.1536865234375, 0.33837890625, -0.2122802734375, 0.269775390625, -0.1737060546875, 0.08905029296875, 0.13818359375, 0.17041015625, 0.12481689453125, 0.288818359375, -0.79248046875, -0.60986328125, 0.228271484375, 0.451171875, 0.33447265625, -0.3486328125, -0.27490234375, 0.1126708984375, 0.068603515625, -0.03741455078125, 0.09222412109375, 0.08111572265625, -1.396484375, -0.310546875, -0.26318359375, 0.214599609375, 0.2489013671875, -0.2626953125, -0.1837158203125, -0.10174560546875, -0.081787109375, -0.33447265625, 0.5302734375, 0.09259033203125, -0.10260009765625, 0.391357421875, -0.317626953125, 0.0406494140625, -0.0611572265625, -0.2025146484375, -0.1641845703125, 0.288330078125, -0.040557861328125, -0.1343994140625, -0.1416015625, -0.1470947265625, -0.189208984375, 0.2410888671875, 0.71875, 0.4541015625, 0.272216796875, 0.00965118408203125, -0.287109375, 0.330078125, -0.0185546875, -1.4013671875, 0.4052734375, 0.337646484375, -0.11358642578125, 0.1732177734375, 0.0118560791015625, 0.2030029296875, 0.10906982421875, 0.092041015625, -0.1873779296875, 0.19775390625, 0.0484619140625, 0.1827392578125, -0.52197265625, -0.01934814453125, 0.0036754608154296875, 0.015655517578125, -0.744140625, -0.006649017333984375, 0.1600341796875, -0.41015625, -0.38037109375, -0.00908660888671875, -0.227294921875, 0.662109375, 0.09722900390625, 0.08038330078125, -0.181640625, -0.0145111083984375, -0.452880859375, -0.1368408203125, 0.349853515625, -0.98876953125, -0.0921630859375, 0.241943359375, 0.22216796875, -0.138916015625, 5.15625, -0.4306640625, 0.173828125, 0.225830078125, -0.59423828125, -0.28076171875, 0.098388671875, -0.3583984375, -0.2030029296875, 0.1844482421875, 0.0863037109375, 0.262939453125, -0.45947265625, 0.2093505859375, -0.04693603515625, -0.136474609375, 0.0011587142944335938, 0.72705078125, 0.017364501953125, -0.10870361328125, -0.6376953125, 0.1619873046875, 0.1898193359375, -0.146240234375, -0.362060546875, 0.349609375, -0.340087890625, 0.501953125, 0.96533203125, 0.3564453125, 0.283203125, 0.34716796875, 0.09442138671875, -0.8291015625, -0.1781005859375, 0.11187744140625, -0.2218017578125, -0.11395263671875, -0.0743408203125, 0.1280517578125, -0.1630859375, 0.318603515625, -1.708984375, -0.174560546875, -1.01171875, -0.583984375, 0.82958984375, 0.010711669921875, -0.143798828125, -0.06414794921875, -0.25732421875, -0.050933837890625, 0.2998046875, -0.0263214111328125, -0.375244140625, 0.29150390625, 0.0889892578125, 0.3916015625, 0.110595703125, 0.03485107421875, 0.002590179443359375, 0.179443359375, -0.11236572265625, -0.41015625, 0.26806640625, -0.020538330078125, -0.59912109375, -0.5419921875, -0.170654296875, 0.474609375, -0.09539794921875, -0.0091705322265625, 0.2183837890625, -0.52197265625, 0.65966796875, 0.81884765625, 0.45166015625, 0.004596710205078125, -0.323486328125, 0.051605224609375, -0.08978271484375, 0.3837890625, -0.18505859375, -0.288818359375, 0.07421875, 0.252685546875, -0.4296875, 0.1868896484375, -0.037445068359375, -0.086669921875, -0.383544921875, -0.363037109375, 0.2247314453125, -0.09442138671875, 0.1932373046875, -0.4150390625, 0.12744140625, 0.1435546875, -0.284423828125, 0.34521484375, -0.44921875, 0.45556640625, 0.08978271484375, -0.1748046875, 0.16162109375, -0.136474609375, 0.0136260986328125, 0.53466796875, -0.12420654296875, 0.697265625, 0.355224609375, -0.1817626953125, -0.1436767578125, 0.46484375, -0.015472412109375, -0.161865234375, -0.1734619140625, 0.052520751953125, 0.387451171875, -0.1943359375, 0.29931640625, 0.01221466064453125, 0.11138916015625, -0.751953125, -2.080078125, 0.0309906005859375, 0.0889892578125, -0.2474365234375, 0.43603515625, -0.464599609375, 0.5205078125, -0.69775390625, 0.84228515625, 0.4306640625, -0.2421875, -0.265380859375, 0.2332763671875, 0.2435302734375, 0.0867919921875, 0.44091796875, -0.23193359375, 0.297607421875, 0.2548828125, -0.4345703125, 0.11993408203125, 0.23583984375, 0.014312744140625, 0.08514404296875, -0.163330078125, -0.351318359375, 1.2529296875, 0.421142578125, -0.52392578125, 0.26611328125, 0.433837890625, -0.0753173828125, 0.12646484375, -1.5712890625, 0.26953125, 0.0540771484375, -0.0259857177734375, -0.5439453125, -0.337890625, -0.11956787109375, -0.05242919921875, -0.17578125, -0.138671875, 0.2452392578125, -0.0863037109375, -0.58203125, -0.370361328125, 0.228271484375, -0.449951171875, 0.43359375, 0.487548828125, 0.097900390625, 0.387939453125, 0.1715087890625, 0.0116424560546875, -0.282958984375, -0.66650390625, -0.15283203125, -0.281494140625, 0.51318359375, -0.0406494140625, 0.1646728515625, 0.040252685546875, -0.0858154296875, -0.355712890625, 0.40869140625, -0.0222320556640625, 0.07421875, -0.1002197265625, 0.34033203125, -0.049224853515625, 0.5732421875, 0.263427734375, 0.2015380859375, 0.291259765625, 0.54638671875, -0.0638427734375, -0.623046875, -0.35400390625, -0.2744140625, 0.38330078125, -0.421630859375, -0.038299560546875, 0.7109375, 0.13818359375, 0.392333984375, 0.62548828125, 0.1610107421875, 0.08990478515625, -0.37255859375, -0.0292816162109375, -0.462158203125, 0.0186920166015625, -0.73095703125, 0.08642578125, -0.1673583984375, -2.533203125, 0.27734375, 0.1314697265625, 0.321044921875, -0.467529296875, 0.5361328125, -0.002349853515625, 0.4638671875, -0.9208984375, 0.2357177734375, -0.1361083984375, 0.1640625, 0.250732421875, -0.07025146484375, -0.07122802734375, 0.5791015625, 0.3076171875, -0.1490478515625, 0.30517578125, -0.2489013671875, -0.2783203125, 0.0496826171875, 1.4599609375, -0.059173583984375, 0.80517578125, -0.040435791015625, -0.298828125, 0.53271484375, -0.10009765625, -0.07818603515625, -0.1348876953125, -0.2000732421875, 0.1920166015625, -0.300048828125, 0.1544189453125, 0.0211944580078125, -0.1781005859375, -0.184814453125, 0.30810546875, -0.2203369140625, -0.293212890625, 0.48046875, -0.2493896484375, 0.299072265625, 0.022003173828125, 0.26513671875, 0.175537109375, -0.52978515625, -0.0183563232421875, -0.09368896484375, -0.413330078125, -0.0570068359375, 0.18017578125, -0.1461181640625, 0.1885986328125, 0.12164306640625, -0.029327392578125, -0.244873046875, -0.327880859375, -0.05584716796875, 0.003326416015625, 0.1219482421875, -0.044342041015625, 0.10980224609375, 0.144287109375]]}"
     ]
    }
   ],
   "source": [
    "%%bash -s $APP_NAME\n",
    "\n",
    "APP_NAME=$1\n",
    "\n",
    "cat > ./${APP_NAME}_predictor/instances.json <<END\n",
    "{ \n",
    "   \"instances\": [\n",
    "     { \n",
    "       \"data\": {\n",
    "         \"b64\": \"$(echo 'hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello ' | base64 --wrap=0)\"\n",
    "       }\n",
    "     }\n",
    "   ]\n",
    "}\n",
    "END\n",
    "\n",
    "curl -s -X POST \\\n",
    "  -H \"Content-Type: application/json; charset=utf-8\" \\\n",
    "  -d @./${APP_NAME}_predictor/instances.json \\\n",
    "  http://localhost:7080/predictions/$APP_NAME/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a89149-812f-4ca7-bce9-9ac8501e7408",
   "metadata": {},
   "source": [
    "**Install apache bench**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0b5775-63b4-4b53-9e05-da6fa6002c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt-get update\n",
    "# !apt-get install apache2-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "2eb0ad30-0021-4206-9901-41b75d4012b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 100 requests\n",
      "Completed 200 requests\n",
      "Completed 300 requests\n",
      "Completed 400 requests\n",
      "Completed 500 requests\n",
      "Completed 600 requests\n",
      "Completed 700 requests\n",
      "Completed 800 requests\n",
      "Completed 900 requests\n",
      "Completed 1000 requests\n",
      "Finished 1000 requests\n"
     ]
    }
   ],
   "source": [
    "REQUESTS = 5000\n",
    "CONCURRENCY = 1000\n",
    "\n",
    "!ab -c $CONCURRENCY -n $REQUESTS -k -p ./$APP_DIR/instances.json -T \"application/json\" http://localhost:7080/predictions/$APP_NAME/ > ./$APP_DIR/result.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3a57fe-217c-4b47-959f-7d0dc510b502",
   "metadata": {},
   "source": [
    "Notes on BGE-sm:\n",
    "- Without batching: ~73 RPS\n",
    "- BS=8: ~70.5 RPS\n",
    "- BS=32: ~70 RPS (GPU Util not maxed out)\n",
    "\n",
    "\n",
    "- Without batching, RPS ~78\n",
    "- BS=32, RPS ~61\n",
    "- BS=64, RPS ~64\n",
    "- BS=256, RPS ~51\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f906fc1",
   "metadata": {},
   "source": [
    "3. Stop the container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726bd349",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker stop local_sbert_embedder_optimum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24c5d0c",
   "metadata": {},
   "source": [
    "### Push image Container Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2611ed58",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker push $CUSTOM_PREDICTOR_IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e9565b",
   "metadata": {},
   "source": [
    "### Create model and endpoint to VertexAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba10b3e0",
   "metadata": {},
   "source": [
    "We create a model resource on Vertex AI and deploy the model to a Vertex AI Endpoints. You must deploy a model to an endpoint before using the model. The deployed model runs the custom container image to serve predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b99644",
   "metadata": {},
   "source": [
    "**Initialize the Vertex AI SDK for Python**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f00242",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e418380",
   "metadata": {},
   "source": [
    "**Create a Model resource with custom serving container**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170d5ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = 1\n",
    "model_display_name = f\"{APP_NAME}-v{VERSION}\"\n",
    "model_description = \"PyTorch based sentence transformers embedder with custom container\"\n",
    "\n",
    "MODEL_NAME = APP_NAME\n",
    "health_route = \"/ping\"\n",
    "predict_route = f\"/predictions/{MODEL_NAME}\"\n",
    "serving_container_ports = [7080]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eab0c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = aiplatform.Model.upload(\n",
    "    display_name=model_display_name,\n",
    "    description=model_description,\n",
    "    serving_container_image_uri=CUSTOM_PREDICTOR_IMAGE_URI,\n",
    "    serving_container_predict_route=predict_route,\n",
    "    serving_container_health_route=health_route,\n",
    "    serving_container_ports=serving_container_ports,\n",
    ")\n",
    "\n",
    "model.wait()\n",
    "\n",
    "print(model.display_name)\n",
    "print(model.resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7343cea4",
   "metadata": {},
   "source": [
    "**Create an Endpoint for Model with Custom Container**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3863a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_display_name = f\"{APP_NAME}-endpoint\"\n",
    "endpoint = aiplatform.Endpoint.create(display_name=endpoint_display_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea432646",
   "metadata": {},
   "source": [
    "**Deploy the Model to Endpoint**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b772534",
   "metadata": {},
   "source": [
    "See more on the [documentation](https://cloud.google.com/vertex-ai/docs/predictions/deploy-model-api)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52b599d",
   "metadata": {},
   "source": [
    "To select the right machine type according to your budget select go to [Google Cloud Pricing Calculator](https://cloud.google.com/products/calculator) and [Finding the ideal machine type](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#finding_the_ideal_machine_type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecad0cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_percentage = 100\n",
    "machine_type = \"n1-standard-8\"\n",
    "deployed_model_display_name = model_display_name\n",
    "min_replica_count = 1\n",
    "max_replica_count = 3\n",
    "sync = True\n",
    "\n",
    "model.deploy(\n",
    "    endpoint=endpoint,\n",
    "    deployed_model_display_name=deployed_model_display_name,\n",
    "    machine_type=machine_type,\n",
    "    traffic_percentage=traffic_percentage,\n",
    "    min_replica_count=min_replica_count,\n",
    "    max_replica_count=max_replica_count,\n",
    "    sync=sync,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1add143",
   "metadata": {},
   "source": [
    "### Invoking the Endpoint with deployed Model using Vertex AI SDK to make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49af35f0",
   "metadata": {},
   "source": [
    "**Get the endpoint id**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a405988",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_display_name = f\"{APP_NAME}-endpoint\"\n",
    "filter = f'display_name=\"{endpoint_display_name}\"'\n",
    "\n",
    "for endpoint_info in aiplatform.Endpoint.list(filter=filter):\n",
    "    print(\n",
    "        f\"Endpoint display name = {endpoint_info.display_name} resource id ={endpoint_info.resource_name} \"\n",
    "    )\n",
    "\n",
    "endpoint = aiplatform.Endpoint(endpoint_info.resource_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c937f2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a59ee57",
   "metadata": {},
   "source": [
    "**Formatting input for online prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cf7c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_instances = [\n",
    "    b\"This is an example of model deployment using a sentence transformers model and optimum\",\n",
    "]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f260939",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer(test_instances[0])[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0268a7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a19a344",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"=\" * 100)\n",
    "for instance in test_instances:\n",
    "    print(f\"Input text: \\n\\t{instance.decode('utf-8')}\\n\")\n",
    "    b64_encoded = base64.b64encode(instance)\n",
    "    test_instance = [{\"data\": {\"b64\": f\"{str(b64_encoded.decode('utf-8'))}\"}}]\n",
    "    print(f\"Formatted input: \\n{json.dumps(test_instance, indent=4)}\\n\")\n",
    "    prediction = endpoint.predict(instances=test_instance)\n",
    "    #print(f\"Prediction response: \\n\\t{prediction}\")\n",
    "    print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1a6d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "prediction = endpoint.predict(instances=test_instance)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.2-0.m110",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.2-0:m110"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
