{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Inference Handler for Gector on Inference Endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Git LFS initialized.\n",
      "Cloning into 'gector-deberta-large-5k'...\n",
      "remote: Enumerating objects: 12, done.\u001b[K\n",
      "remote: Counting objects: 100% (1/1), done.\u001b[K\n",
      "remote: Total 12 (delta 0), reused 1 (delta 0), pack-reused 11\u001b[K\n",
      "Unpacking objects: 100% (12/12), 1.15 MiB | 7.28 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "!git lfs install\n",
    "!git clone https://huggingface.co/andrewrreed/gector-deberta-large-5k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup cli with token\n",
    "!huggingface-cli login\n",
    "!git config --global credential.helper store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m venv .venv\n",
    "!source .venv/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download Gector lirary\n",
    "!git clone https://github.com/gotutiyan/gector.git\n",
    "\n",
    "# copy in necessary files\n",
    "%cp gector/requirements.txt gector-deberta-large-5k/.\n",
    "%cp -r gector/gector gector-deberta-large-5k/.\n",
    "\n",
    "# remove the old gector repo\n",
    "%rm -rf gector\n",
    "\n",
    "# change dir\n",
    "%cd gector-deberta-large-5k\n",
    "\n",
    "# remove nvidia/triton dependencies\n",
    "!grep -viE \"^(nvidia|triton)\" requirements.txt > temp_requirements.txt && mv temp_requirements.txt requirements.txt\n",
    "\n",
    "# install reqs\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "# download verb form vocab\n",
    "!mkdir data\n",
    "!cd data && curl -LO https://github.com/grammarly/gector/raw/master/data/verb-form-vocab.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create custom handler for Inference Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing handler.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile handler.py\n",
    "import os\n",
    "import torch\n",
    "from typing import Dict, List, Any\n",
    "from transformers import AutoTokenizer\n",
    "from gector import GECToR, predict, load_verb_dict\n",
    "\n",
    "\n",
    "class EndpointHandler:\n",
    "    def __init__(self, path=\"\"):\n",
    "        self.model = GECToR.from_pretrained(path)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "        self.encode, self.decode = load_verb_dict(\n",
    "            os.path.join(path, \"data/verb-form-vocab.txt\")\n",
    "        )\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(device)\n",
    "\n",
    "    def __call__(self, data: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Process the input data and return the predicted results.\n",
    "\n",
    "        Args:\n",
    "            data (Dict[str, Any]): The input data dictionary containing the following keys:\n",
    "                - \"inputs\" (List[str]): A list of input strings to be processed.\n",
    "                - \"n_iterations\" (int, optional): The number of iterations for prediction. Defaults to 5.\n",
    "                - \"batch_size\" (int, optional): The batch size for prediction. Defaults to 2.\n",
    "                - \"keep_confidence\" (float, optional): The confidence threshold for keeping predictions. Defaults to 0.0.\n",
    "                - \"min_error_prob\" (float, optional): The minimum error probability for keeping predictions. Defaults to 0.0.\n",
    "\n",
    "        Returns:\n",
    "            List[Dict[str, Any]]: A list of dictionaries containing the predicted results for each input string.\n",
    "        \"\"\"\n",
    "        srcs = data[\"inputs\"]\n",
    "\n",
    "        # Extract optional parameters from data, with defaults\n",
    "        n_iterations = data.get(\"n_iterations\", 5)\n",
    "        batch_size = data.get(\"batch_size\", 2)\n",
    "        keep_confidence = data.get(\"keep_confidence\", 0.0)\n",
    "        min_error_prob = data.get(\"min_error_prob\", 0.0)\n",
    "\n",
    "        return predict(\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            srcs=srcs,\n",
    "            encode=self.encode,\n",
    "            decode=self.decode,\n",
    "            keep_confidence=keep_confidence,\n",
    "            min_error_prob=min_error_prob,\n",
    "            n_iteration=n_iterations,\n",
    "            batch_size=batch_size,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteratoin 0. the number of to_be_processed: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteratoin 1. the number of to_be_processed: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  5.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a correct sentence.', 'This is an incorrect sentence .']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from handler import EndpointHandler\n",
    "\n",
    "my_handler = EndpointHandler(path=\".\")\n",
    "\n",
    "# prepare sample payload\n",
    "payload = {\n",
    "    \"inputs\": [\"This is a correct sentence.\", \"this is a INcorrect sentence\"],\n",
    "    \"n_iterations\": 25,\n",
    "}\n",
    "# test\n",
    "out = my_handler(payload)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push changes to Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add *\n",
    "!git commit -m \"add handler\"\n",
    "!git push"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
