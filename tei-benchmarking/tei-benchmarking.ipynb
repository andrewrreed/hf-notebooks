{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEI Bulk Embedding Load Test\n",
    "\n",
    "I think a good workflow:\n",
    "\n",
    "1. Use load tests with k6 to do grid search (it handles failures well)\n",
    "2. Then use my scripts to confirm we get similar performance when making async calls\n",
    "\n",
    "Also need to include a demo of how to keep track of the order / tie a specific response to a given input record... apparently asyncio respects the ordering of requests..\n",
    "\n",
    "Resources:\n",
    "\n",
    "- On asyncio in Python: https://superfastpython.com/asyncio-gather/\n",
    "- On `as_completed()` vs. `gather()`: https://jxnl.github.io/instructor/blog/2023/11/13/learn-async/#asyncioas_completed-handling-tasks-as-they-complete\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal:\n",
    "\n",
    "- Embed N chunks of data in total\n",
    "- Vary the batch_size of each request, as well as the concurrency of requests\n",
    "- Analyze the total time to complete, RPS, and latency to see which bs / concurrency combination is best.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Embedding Model with TEI on Inference Endpoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = os.getenv(\"HF_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import create_inference_endpoint\n",
    "\n",
    "endpoint = create_inference_endpoint(\n",
    "    name=\"bge-base-en-v15-arr-tei-test\",\n",
    "    repository=\"BAAI/bge-base-en-v1.5\",\n",
    "    framework=\"pytorch\",\n",
    "    task=\"sentence-embeddings\",\n",
    "    accelerator=\"gpu\",\n",
    "    vendor=\"aws\",\n",
    "    region=\"us-east-1\",\n",
    "    type=\"protected\",\n",
    "    namespace=\"HF-test-lab\",\n",
    "    instance_type=\"g5.2xlarge\",\n",
    "    instance_size=\"medium\",\n",
    "    custom_image={\n",
    "        \"health_route\": \"/health\",\n",
    "        \"env\": {\n",
    "            \"MAX_CONCURRENT_REQUESTS\": \"4096\",\n",
    "            \"MAX_BATCH_TOKENS\": \"65536\",\n",
    "            \"MAX_CLIENT_BATCH_SIZE\": \"2048\",\n",
    "            \"MODEL_ID\": \"/repository\",\n",
    "        },\n",
    "        \"url\": \"ghcr.io/huggingface/text-embeddings-inference:86-0.6\",  # A10 GPU specific!\n",
    "    },\n",
    ")\n",
    "\n",
    "endpoint.wait()\n",
    "print(endpoint.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://qv5gh3puw805qym7.us-east-1.aws.endpoints.huggingface.cloud'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tei_benchmark import main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [{\"unique_id\": n, \"text\": \"hello \" * 512} for n in range(10_000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size: 64, Concurrency Level: 100, Total Time: 22.5188 seconds, Embed per sec: 444.0734, Num Success: 10000, Num Failures: 0\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "concurrency = 100\n",
    "\n",
    "os.remove(\"embeddings.jsonl\")\n",
    "results = await main(data, batch_size, concurrency, filename=\"embeddings.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'timing_statistics': {'total_time': {'min': 43.0,\n",
       "   'max': 13431.0,\n",
       "   'mean': 9044.0304,\n",
       "   'median': 10177.0,\n",
       "   'p90': 13149.0,\n",
       "   'p95': 13172.0},\n",
       "  'tokenization_time': {'min': 5.0,\n",
       "   'max': 1107.0,\n",
       "   'mean': 300.1616,\n",
       "   'median': 48.0,\n",
       "   'p90': 832.0,\n",
       "   'p95': 977.0},\n",
       "  'queue_time': {'min': 9.0,\n",
       "   'max': 8467.0,\n",
       "   'mean': 6225.8384,\n",
       "   'median': 7806.0,\n",
       "   'p90': 8455.0,\n",
       "   'p95': 8463.0},\n",
       "  'inference_time': {'min': 16.0,\n",
       "   'max': 273.0,\n",
       "   'mean': 269.3312,\n",
       "   'median': 272.0,\n",
       "   'p90': 273.0,\n",
       "   'p95': 273.0}},\n",
       " 'total_time': 22.5188,\n",
       " 'embeddings_per_second': 444.0734,\n",
       " 'request_metrics': {'success': 10000, 'failure': 0, 'total': 10000},\n",
       " 'run_metadata': {'batch_size': 64,\n",
       "  'concurrency': 100,\n",
       "  'filename': 'embeddings.jsonl'}}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# async def run_experiments():\n",
    "#     batch_sizes = [1, 4, 8, 16, 32]\n",
    "#     concurrency_levels = [1, 25, 50, 100, 205, 500, 1000]\n",
    "\n",
    "#     results = []\n",
    "#     for batch_size in batch_sizes[-2:]:\n",
    "#         for concurrency_level in concurrency_levels[-2:]:\n",
    "#             batches = generate_batches(\n",
    "#                 \"This is a test\", bs=batch_size, total_chunks=10_000\n",
    "#             )\n",
    "#             results.append(\n",
    "#                 await embed(\n",
    "#                     batches,\n",
    "#                     concurrency_level=concurrency_level,\n",
    "#                     collect_embeddings=False,\n",
    "#                 )\n",
    "#             )\n",
    "#     return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
