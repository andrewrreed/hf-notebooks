{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate VRAM usage for TGI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "from huggingface_hub import get_safetensors_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bytes_per_dtype = {\"int4\": 0.5, \"int8\": 1, \"float8\": 1, \"float16\": 2, \"float32\": 4}\n",
    "\n",
    "\n",
    "def bytes_to_gb(bytes: int):\n",
    "    \"\"\"\n",
    "    Calculate the memory in GB for a given number of bytes.\n",
    "\n",
    "    Note:\n",
    "        - 1 GB = 10^9 bytes and 1 GiB = 2^30 bytes\n",
    "        - We estimate VRAM usage in GB for more conservative estimates\n",
    "    \"\"\"\n",
    "    return round((bytes) / 10**9, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA Overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cuda_overhead():\n",
    "    \"\"\"\n",
    "    When PyTorch uses CUDA for the first time, it may use up 0.5-2GB of GPU memory, reducing the GPU's total available memory.\n",
    "\n",
    "    Here, we assume 1GB of overhead based on torch>=2.1.1 as discussed here:\n",
    "    https://github.com/stas00/ml-engineering/tree/master/training/performance#additional-gpu-memory-usage\n",
    "    \"\"\"\n",
    "    return 1 * 10**9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_overhead(model_id: str, dtype: str = \"float16\") -> int:\n",
    "    \"\"\"\n",
    "    Get the model size in GB for a given model ID and data type.\n",
    "    \"\"\"\n",
    "    metadata = get_safetensors_metadata(model_id)\n",
    "\n",
    "    if hasattr(metadata, \"metadata\"):\n",
    "        if metadata.metadata.get(\"total_size\", False):\n",
    "            return metadata.metadata[\"total_size\"]\n",
    "    else:\n",
    "        num_params = list(metadata.parameter_count.values())[0]\n",
    "        num_params = int(num_params)\n",
    "        bytes = num_params * bytes_per_dtype[dtype]\n",
    "        return bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prefill Overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def calculate_mlp_prefill_overhead(\n",
    "    seq_len: int,\n",
    "    batch_size: int,\n",
    "    hidden_size: int,\n",
    "    intermediate_size: int,\n",
    "    dtype: str = \"float16\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate the prefill overhead for single MLP block/layer.\n",
    "\n",
    "    Accounts for following activations:\n",
    "        - Input to the first Linear layer\n",
    "        - Input to the activation function\n",
    "        - Input to the second Linear layer\n",
    "        - Dropout mask\n",
    "    \"\"\"\n",
    "\n",
    "    bytes_per_unit = bytes_per_dtype[dtype]\n",
    "\n",
    "    mlp_input = batch_size * seq_len * hidden_size * bytes_per_unit\n",
    "    act_input = batch_size * seq_len * intermediate_size * bytes_per_unit\n",
    "    down_proj_input = batch_size * seq_len * intermediate_size * bytes_per_unit\n",
    "    dropout_mask = (\n",
    "        batch_size * seq_len * hidden_size * bytes_per_dtype[\"int8\"]\n",
    "    )  # binary mask\n",
    "\n",
    "    mlp_block_bytes = mlp_input + act_input + down_proj_input + dropout_mask\n",
    "\n",
    "    return mlp_block_bytes\n",
    "\n",
    "\n",
    "def calculate_attention_prefill_overhead(\n",
    "    seq_len: int,\n",
    "    batch_size: int,\n",
    "    hidden_size: int,\n",
    "    head_dim: int,\n",
    "    num_attention_heads: int,\n",
    "    num_key_value_heads: int,\n",
    "    dtype: str = \"float16\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate the prefill overhead for single attention block/layer.\n",
    "    \"\"\"\n",
    "    bytes_per_unit = bytes_per_dtype[dtype]\n",
    "\n",
    "    attention_input = batch_size * seq_len * hidden_size * bytes_per_unit\n",
    "    q = batch_size * seq_len * head_dim * num_attention_heads * bytes_per_unit\n",
    "    k = batch_size * seq_len * head_dim * num_key_value_heads * bytes_per_unit\n",
    "    softmax_output = batch_size * num_attention_heads * seq_len**2 * bytes_per_unit\n",
    "    softmax_dropout_mask = (\n",
    "        batch_size * num_attention_heads * seq_len**2 * bytes_per_dtype[\"int8\"]\n",
    "    )\n",
    "    dropout_output = batch_size * num_attention_heads * seq_len**2 * bytes_per_unit\n",
    "    v = batch_size * seq_len * head_dim * num_key_value_heads * bytes_per_unit\n",
    "    out_proj_input = (\n",
    "        batch_size * seq_len * num_attention_heads * head_dim * bytes_per_unit\n",
    "    )\n",
    "    attention_dropout_mask = (\n",
    "        batch_size * seq_len * hidden_size * bytes_per_dtype[\"int8\"]\n",
    "    )\n",
    "\n",
    "    attention_block_bytes = (\n",
    "        attention_input\n",
    "        + q\n",
    "        + k\n",
    "        + softmax_output\n",
    "        + softmax_dropout_mask\n",
    "        + dropout_output\n",
    "        + v\n",
    "        + out_proj_input\n",
    "        + attention_dropout_mask\n",
    "    )\n",
    "\n",
    "    return attention_block_bytes\n",
    "\n",
    "\n",
    "def calculate_flash_attention_prefill_overhead(\n",
    "    seq_len: int,\n",
    "    batch_size: int,\n",
    "    hidden_size: int,\n",
    "    head_dim: int,\n",
    "    num_attention_heads: int,\n",
    "    num_key_value_heads: int,\n",
    "    dtype: str = \"float16\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate the prefill overhead for single attention block/layer.\n",
    "    \"\"\"\n",
    "    bytes_per_unit = bytes_per_dtype[dtype]\n",
    "\n",
    "    attention_input = batch_size * seq_len * hidden_size * bytes_per_unit\n",
    "    q = batch_size * seq_len * head_dim * num_attention_heads * bytes_per_unit\n",
    "    k = batch_size * seq_len * head_dim * num_key_value_heads * bytes_per_unit\n",
    "    v = batch_size * seq_len * head_dim * num_key_value_heads * bytes_per_unit\n",
    "\n",
    "    softmax_output = batch_size * num_attention_heads * seq_len * bytes_per_unit\n",
    "    softmax_dropout_mask = (\n",
    "        batch_size * num_attention_heads * seq_len * bytes_per_dtype[\"int8\"]\n",
    "    )\n",
    "    dropout_output = batch_size * num_attention_heads * seq_len * bytes_per_unit\n",
    "\n",
    "    out_proj_input = (\n",
    "        batch_size * seq_len * num_attention_heads * head_dim * bytes_per_unit\n",
    "    )\n",
    "    attention_dropout_mask = (\n",
    "        batch_size * seq_len * hidden_size * bytes_per_dtype[\"int8\"]\n",
    "    )\n",
    "\n",
    "    attention_block_bytes = (\n",
    "        attention_input\n",
    "        + q\n",
    "        + k\n",
    "        + v\n",
    "        + softmax_output\n",
    "        + softmax_dropout_mask\n",
    "        + dropout_output\n",
    "        + out_proj_input\n",
    "        + attention_dropout_mask\n",
    "    )\n",
    "\n",
    "    return attention_block_bytes\n",
    "\n",
    "\n",
    "def calculate_layernorm_prefill_overhead(\n",
    "    seq_len: int,\n",
    "    batch_size: int,\n",
    "    hidden_size: int,\n",
    "    dtype: str = \"float16\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate the prefill overhead for single layer norm layer.\n",
    "    \"\"\"\n",
    "    bytes_per_unit = bytes_per_dtype[dtype]\n",
    "    layernorm_bytes = batch_size * seq_len * hidden_size * bytes_per_unit\n",
    "\n",
    "    return layernorm_bytes\n",
    "\n",
    "\n",
    "def get_prefill_overhead(\n",
    "    config: AutoConfig,\n",
    "    max_batch_prefill_tokens: int,\n",
    "    max_input_tokens: int,\n",
    "    dtype: str = \"float16\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate the prefill overhead for a given model and TGI config.\n",
    "\n",
    "    Size of a biggest tensor within forward pass.\n",
    "    It is estimated as the sum of all intermediate tensors within computation of a single layer.\n",
    "    Activations size have quadratic dependence on Sequence Length.\n",
    "\n",
    "    Reference:\n",
    "        - https://arxiv.org/pdf/2205.05198\n",
    "        - https://asmirnov.xyz/vram#fn1\n",
    "    \"\"\"\n",
    "    batch_size = math.ceil(max_batch_prefill_tokens / max_input_tokens)\n",
    "    head_dim = (\n",
    "        config.head_dim\n",
    "        if hasattr(config, \"head_dim\")\n",
    "        else config.hidden_size // config.num_key_value_heads\n",
    "    )\n",
    "\n",
    "    mlp_overhead = calculate_mlp_prefill_overhead(\n",
    "        seq_len=max_input_tokens,\n",
    "        batch_size=batch_size,\n",
    "        hidden_size=config.hidden_size,\n",
    "        intermediate_size=config.intermediate_size,\n",
    "        dtype=dtype,\n",
    "    )\n",
    "\n",
    "    attention_overhead = calculate_flash_attention_prefill_overhead(\n",
    "        seq_len=max_input_tokens,\n",
    "        batch_size=batch_size,\n",
    "        hidden_size=config.hidden_size,\n",
    "        head_dim=head_dim,\n",
    "        num_attention_heads=config.num_attention_heads,\n",
    "        num_key_value_heads=config.num_key_value_heads,\n",
    "        dtype=dtype,\n",
    "    )\n",
    "\n",
    "    layernorm_overhead = calculate_layernorm_prefill_overhead(\n",
    "        seq_len=max_input_tokens,\n",
    "        batch_size=batch_size,\n",
    "        hidden_size=config.hidden_size,\n",
    "        dtype=dtype,\n",
    "    )\n",
    "\n",
    "    return mlp_overhead + attention_overhead + (layernorm_overhead * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KV Cache Overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caclulate_kv_cache_memory_per_token(config: AutoConfig, dtype: str):\n",
    "    \"\"\"Calculates the memory required for the key-value cache per token in a Large Language Model (LLM).\"\"\"\n",
    "    dtype_bytes = bytes_per_dtype[dtype]\n",
    "    head_dim = (\n",
    "        config.head_dim\n",
    "        if hasattr(config, \"head_dim\")\n",
    "        else config.hidden_size // config.num_key_value_heads\n",
    "    )\n",
    "\n",
    "    bytes_per_token = (\n",
    "        2  # k & v\n",
    "        * config.num_hidden_layers\n",
    "        * config.num_key_value_heads\n",
    "        * head_dim\n",
    "        * dtype_bytes\n",
    "    )\n",
    "    return bytes_per_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131072"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caclulate_kv_cache_memory_per_token(\n",
    "    config=AutoConfig.from_pretrained(\"mistralai/Mistral-7B-v0.1\"),\n",
    "    dtype=\"float16\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate VRAM Overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_vram_overhead(\n",
    "    model_id: str,\n",
    "    num_gpus: int,\n",
    "    vram_per_gpu: int,  # in GB\n",
    "    max_input_tokens: int,\n",
    "    max_total_tokens: int,\n",
    "    max_batch_prefill_tokens: int,\n",
    "    cuda_memory_fraction: float = 1.0,\n",
    "    dtype: str = \"float16\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Validate the VRAM overhead for a given model and TGI config.\n",
    "    \"\"\"\n",
    "    config = AutoConfig.from_pretrained(model_id)\n",
    "\n",
    "    # determine overhead\n",
    "    cuda_overhead = get_cuda_overhead() * num_gpus\n",
    "    model_overhead = get_model_overhead(model_id, dtype)\n",
    "    prefill_overhead = get_prefill_overhead(\n",
    "        config, max_batch_prefill_tokens, max_input_tokens, dtype\n",
    "    )\n",
    "    kv_cache_overhead_per_token = caclulate_kv_cache_memory_per_token(config, dtype)\n",
    "\n",
    "    # calculate free vram for kv cache\n",
    "    free_vram = vram_per_gpu * num_gpus * 10**9\n",
    "    free_vram = free_vram - cuda_overhead - model_overhead - prefill_overhead\n",
    "    free_vram = (\n",
    "        free_vram * cuda_memory_fraction * 0.95\n",
    "    )  # by default, TGI allocates 95% of free VRAM to kv-cache\n",
    "\n",
    "    print(f\"Model Overhead: {bytes_to_gb(model_overhead)} GB\")\n",
    "    print(f\"CUDA Overhead: {bytes_to_gb(cuda_overhead)} GB\")\n",
    "    print(f\"Prefill Overhead: {bytes_to_gb(prefill_overhead)} GB\")\n",
    "    print(f\"Free VRAM for KV Cache: {bytes_to_gb(free_vram)} GB\")\n",
    "\n",
    "    # calculate token budget available for full kv cache\n",
    "    kv_cache_token_budget = int((free_vram) // kv_cache_overhead_per_token)\n",
    "\n",
    "    # check if we have enough vram for full kv cache (num_batches * max_total_tokens)\n",
    "    batch_size = math.ceil(max_batch_prefill_tokens / max_input_tokens)\n",
    "    kv_cache_tokens_needed = batch_size * (max_total_tokens - 1)\n",
    "    vram_needed_for_kv_cache = kv_cache_tokens_needed * kv_cache_overhead_per_token\n",
    "    is_enough_vram = kv_cache_token_budget >= kv_cache_tokens_needed\n",
    "\n",
    "    result = {\n",
    "        \"cuda_overhead\": bytes_to_gb(cuda_overhead),\n",
    "        \"model_overhead\": bytes_to_gb(model_overhead),\n",
    "        \"prefill_overhead\": bytes_to_gb(prefill_overhead),\n",
    "        \"kv_cache_overhead_per_token\": bytes_to_gb(kv_cache_overhead_per_token),\n",
    "        \"free_vram_for_kv_cache\": bytes_to_gb(free_vram),\n",
    "        \"vram_needed_for_kv_cache\": bytes_to_gb(vram_needed_for_kv_cache),\n",
    "        \"kv_cache_token_budget\": kv_cache_token_budget,\n",
    "        \"kv_cache_tokens_needed\": kv_cache_tokens_needed,\n",
    "        \"is_enough_vram\": is_enough_vram,\n",
    "    }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parse safetensors files: 100%|██████████| 4/4 [00:00<00:00,  7.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Overhead: 16.06052 GB\n",
      "CUDA Overhead: 4.0 GB\n",
      "Prefill Overhead: 7.38348 GB\n",
      "Free VRAM for KV Cache: 65.1282 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cuda_overhead': 4.0,\n",
       " 'model_overhead': 16.06052,\n",
       " 'prefill_overhead': 7.38348,\n",
       " 'kv_cache_overhead_per_token': 0.00052,\n",
       " 'free_vram_for_kv_cache': 65.1282,\n",
       " 'vram_needed_for_kv_cache': 21.52622,\n",
       " 'kv_cache_token_budget': 124222,\n",
       " 'kv_cache_tokens_needed': 41058,\n",
       " 'is_enough_vram': True}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max_input_tokens = 8750\n",
    "# max_total_tokens = 8782\n",
    "# max_batch_prefill_tokens = 8782\n",
    "\n",
    "# max_input_tokens = 15520\n",
    "# max_total_tokens = 15554\n",
    "# max_batch_prefill_tokens = 15554\n",
    "\n",
    "max_input_tokens = 20466\n",
    "max_total_tokens = 20530\n",
    "max_batch_prefill_tokens = 20530\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "validate_vram_overhead(\n",
    "    model_id=model_id,\n",
    "    num_gpus=4,\n",
    "    vram_per_gpu=24,\n",
    "    max_input_tokens=max_input_tokens,\n",
    "    max_total_tokens=max_total_tokens,\n",
    "    max_batch_prefill_tokens=max_batch_prefill_tokens,\n",
    "    dtype=\"float16\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parse safetensors files: 100%|██████████| 4/4 [00:00<00:00,  7.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Overhead: 16.06052 GB\n",
      "CUDA Overhead: 4.0 GB\n",
      "Prefill Overhead: 11.07522 GB\n",
      "Free VRAM for KV Cache: 61.62105 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cuda_overhead': 4.0,\n",
       " 'model_overhead': 16.06052,\n",
       " 'prefill_overhead': 11.07522,\n",
       " 'kv_cache_overhead_per_token': 0.00052,\n",
       " 'free_vram_for_kv_cache': 61.62105,\n",
       " 'vram_needed_for_kv_cache': 32.28933,\n",
       " 'kv_cache_token_budget': 117532,\n",
       " 'kv_cache_tokens_needed': 61587,\n",
       " 'is_enough_vram': True}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_input_tokens = 20466\n",
    "max_total_tokens = 20530\n",
    "max_batch_prefill_tokens = 50000\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "validate_vram_overhead(\n",
    "    model_id=model_id,\n",
    "    num_gpus=4,\n",
    "    vram_per_gpu=24,\n",
    "    max_input_tokens=max_input_tokens,\n",
    "    max_total_tokens=max_total_tokens,\n",
    "    max_batch_prefill_tokens=max_batch_prefill_tokens,\n",
    "    dtype=\"float16\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
