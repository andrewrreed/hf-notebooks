{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate VRAM usage for TGI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewreed/Documents/hf-notebooks/tgi-vram-estimate/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "from huggingface_hub import get_safetensors_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bytes_per_dtype = {\"int4\": 0.5, \"int8\": 1, \"float8\": 1, \"float16\": 2, \"float32\": 4}\n",
    "\n",
    "\n",
    "def bytes_to_gb(bytes: int):\n",
    "    \"\"\"\n",
    "    Calculate the memory in GB for a given number of bytes.\n",
    "\n",
    "    Note:\n",
    "        - 1 GB = 10^9 bytes and 1 GiB = 2^30 bytes\n",
    "        - We estimate VRAM usage in GB for more conservative estimates\n",
    "    \"\"\"\n",
    "    return round((bytes) / 10**9, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA Overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cuda_overhead(buffer: float = 1.2):\n",
    "    \"\"\"\n",
    "    When PyTorch uses CUDA for the first time, it may use up 0.5-2GB of GPU memory, reducing the GPU's total available memory.\n",
    "\n",
    "    Here, we assume 1GB of overhead based on torch>=2.1.1 as discussed here:\n",
    "    https://github.com/stas00/ml-engineering/tree/master/training/performance#additional-gpu-memory-usage\n",
    "    \"\"\"\n",
    "    return 1 * 10**9 * buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_overhead(model_id: str, dtype: str = \"float16\") -> int:\n",
    "    \"\"\"\n",
    "    Get the model size in GB for a given model ID and data type.\n",
    "    \"\"\"\n",
    "    metadata = get_safetensors_metadata(model_id)\n",
    "\n",
    "    if hasattr(metadata, \"metadata\"):\n",
    "        if metadata.metadata.get(\"total_size\", False):\n",
    "            return metadata.metadata[\"total_size\"]\n",
    "    else:\n",
    "        num_params = list(metadata.parameter_count.values())[0]\n",
    "        num_params = int(num_params)\n",
    "        bytes = num_params * bytes_per_dtype[dtype]\n",
    "        return bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prefill Overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def calculate_mlp_overhead(\n",
    "    seq_len: int,\n",
    "    batch_size: int,\n",
    "    hidden_size: int,\n",
    "    intermediate_size: int,\n",
    "    dtype: str = \"float16\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate the prefill overhead for single MLP block/layer.\n",
    "\n",
    "    Accounts for following activations:\n",
    "        - Input to the first Linear layer\n",
    "        - Input to the activation function\n",
    "        - Input to the second Linear layer\n",
    "        - Dropout mask\n",
    "    \"\"\"\n",
    "\n",
    "    bytes_per_unit = bytes_per_dtype[dtype]\n",
    "\n",
    "    mlp_input = batch_size * seq_len * hidden_size * bytes_per_unit\n",
    "    act_input = batch_size * seq_len * intermediate_size * bytes_per_unit\n",
    "    down_proj_input = batch_size * seq_len * intermediate_size * bytes_per_unit\n",
    "    dropout_mask = (\n",
    "        batch_size * seq_len * hidden_size * bytes_per_dtype[\"int8\"]\n",
    "    )  # binary mask\n",
    "\n",
    "    mlp_block_bytes = mlp_input + act_input + down_proj_input + dropout_mask\n",
    "\n",
    "    return mlp_block_bytes\n",
    "\n",
    "\n",
    "def calculate_attention_overhead(\n",
    "    seq_len: int,\n",
    "    batch_size: int,\n",
    "    hidden_size: int,\n",
    "    head_dim: int,\n",
    "    num_attention_heads: int,\n",
    "    num_key_value_heads: int,\n",
    "    dtype: str = \"float16\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate the prefill overhead for single attention block/layer.\n",
    "    \"\"\"\n",
    "    bytes_per_unit = bytes_per_dtype[dtype]\n",
    "\n",
    "    attention_input = batch_size * seq_len * hidden_size * bytes_per_unit\n",
    "    q = batch_size * seq_len * head_dim * num_attention_heads * bytes_per_unit\n",
    "    k = batch_size * seq_len * head_dim * num_key_value_heads * bytes_per_unit\n",
    "    softmax_output = batch_size * num_attention_heads * seq_len**2 * bytes_per_unit\n",
    "    softmax_dropout_mask = (\n",
    "        batch_size * num_attention_heads * seq_len**2 * bytes_per_dtype[\"int8\"]\n",
    "    )\n",
    "    dropout_output = batch_size * num_attention_heads * seq_len**2 * bytes_per_unit\n",
    "    v = batch_size * seq_len * head_dim * num_key_value_heads * bytes_per_unit\n",
    "    out_proj_input = (\n",
    "        batch_size * seq_len * num_attention_heads * head_dim * bytes_per_unit\n",
    "    )\n",
    "    attention_dropout_mask = (\n",
    "        batch_size * seq_len * hidden_size * bytes_per_dtype[\"int8\"]\n",
    "    )\n",
    "\n",
    "    attention_block_bytes = (\n",
    "        attention_input\n",
    "        + q\n",
    "        + k\n",
    "        + softmax_output\n",
    "        + softmax_dropout_mask\n",
    "        + dropout_output\n",
    "        + v\n",
    "        + out_proj_input\n",
    "        + attention_dropout_mask\n",
    "    )\n",
    "\n",
    "    return attention_block_bytes\n",
    "\n",
    "\n",
    "def calculate_flash_attention_overhead(\n",
    "    seq_len: int,\n",
    "    batch_size: int,\n",
    "    hidden_size: int,\n",
    "    head_dim: int,\n",
    "    num_attention_heads: int,\n",
    "    num_key_value_heads: int,\n",
    "    dtype: str = \"float16\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate the prefill overhead for single attention block/layer.\n",
    "    \"\"\"\n",
    "    bytes_per_unit = bytes_per_dtype[dtype]\n",
    "\n",
    "    attention_input = batch_size * seq_len * hidden_size * bytes_per_unit\n",
    "    q = batch_size * seq_len * head_dim * num_attention_heads * bytes_per_unit\n",
    "    k = batch_size * seq_len * head_dim * num_key_value_heads * bytes_per_unit\n",
    "    v = batch_size * seq_len * head_dim * num_key_value_heads * bytes_per_unit\n",
    "\n",
    "    softmax_output = batch_size * num_attention_heads * seq_len * bytes_per_unit\n",
    "    out_proj_input = (\n",
    "        batch_size * seq_len * num_attention_heads * head_dim * bytes_per_unit\n",
    "    )\n",
    "\n",
    "    # not necessary, but conservative\n",
    "    softmax_dropout_mask = (\n",
    "        batch_size * num_attention_heads * seq_len * bytes_per_dtype[\"int8\"]\n",
    "    )\n",
    "    dropout_output = batch_size * num_attention_heads * seq_len * bytes_per_unit\n",
    "\n",
    "    attention_dropout_mask = (\n",
    "        batch_size * seq_len * hidden_size * bytes_per_dtype[\"int8\"]\n",
    "    )\n",
    "\n",
    "    attention_block_bytes = (\n",
    "        attention_input\n",
    "        + q\n",
    "        + k\n",
    "        + v\n",
    "        + softmax_output\n",
    "        + out_proj_input\n",
    "        # not necessary, but conservative\n",
    "        + softmax_dropout_mask\n",
    "        + dropout_output\n",
    "        + attention_dropout_mask\n",
    "    )\n",
    "\n",
    "    return attention_block_bytes\n",
    "\n",
    "\n",
    "def calculate_layernorm_overhead(\n",
    "    seq_len: int,\n",
    "    batch_size: int,\n",
    "    hidden_size: int,\n",
    "    dtype: str = \"float16\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate the prefill overhead for single layer norm layer.\n",
    "    \"\"\"\n",
    "    bytes_per_unit = bytes_per_dtype[dtype]\n",
    "    layernorm_bytes = batch_size * seq_len * hidden_size * bytes_per_unit\n",
    "\n",
    "    return layernorm_bytes\n",
    "\n",
    "\n",
    "def get_forward_pass_overhead(\n",
    "    config: AutoConfig,\n",
    "    max_batch_prefill_tokens: int,\n",
    "    max_input_tokens: int,\n",
    "    dtype: str = \"float16\",\n",
    "    is_decode: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate the prefill overhead for a given model and TGI config.\n",
    "\n",
    "    Size of a biggest tensor within forward pass.\n",
    "    It is estimated as the sum of all intermediate tensors within computation of a single layer.\n",
    "    Activations size have quadratic dependence on Sequence Length.\n",
    "\n",
    "    Reference:\n",
    "        - https://arxiv.org/pdf/2205.05198\n",
    "        - https://asmirnov.xyz/vram#fn1\n",
    "    \"\"\"\n",
    "    batch_size = math.ceil(max_batch_prefill_tokens / max_input_tokens)\n",
    "    head_dim = (\n",
    "        config.head_dim\n",
    "        if hasattr(config, \"head_dim\")\n",
    "        else config.hidden_size // config.num_key_value_heads\n",
    "    )\n",
    "\n",
    "    mlp_overhead = calculate_mlp_overhead(\n",
    "        seq_len=(max_input_tokens if not is_decode else 1),\n",
    "        batch_size=batch_size,\n",
    "        hidden_size=config.hidden_size,\n",
    "        intermediate_size=config.intermediate_size,\n",
    "        dtype=dtype,\n",
    "    )\n",
    "\n",
    "    attention_overhead = calculate_flash_attention_overhead(\n",
    "        seq_len=(max_input_tokens if not is_decode else 1),\n",
    "        batch_size=batch_size,\n",
    "        hidden_size=config.hidden_size,\n",
    "        head_dim=head_dim,\n",
    "        num_attention_heads=config.num_attention_heads,\n",
    "        num_key_value_heads=config.num_key_value_heads,\n",
    "        dtype=dtype,\n",
    "    )\n",
    "\n",
    "    layernorm_overhead = calculate_layernorm_overhead(\n",
    "        seq_len=(max_input_tokens if not is_decode else 1),\n",
    "        batch_size=batch_size,\n",
    "        hidden_size=config.hidden_size,\n",
    "        dtype=dtype,\n",
    "    )\n",
    "\n",
    "    return mlp_overhead + attention_overhead + (layernorm_overhead * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KV Cache Overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caclulate_kv_cache_memory_per_token(config: AutoConfig, dtype: str):\n",
    "    \"\"\"Calculates the memory required for the key-value cache per token in a Large Language Model (LLM).\"\"\"\n",
    "    dtype_bytes = bytes_per_dtype[dtype]\n",
    "    head_dim = (\n",
    "        config.head_dim\n",
    "        if hasattr(config, \"head_dim\")\n",
    "        else config.hidden_size // config.num_key_value_heads\n",
    "    )\n",
    "\n",
    "    bytes_per_token = (\n",
    "        2  # k & v\n",
    "        * config.num_hidden_layers\n",
    "        * config.num_key_value_heads\n",
    "        * head_dim\n",
    "        * dtype_bytes\n",
    "    )\n",
    "    return bytes_per_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate VRAM Overhead (mimic TGI logic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_vram_overhead(\n",
    "    model_id: str,\n",
    "    num_gpus: int,\n",
    "    vram_per_gpu: int,  # in GB\n",
    "    max_input_tokens: int,\n",
    "    max_total_tokens: int,\n",
    "    max_batch_prefill_tokens: int,\n",
    "    cuda_memory_fraction: float = 1.0,\n",
    "    dtype: str = \"float16\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Validate the VRAM overhead for a given model and TGI config.\n",
    "    \"\"\"\n",
    "    config = AutoConfig.from_pretrained(model_id)\n",
    "\n",
    "    # determine overhead\n",
    "    cuda_overhead = get_cuda_overhead() * num_gpus\n",
    "    model_overhead = get_model_overhead(model_id, dtype)\n",
    "    prefill_overhead = get_forward_pass_overhead(\n",
    "        config, max_batch_prefill_tokens, max_input_tokens, dtype\n",
    "    )\n",
    "    decode_overhead = get_forward_pass_overhead(\n",
    "        config, max_batch_prefill_tokens, max_input_tokens, dtype, is_decode=True\n",
    "    )\n",
    "    kv_cache_overhead_per_token = caclulate_kv_cache_memory_per_token(config, dtype)\n",
    "\n",
    "    # calculate free vram for kv cache\n",
    "    free_vram = vram_per_gpu * num_gpus * 10**9\n",
    "    free_vram = (\n",
    "        free_vram - cuda_overhead - model_overhead - prefill_overhead - decode_overhead\n",
    "    )\n",
    "    free_vram = (\n",
    "        free_vram * cuda_memory_fraction * 0.95\n",
    "    )  # by default, TGI allocates 95% of free VRAM to kv-cache\n",
    "\n",
    "    print(f\"Model Overhead: {bytes_to_gb(model_overhead)} GB\")\n",
    "    print(f\"CUDA Overhead: {bytes_to_gb(cuda_overhead)} GB\")\n",
    "    print(f\"Prefill Overhead: {bytes_to_gb(prefill_overhead)} GB\")\n",
    "    print(f\"Free VRAM for KV Cache: {bytes_to_gb(free_vram)} GB\")\n",
    "\n",
    "    # calculate token budget available for full kv cache\n",
    "    kv_cache_token_budget = int((free_vram) // kv_cache_overhead_per_token)\n",
    "\n",
    "    # check if we have enough vram for full kv cache (num_batches * max_total_tokens)\n",
    "    batch_size = math.ceil(max_batch_prefill_tokens / max_input_tokens)\n",
    "    kv_cache_tokens_needed = batch_size * (max_total_tokens - 1)\n",
    "    vram_needed_for_kv_cache = kv_cache_tokens_needed * kv_cache_overhead_per_token\n",
    "    is_enough_vram = kv_cache_token_budget >= kv_cache_tokens_needed\n",
    "\n",
    "    result = {\n",
    "        \"cuda_overhead\": bytes_to_gb(cuda_overhead),\n",
    "        \"model_overhead\": bytes_to_gb(model_overhead),\n",
    "        \"prefill_overhead\": bytes_to_gb(prefill_overhead),\n",
    "        \"kv_cache_overhead_per_token\": bytes_to_gb(kv_cache_overhead_per_token),\n",
    "        \"free_vram_for_kv_cache\": bytes_to_gb(free_vram),\n",
    "        \"vram_needed_for_kv_cache\": bytes_to_gb(vram_needed_for_kv_cache),\n",
    "        \"kv_cache_token_budget\": kv_cache_token_budget,\n",
    "        \"kv_cache_tokens_needed\": kv_cache_tokens_needed,\n",
    "        \"is_enough_vram\": is_enough_vram,\n",
    "    }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate VRAM Overhead (Simplified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_vram_overhead(\n",
    "    model_id: str,\n",
    "    num_gpus: int,\n",
    "    vram_per_gpu: int,  # in GB\n",
    "    max_input_tokens: int,\n",
    "    max_total_tokens: int,\n",
    "    max_batch_prefill_tokens: int,\n",
    "    cuda_memory_fraction: float = 1.0,\n",
    "    dtype: str = \"float16\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Validate the VRAM overhead for a given model and TGI config.\n",
    "    \"\"\"\n",
    "    config = AutoConfig.from_pretrained(model_id)\n",
    "\n",
    "    # calculate total vram\n",
    "    total_vram_available = (\n",
    "        (vram_per_gpu * num_gpus * 10**9) * cuda_memory_fraction * 0.95\n",
    "    )  # 95% of free VRAM\n",
    "\n",
    "    # determine overhead\n",
    "    cuda_overhead = get_cuda_overhead() * num_gpus\n",
    "    model_overhead = get_model_overhead(model_id, dtype)\n",
    "    prefill_overhead = get_forward_pass_overhead(\n",
    "        config, max_batch_prefill_tokens, max_input_tokens, dtype\n",
    "    )\n",
    "    decode_overhead = get_forward_pass_overhead(\n",
    "        config,\n",
    "        max_batch_prefill_tokens,\n",
    "        max_input_tokens,\n",
    "        dtype,\n",
    "        is_decode=True,\n",
    "    )\n",
    "\n",
    "    kv_cache_overhead_per_token = caclulate_kv_cache_memory_per_token(config, dtype)\n",
    "    batch_size = math.ceil(max_batch_prefill_tokens / max_input_tokens)\n",
    "    kv_cache_full_overhead = (\n",
    "        batch_size * (max_total_tokens - 1) * kv_cache_overhead_per_token\n",
    "    )\n",
    "\n",
    "    total_vram_needed = (\n",
    "        model_overhead\n",
    "        + cuda_overhead\n",
    "        + prefill_overhead\n",
    "        + kv_cache_full_overhead\n",
    "        + decode_overhead\n",
    "    )\n",
    "\n",
    "    # print(f\"Total VRAM Available: {bytes_to_gb(total_vram_available)} GB\")\n",
    "    # print(f\"Model Overhead: {bytes_to_gb(model_overhead)} GB\")\n",
    "    # print(f\"CUDA Overhead: {bytes_to_gb(cuda_overhead)} GB\")\n",
    "    # print(f\"Prefill Overhead: {bytes_to_gb(prefill_overhead)} GB\")\n",
    "    # print(f\"KV Cache Full Overhead: {bytes_to_gb(kv_cache_full_overhead)} GB\")\n",
    "\n",
    "    is_enough_vram = total_vram_available >= total_vram_needed\n",
    "\n",
    "    result = {\n",
    "        \"cuda_overhead\": bytes_to_gb(cuda_overhead),\n",
    "        \"model_overhead\": bytes_to_gb(model_overhead),\n",
    "        \"prefill_overhead\": bytes_to_gb(prefill_overhead),\n",
    "        \"decode_overhead\": bytes_to_gb(decode_overhead),\n",
    "        \"kv_cache_full_overhead\": bytes_to_gb(kv_cache_full_overhead),\n",
    "        \"total_vram_available\": bytes_to_gb(total_vram_available),\n",
    "        \"total_vram_needed\": bytes_to_gb(total_vram_needed),\n",
    "        \"is_enough_vram\": is_enough_vram,\n",
    "    }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parse safetensors files: 100%|██████████| 4/4 [00:00<00:00,  8.31it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cuda_overhead': 1.2,\n",
       " 'model_overhead': 16.06052,\n",
       " 'prefill_overhead': 3.15672,\n",
       " 'decode_overhead': 0.00036,\n",
       " 'kv_cache_full_overhead': 9.20755,\n",
       " 'total_vram_available': 22.8,\n",
       " 'total_vram_needed': 29.62515,\n",
       " 'is_enough_vram': False}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_gpus = 1\n",
    "max_input_tokens = 8750\n",
    "max_total_tokens = 8782\n",
    "max_batch_prefill_tokens = 8782\n",
    "\n",
    "# num_gpus = 2\n",
    "# max_input_tokens = 15520\n",
    "# max_total_tokens = 15554\n",
    "# max_batch_prefill_tokens = 15554\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "validate_vram_overhead(\n",
    "    model_id=model_id,\n",
    "    num_gpus=num_gpus,\n",
    "    vram_per_gpu=24,\n",
    "    max_input_tokens=max_input_tokens,\n",
    "    max_total_tokens=max_total_tokens,\n",
    "    max_batch_prefill_tokens=max_batch_prefill_tokens,\n",
    "    dtype=\"float16\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parse safetensors files: 100%|██████████| 4/4 [00:00<00:00,  4.90it/s]\n",
      "Parse safetensors files: 100%|██████████| 4/4 [00:00<00:00, 17.70it/s]\n",
      "Parse safetensors files: 100%|██████████| 4/4 [00:00<00:00,  4.21it/s]\n",
      "Parse safetensors files: 100%|██████████| 4/4 [00:00<00:00, 13.48it/s]\n",
      "Parse safetensors files: 100%|██████████| 4/4 [00:00<00:00, 19.32it/s]\n",
      "Parse safetensors files: 100%|██████████| 4/4 [00:00<00:00, 10.95it/s]\n",
      "Parse safetensors files: 100%|██████████| 4/4 [00:00<00:00, 23.10it/s]\n",
      "Parse safetensors files: 100%|██████████| 4/4 [00:00<00:00, 11.63it/s]\n",
      "Parse safetensors files: 100%|██████████| 4/4 [00:00<00:00, 17.61it/s]\n",
      "Parse safetensors files: 100%|██████████| 4/4 [00:00<00:00, 14.52it/s]\n",
      "Parse safetensors files: 100%|██████████| 4/4 [00:00<00:00, 11.28it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "metrics = []\n",
    "for tok in range(10000, 20001, 1000):\n",
    "\n",
    "    result = validate_vram_overhead(\n",
    "        model_id=model_id,\n",
    "        num_gpus=4,\n",
    "        vram_per_gpu=24,\n",
    "        max_input_tokens=tok - 1,\n",
    "        max_total_tokens=tok,\n",
    "        max_batch_prefill_tokens=tok,\n",
    "        dtype=\"float16\",\n",
    "    )\n",
    "    result[\"max_input_tokens\"] = tok - 1\n",
    "    metrics.append(result)\n",
    "\n",
    "df = pd.DataFrame(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuda_overhead</th>\n",
       "      <th>model_overhead</th>\n",
       "      <th>prefill_overhead</th>\n",
       "      <th>decode_overhead</th>\n",
       "      <th>kv_cache_full_overhead</th>\n",
       "      <th>total_vram_available</th>\n",
       "      <th>total_vram_needed</th>\n",
       "      <th>is_enough_vram</th>\n",
       "      <th>max_input_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.8</td>\n",
       "      <td>16.06052</td>\n",
       "      <td>3.60732</td>\n",
       "      <td>0.00036</td>\n",
       "      <td>10.48471</td>\n",
       "      <td>91.2</td>\n",
       "      <td>34.95291</td>\n",
       "      <td>True</td>\n",
       "      <td>9999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.8</td>\n",
       "      <td>16.06052</td>\n",
       "      <td>3.96809</td>\n",
       "      <td>0.00036</td>\n",
       "      <td>11.53329</td>\n",
       "      <td>91.2</td>\n",
       "      <td>36.36226</td>\n",
       "      <td>True</td>\n",
       "      <td>10999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.8</td>\n",
       "      <td>16.06052</td>\n",
       "      <td>4.32886</td>\n",
       "      <td>0.00036</td>\n",
       "      <td>12.58186</td>\n",
       "      <td>91.2</td>\n",
       "      <td>37.77160</td>\n",
       "      <td>True</td>\n",
       "      <td>11999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.8</td>\n",
       "      <td>16.06052</td>\n",
       "      <td>4.68962</td>\n",
       "      <td>0.00036</td>\n",
       "      <td>13.63044</td>\n",
       "      <td>91.2</td>\n",
       "      <td>39.18095</td>\n",
       "      <td>True</td>\n",
       "      <td>12999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.8</td>\n",
       "      <td>16.06052</td>\n",
       "      <td>5.05039</td>\n",
       "      <td>0.00036</td>\n",
       "      <td>14.67902</td>\n",
       "      <td>91.2</td>\n",
       "      <td>40.59029</td>\n",
       "      <td>True</td>\n",
       "      <td>13999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.8</td>\n",
       "      <td>16.06052</td>\n",
       "      <td>5.41116</td>\n",
       "      <td>0.00036</td>\n",
       "      <td>15.72759</td>\n",
       "      <td>91.2</td>\n",
       "      <td>41.99963</td>\n",
       "      <td>True</td>\n",
       "      <td>14999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.8</td>\n",
       "      <td>16.06052</td>\n",
       "      <td>5.77193</td>\n",
       "      <td>0.00036</td>\n",
       "      <td>16.77617</td>\n",
       "      <td>91.2</td>\n",
       "      <td>43.40898</td>\n",
       "      <td>True</td>\n",
       "      <td>15999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4.8</td>\n",
       "      <td>16.06052</td>\n",
       "      <td>6.13270</td>\n",
       "      <td>0.00036</td>\n",
       "      <td>17.82474</td>\n",
       "      <td>91.2</td>\n",
       "      <td>44.81832</td>\n",
       "      <td>True</td>\n",
       "      <td>16999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.8</td>\n",
       "      <td>16.06052</td>\n",
       "      <td>6.49346</td>\n",
       "      <td>0.00036</td>\n",
       "      <td>18.87332</td>\n",
       "      <td>91.2</td>\n",
       "      <td>46.22767</td>\n",
       "      <td>True</td>\n",
       "      <td>17999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.8</td>\n",
       "      <td>16.06052</td>\n",
       "      <td>6.85423</td>\n",
       "      <td>0.00036</td>\n",
       "      <td>19.92190</td>\n",
       "      <td>91.2</td>\n",
       "      <td>47.63701</td>\n",
       "      <td>True</td>\n",
       "      <td>18999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4.8</td>\n",
       "      <td>16.06052</td>\n",
       "      <td>7.21500</td>\n",
       "      <td>0.00036</td>\n",
       "      <td>20.97047</td>\n",
       "      <td>91.2</td>\n",
       "      <td>49.04635</td>\n",
       "      <td>True</td>\n",
       "      <td>19999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cuda_overhead  model_overhead  prefill_overhead  decode_overhead  \\\n",
       "0             4.8        16.06052           3.60732          0.00036   \n",
       "1             4.8        16.06052           3.96809          0.00036   \n",
       "2             4.8        16.06052           4.32886          0.00036   \n",
       "3             4.8        16.06052           4.68962          0.00036   \n",
       "4             4.8        16.06052           5.05039          0.00036   \n",
       "5             4.8        16.06052           5.41116          0.00036   \n",
       "6             4.8        16.06052           5.77193          0.00036   \n",
       "7             4.8        16.06052           6.13270          0.00036   \n",
       "8             4.8        16.06052           6.49346          0.00036   \n",
       "9             4.8        16.06052           6.85423          0.00036   \n",
       "10            4.8        16.06052           7.21500          0.00036   \n",
       "\n",
       "    kv_cache_full_overhead  total_vram_available  total_vram_needed  \\\n",
       "0                 10.48471                  91.2           34.95291   \n",
       "1                 11.53329                  91.2           36.36226   \n",
       "2                 12.58186                  91.2           37.77160   \n",
       "3                 13.63044                  91.2           39.18095   \n",
       "4                 14.67902                  91.2           40.59029   \n",
       "5                 15.72759                  91.2           41.99963   \n",
       "6                 16.77617                  91.2           43.40898   \n",
       "7                 17.82474                  91.2           44.81832   \n",
       "8                 18.87332                  91.2           46.22767   \n",
       "9                 19.92190                  91.2           47.63701   \n",
       "10                20.97047                  91.2           49.04635   \n",
       "\n",
       "    is_enough_vram  max_input_tokens  \n",
       "0             True              9999  \n",
       "1             True             10999  \n",
       "2             True             11999  \n",
       "3             True             12999  \n",
       "4             True             13999  \n",
       "5             True             14999  \n",
       "6             True             15999  \n",
       "7             True             16999  \n",
       "8             True             17999  \n",
       "9             True             18999  \n",
       "10            True             19999  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
