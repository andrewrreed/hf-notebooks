{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate VRAM usage for TGI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.10/site-packages (from transformers) (24.1)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Collecting safetensors>=0.4.1\n",
      "  Using cached safetensors-0.4.5-cp310-cp310-macosx_11_0_arm64.whl (381 kB)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2024.9.11-cp310-cp310-macosx_11_0_arm64.whl (284 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.6/284.6 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.23.2\n",
      "  Downloading huggingface_hub-0.25.1-py3-none-any.whl (436 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m436.4/436.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers<0.20,>=0.19\n",
      "  Using cached tokenizers-0.19.1-cp310-cp310-macosx_11_0_arm64.whl (2.4 MB)\n",
      "Collecting requests\n",
      "  Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Collecting tqdm>=4.27\n",
      "  Using cached tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Collecting numpy>=1.17\n",
      "  Using cached numpy-2.1.1-cp310-cp310-macosx_11_0_arm64.whl (13.8 MB)\n",
      "Collecting pyyaml>=5.1\n",
      "  Using cached PyYAML-6.0.2-cp310-cp310-macosx_11_0_arm64.whl (171 kB)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Using cached fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.3/126.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting idna<4,>=2.5\n",
      "  Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m567.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting charset-normalizer<4,>=2\n",
      "  Using cached charset_normalizer-3.3.2-cp310-cp310-macosx_11_0_arm64.whl (120 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Installing collected packages: urllib3, tqdm, safetensors, regex, pyyaml, numpy, idna, fsspec, filelock, charset-normalizer, certifi, requests, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed certifi-2024.8.30 charset-normalizer-3.3.2 filelock-3.16.1 fsspec-2024.9.0 huggingface-hub-0.25.1 idna-3.10 numpy-2.1.1 pyyaml-6.0.2 regex-2024.9.11 requests-2.32.3 safetensors-0.4.5 tokenizers-0.19.1 tqdm-4.66.5 transformers-4.44.2 urllib3-2.2.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewreed/Documents/hf-notebooks/tgi-vram-estimate/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "from huggingface_hub import get_safetensors_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bytes_per_dtype = {\"int4\": 0.5, \"int8\": 1, \"float8\": 1, \"float16\": 2, \"float32\": 4}\n",
    "\n",
    "\n",
    "def bytes_to_gb(bytes: int):\n",
    "    \"\"\"\n",
    "    Calculate the memory in GB for a given number of bytes.\n",
    "\n",
    "    Note:\n",
    "        - 1 GB = 10^9 bytes and 1 GiB = 2^30 bytes\n",
    "        - We estimate VRAM usage in GB for more conservative estimates\n",
    "    \"\"\"\n",
    "    return round((bytes) / 10**9, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA Overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cuda_overhead():\n",
    "    \"\"\"\n",
    "    When PyTorch uses CUDA for the first time, it may use up 0.5-2GB of GPU memory, reducing the GPU's total available memory.\n",
    "\n",
    "    Here, we assume 1GB of overhead based on torch>=2.1.1 as discussed here:\n",
    "    https://github.com/stas00/ml-engineering/tree/master/training/performance#additional-gpu-memory-usage\n",
    "    \"\"\"\n",
    "    return 1 * 10**9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_overhead(model_id: str, dtype: str = \"float16\") -> int:\n",
    "    \"\"\"\n",
    "    Get the model size in GB for a given model ID and data type.\n",
    "    \"\"\"\n",
    "    metadata = get_safetensors_metadata(model_id)\n",
    "\n",
    "    if hasattr(metadata, \"metadata\"):\n",
    "        if metadata.metadata.get(\"total_size\", False):\n",
    "            return metadata.metadata[\"total_size\"]\n",
    "    else:\n",
    "        num_params = list(metadata.parameter_count.values())[0]\n",
    "        num_params = int(num_params)\n",
    "        bytes = num_params * bytes_per_dtype[dtype]\n",
    "        return bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prefill Overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def calculate_mlp_prefill_overhead(\n",
    "    seq_len: int,\n",
    "    batch_size: int,\n",
    "    hidden_size: int,\n",
    "    intermediate_size: int,\n",
    "    dtype: str = \"float16\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate the prefill overhead for single MLP block/layer.\n",
    "\n",
    "    Accounts for following activations:\n",
    "        - Input to the first Linear layer\n",
    "        - Input to the activation function\n",
    "        - Input to the second Linear layer\n",
    "        - Dropout mask\n",
    "    \"\"\"\n",
    "\n",
    "    bytes_per_unit = bytes_per_dtype[dtype]\n",
    "\n",
    "    mlp_input = batch_size * seq_len * hidden_size * bytes_per_unit\n",
    "    act_input = batch_size * seq_len * intermediate_size * bytes_per_unit\n",
    "    down_proj_input = batch_size * seq_len * intermediate_size * bytes_per_unit\n",
    "    dropout_mask = (\n",
    "        batch_size * seq_len * hidden_size * bytes_per_dtype[\"int8\"]\n",
    "    )  # binary mask\n",
    "\n",
    "    mlp_block_bytes = mlp_input + act_input + down_proj_input + dropout_mask\n",
    "\n",
    "    return mlp_block_bytes\n",
    "\n",
    "\n",
    "def calculate_attention_prefill_overhead(\n",
    "    seq_len: int,\n",
    "    batch_size: int,\n",
    "    hidden_size: int,\n",
    "    head_dim: int,\n",
    "    num_attention_heads: int,\n",
    "    num_key_value_heads: int,\n",
    "    dtype: str = \"float16\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate the prefill overhead for single attention block/layer.\n",
    "    \"\"\"\n",
    "    bytes_per_unit = bytes_per_dtype[dtype]\n",
    "\n",
    "    attention_input = batch_size * seq_len * hidden_size * bytes_per_unit\n",
    "    q = batch_size * seq_len * head_dim * num_attention_heads * bytes_per_unit\n",
    "    k = batch_size * seq_len * head_dim * num_key_value_heads * bytes_per_unit\n",
    "    softmax_output = batch_size * num_attention_heads * seq_len**2 * bytes_per_unit\n",
    "    softmax_dropout_mask = (\n",
    "        batch_size * num_attention_heads * seq_len**2 * bytes_per_dtype[\"int8\"]\n",
    "    )\n",
    "    dropout_output = batch_size * num_attention_heads * seq_len**2 * bytes_per_unit\n",
    "    v = batch_size * seq_len * head_dim * num_key_value_heads * bytes_per_unit\n",
    "    out_proj_input = (\n",
    "        batch_size * seq_len * num_attention_heads * head_dim * bytes_per_unit\n",
    "    )\n",
    "    attention_dropout_mask = (\n",
    "        batch_size * seq_len * hidden_size * bytes_per_dtype[\"int8\"]\n",
    "    )\n",
    "\n",
    "    attention_block_bytes = (\n",
    "        attention_input\n",
    "        + q\n",
    "        + k\n",
    "        + softmax_output\n",
    "        + softmax_dropout_mask\n",
    "        + dropout_output\n",
    "        + v\n",
    "        + out_proj_input\n",
    "        + attention_dropout_mask\n",
    "    )\n",
    "\n",
    "    return attention_block_bytes\n",
    "\n",
    "\n",
    "def calculate_flash_attention_prefill_overhead(\n",
    "    seq_len: int,\n",
    "    batch_size: int,\n",
    "    hidden_size: int,\n",
    "    head_dim: int,\n",
    "    num_attention_heads: int,\n",
    "    num_key_value_heads: int,\n",
    "    dtype: str = \"float16\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate the prefill overhead for single attention block/layer.\n",
    "    \"\"\"\n",
    "    bytes_per_unit = bytes_per_dtype[dtype]\n",
    "\n",
    "    attention_input = batch_size * seq_len * hidden_size * bytes_per_unit\n",
    "    q = batch_size * seq_len * head_dim * num_attention_heads * bytes_per_unit\n",
    "    k = batch_size * seq_len * head_dim * num_key_value_heads * bytes_per_unit\n",
    "    v = batch_size * seq_len * head_dim * num_key_value_heads * bytes_per_unit\n",
    "\n",
    "    softmax_output = batch_size * num_attention_heads * seq_len * bytes_per_unit\n",
    "    softmax_dropout_mask = (\n",
    "        batch_size * num_attention_heads * seq_len * bytes_per_dtype[\"int8\"]\n",
    "    )\n",
    "    dropout_output = batch_size * num_attention_heads * seq_len * bytes_per_unit\n",
    "\n",
    "    out_proj_input = (\n",
    "        batch_size * seq_len * num_attention_heads * head_dim * bytes_per_unit\n",
    "    )\n",
    "    attention_dropout_mask = (\n",
    "        batch_size * seq_len * hidden_size * bytes_per_dtype[\"int8\"]\n",
    "    )\n",
    "\n",
    "    attention_block_bytes = (\n",
    "        attention_input\n",
    "        + q\n",
    "        + k\n",
    "        + v\n",
    "        + softmax_output\n",
    "        + softmax_dropout_mask\n",
    "        + dropout_output\n",
    "        + out_proj_input\n",
    "        + attention_dropout_mask\n",
    "    )\n",
    "\n",
    "    return attention_block_bytes\n",
    "\n",
    "\n",
    "def calculate_layernorm_prefill_overhead(\n",
    "    seq_len: int,\n",
    "    batch_size: int,\n",
    "    hidden_size: int,\n",
    "    dtype: str = \"float16\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate the prefill overhead for single layer norm layer.\n",
    "    \"\"\"\n",
    "    bytes_per_unit = bytes_per_dtype[dtype]\n",
    "    layernorm_bytes = batch_size * seq_len * hidden_size * bytes_per_unit\n",
    "\n",
    "    return layernorm_bytes\n",
    "\n",
    "\n",
    "def get_prefill_overhead(\n",
    "    config: AutoConfig,\n",
    "    max_batch_prefill_tokens: int,\n",
    "    max_input_tokens: int,\n",
    "    dtype: str = \"float16\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate the prefill overhead for a given model and TGI config.\n",
    "\n",
    "    Size of a biggest tensor within forward pass.\n",
    "    It is estimated as the sum of all intermediate tensors within computation of a single layer.\n",
    "    Activations size have quadratic dependence on Sequence Length.\n",
    "\n",
    "    Reference:\n",
    "        - https://arxiv.org/pdf/2205.05198\n",
    "        - https://asmirnov.xyz/vram#fn1\n",
    "    \"\"\"\n",
    "    batch_size = math.ceil(max_batch_prefill_tokens / max_input_tokens)\n",
    "    head_dim = (\n",
    "        config.head_dim\n",
    "        if hasattr(config, \"head_dim\")\n",
    "        else config.hidden_size // config.num_key_value_heads\n",
    "    )\n",
    "\n",
    "    mlp_overhead = calculate_mlp_prefill_overhead(\n",
    "        seq_len=max_input_tokens,\n",
    "        batch_size=batch_size,\n",
    "        hidden_size=config.hidden_size,\n",
    "        intermediate_size=config.intermediate_size,\n",
    "        dtype=dtype,\n",
    "    )\n",
    "\n",
    "    attention_overhead = calculate_flash_attention_prefill_overhead(\n",
    "        seq_len=max_input_tokens,\n",
    "        batch_size=batch_size,\n",
    "        hidden_size=config.hidden_size,\n",
    "        head_dim=head_dim,\n",
    "        num_attention_heads=config.num_attention_heads,\n",
    "        num_key_value_heads=config.num_key_value_heads,\n",
    "        dtype=dtype,\n",
    "    )\n",
    "\n",
    "    layernorm_overhead = calculate_layernorm_prefill_overhead(\n",
    "        seq_len=max_input_tokens,\n",
    "        batch_size=batch_size,\n",
    "        hidden_size=config.hidden_size,\n",
    "        dtype=dtype,\n",
    "    )\n",
    "\n",
    "    return mlp_overhead + attention_overhead + (layernorm_overhead * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KV Cache Overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caclulate_kv_cache_memory_per_token(config: AutoConfig, dtype: str):\n",
    "    \"\"\"Calculates the memory required for the key-value cache per token in a Large Language Model (LLM).\"\"\"\n",
    "    dtype_bytes = bytes_per_dtype[dtype]\n",
    "    head_dim = (\n",
    "        config.head_dim\n",
    "        if hasattr(config, \"head_dim\")\n",
    "        else config.hidden_size // config.num_key_value_heads\n",
    "    )\n",
    "\n",
    "    bytes_per_token = (\n",
    "        2  # k & v\n",
    "        * config.num_hidden_layers\n",
    "        * config.num_key_value_heads\n",
    "        * head_dim\n",
    "        * dtype_bytes\n",
    "    )\n",
    "    return bytes_per_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131072"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caclulate_kv_cache_memory_per_token(\n",
    "    config=AutoConfig.from_pretrained(\"mistralai/Mistral-7B-v0.1\"),\n",
    "    dtype=\"float16\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate VRAM usage (normal attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_vram_overhead(\n",
    "    model_id: str,\n",
    "    num_gpus: int,\n",
    "    vram_per_gpu: int,  # in GB\n",
    "    max_input_tokens: int,\n",
    "    max_total_tokens: int,\n",
    "    max_batch_prefill_tokens: int,\n",
    "    cuda_memory_fraction: float = 1.0,\n",
    "    dtype: str = \"float16\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Validate the VRAM overhead for a given model and TGI config.\n",
    "    \"\"\"\n",
    "    config = AutoConfig.from_pretrained(model_id)\n",
    "\n",
    "    # determine overhead\n",
    "    cuda_overhead = get_cuda_overhead() * num_gpus\n",
    "    model_overhead = get_model_overhead(model_id, dtype)\n",
    "    prefill_overhead = get_prefill_overhead(\n",
    "        config, max_batch_prefill_tokens, max_input_tokens, dtype\n",
    "    )\n",
    "    kv_cache_overhead_per_token = caclulate_kv_cache_memory_per_token(config, dtype)\n",
    "\n",
    "    # calculate free vram for kv cache\n",
    "    free_vram = vram_per_gpu * num_gpus * 10**9\n",
    "    free_vram = free_vram - cuda_overhead - model_overhead - prefill_overhead\n",
    "    free_vram = (\n",
    "        free_vram * cuda_memory_fraction * 0.95\n",
    "    )  # by default, TGI allocates 95% of free VRAM to kv-cache\n",
    "\n",
    "    print(f\"Model Overhead: {bytes_to_gb(model_overhead)} GB\")\n",
    "    print(f\"CUDA Overhead: {bytes_to_gb(cuda_overhead)} GB\")\n",
    "    print(f\"Prefill Overhead: {bytes_to_gb(prefill_overhead)} GB\")\n",
    "    print(f\"Free VRAM for KV Cache: {bytes_to_gb(free_vram)} GB\")\n",
    "\n",
    "    # calculate token budget available for full kv cache\n",
    "    kv_cache_token_budget = int((free_vram) // kv_cache_overhead_per_token)\n",
    "\n",
    "    # check if we have enough vram for full kv cache (num_batches * max_total_tokens)\n",
    "    batch_size = math.ceil(max_batch_prefill_tokens / max_input_tokens)\n",
    "    kv_cache_tokens_needed = batch_size * (max_total_tokens - 1)\n",
    "    vram_needed_for_kv_cache = kv_cache_tokens_needed * kv_cache_overhead_per_token\n",
    "    is_enough_vram = kv_cache_token_budget >= kv_cache_tokens_needed\n",
    "\n",
    "    result = {\n",
    "        \"cuda_overhead\": bytes_to_gb(cuda_overhead),\n",
    "        \"model_overhead\": bytes_to_gb(model_overhead),\n",
    "        \"prefill_overhead\": bytes_to_gb(prefill_overhead),\n",
    "        \"kv_cache_overhead_per_token\": bytes_to_gb(kv_cache_overhead_per_token),\n",
    "        \"free_vram_for_kv_cache\": bytes_to_gb(free_vram),\n",
    "        \"vram_needed_for_kv_cache\": bytes_to_gb(vram_needed_for_kv_cache),\n",
    "        \"kv_cache_token_budget\": kv_cache_token_budget,\n",
    "        \"kv_cache_tokens_needed\": kv_cache_tokens_needed,\n",
    "        \"is_enough_vram\": is_enough_vram,\n",
    "    }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parse safetensors files: 100%|██████████| 4/4 [00:00<00:00,  7.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Overhead: 16.06052 GB\n",
      "CUDA Overhead: 4.0 GB\n",
      "Prefill Overhead: 7.38348 GB\n",
      "Free VRAM for KV Cache: 65.1282 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cuda_overhead': 4.0,\n",
       " 'model_overhead': 16.06052,\n",
       " 'prefill_overhead': 7.38348,\n",
       " 'kv_cache_overhead_per_token': 0.00052,\n",
       " 'free_vram_for_kv_cache': 65.1282,\n",
       " 'vram_needed_for_kv_cache': 21.52622,\n",
       " 'kv_cache_token_budget': 124222,\n",
       " 'kv_cache_tokens_needed': 41058,\n",
       " 'is_enough_vram': True}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max_input_tokens = 8750\n",
    "# max_total_tokens = 8782\n",
    "# max_batch_prefill_tokens = 8782\n",
    "\n",
    "# max_input_tokens = 15520\n",
    "# max_total_tokens = 15554\n",
    "# max_batch_prefill_tokens = 15554\n",
    "\n",
    "max_input_tokens = 20466\n",
    "max_total_tokens = 20530\n",
    "max_batch_prefill_tokens = 20530\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "validate_vram_overhead(\n",
    "    model_id=model_id,\n",
    "    num_gpus=4,\n",
    "    vram_per_gpu=24,\n",
    "    max_input_tokens=max_input_tokens,\n",
    "    max_total_tokens=max_total_tokens,\n",
    "    max_batch_prefill_tokens=max_batch_prefill_tokens,\n",
    "    dtype=\"float16\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parse safetensors files: 100%|██████████| 4/4 [00:00<00:00,  7.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Overhead: 16.06052 GB\n",
      "CUDA Overhead: 4.0 GB\n",
      "Prefill Overhead: 11.07522 GB\n",
      "Free VRAM for KV Cache: 61.62105 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cuda_overhead': 4.0,\n",
       " 'model_overhead': 16.06052,\n",
       " 'prefill_overhead': 11.07522,\n",
       " 'kv_cache_overhead_per_token': 0.00052,\n",
       " 'free_vram_for_kv_cache': 61.62105,\n",
       " 'vram_needed_for_kv_cache': 32.28933,\n",
       " 'kv_cache_token_budget': 117532,\n",
       " 'kv_cache_tokens_needed': 61587,\n",
       " 'is_enough_vram': True}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_input_tokens = 20466\n",
    "max_total_tokens = 20530\n",
    "max_batch_prefill_tokens = 50000\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "validate_vram_overhead(\n",
    "    model_id=model_id,\n",
    "    num_gpus=4,\n",
    "    vram_per_gpu=24,\n",
    "    max_input_tokens=max_input_tokens,\n",
    "    max_total_tokens=max_total_tokens,\n",
    "    max_batch_prefill_tokens=max_batch_prefill_tokens,\n",
    "    dtype=\"float16\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
