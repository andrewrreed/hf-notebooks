{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate VRAM usage for TGI\n",
    "\n",
    "\n",
    "free_vram = total_gpu_vram - model_params_vram - prefill_overhead_vram - kv_cache_vram - cuda_overhead_vram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (751 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m751.2/751.2 kB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting numpy>=1.17\n",
      "  Downloading numpy-2.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tqdm>=4.27\n",
      "  Downloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Collecting tokenizers<0.20,>=0.19\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading regex-2024.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (782 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m782.7/782.7 kB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.10/site-packages (from transformers) (24.1)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2\n",
      "  Downloading huggingface_hub-0.25.1-py3-none-any.whl (436 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m436.4/436.4 kB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.4.1\n",
      "  Downloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.0/435.0 kB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests\n",
      "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.1/142.1 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.3/126.3 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting certifi>=2017.4.17\n",
      "  Downloading certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.3/167.3 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting idna<4,>=2.5\n",
      "  Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: urllib3, tqdm, safetensors, regex, pyyaml, numpy, idna, fsspec, filelock, charset-normalizer, certifi, requests, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed certifi-2024.8.30 charset-normalizer-3.3.2 filelock-3.16.1 fsspec-2024.9.0 huggingface-hub-0.25.1 idna-3.10 numpy-2.1.1 pyyaml-6.0.2 regex-2024.9.11 requests-2.32.3 safetensors-0.4.5 tokenizers-0.19.1 tqdm-4.66.5 transformers-4.44.2 urllib3-2.2.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "from huggingface_hub import get_safetensors_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "bytes_per_dtype = {\"int4\": 0.5, \"int8\": 1, \"float8\": 1, \"float16\": 2, \"float32\": 4}\n",
    "\n",
    "\n",
    "def bytes_to_gb(bytes: int):\n",
    "    \"\"\"\n",
    "    Calculate the memory in GB for a given number of bytes.\n",
    "\n",
    "    Note:\n",
    "        - 1 GB = 10^9 bytes and 1 GiB = 2^30 bytes\n",
    "        - We estimate VRAM usage in GB for more conservative estimates\n",
    "    \"\"\"\n",
    "    return round((bytes) / 10**9, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA Overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cuda_overhead():\n",
    "    \"\"\"\n",
    "    When PyTorch uses CUDA for the first time, it may use up 0.5-2GB of GPU memory, reducing the GPU's total available memory.\n",
    "\n",
    "    Here, we assume 1GB of overhead based on torch>=2.1.1 as discussed here:\n",
    "    https://github.com/stas00/ml-engineering/tree/master/training/performance#additional-gpu-memory-usage\n",
    "    \"\"\"\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_overhead(model_id: str, dtype: str = \"float16\") -> int:\n",
    "    \"\"\"\n",
    "    Get the model size in GB for a given model ID and data type.\n",
    "    \"\"\"\n",
    "    metadata = get_safetensors_metadata(model_id)\n",
    "\n",
    "    if hasattr(metadata, \"metadata\"):\n",
    "        if metadata.metadata.get(\"total_size\", False):\n",
    "            return bytes_to_gb(metadata.metadata[\"total_size\"])\n",
    "    else:\n",
    "        num_params = list(metadata.parameter_count.values())[0]\n",
    "        num_params = int(num_params)\n",
    "        bytes = num_params * bytes_per_dtype[dtype]\n",
    "        return bytes_to_gb(bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SafetensorsRepoMetadata(metadata={'total_size': 5559367680}, sharded=True, weight_map={'lm_head.bias': 'model-00002-of-00002.safetensors', 'lm_head.weight': 'model-00002-of-00002.safetensors', 'model.embed_tokens.weight': 'model-00001-of-00002.safetensors', 'model.final_layernorm.bias': 'model-00002-of-00002.safetensors', 'model.final_layernorm.weight': 'model-00002-of-00002.safetensors', 'model.layers.0.input_layernorm.bias': 'model-00001-of-00002.safetensors', 'model.layers.0.input_layernorm.weight': 'model-00001-of-00002.safetensors', 'model.layers.0.mlp.fc1.bias': 'model-00001-of-00002.safetensors', 'model.layers.0.mlp.fc1.weight': 'model-00001-of-00002.safetensors', 'model.layers.0.mlp.fc2.bias': 'model-00001-of-00002.safetensors', 'model.layers.0.mlp.fc2.weight': 'model-00001-of-00002.safetensors', 'model.layers.0.self_attn.dense.bias': 'model-00001-of-00002.safetensors', 'model.layers.0.self_attn.dense.weight': 'model-00001-of-00002.safetensors', 'model.layers.0.self_attn.k_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.0.self_attn.k_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.0.self_attn.q_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.0.self_attn.q_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.0.self_attn.v_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.0.self_attn.v_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.1.input_layernorm.bias': 'model-00001-of-00002.safetensors', 'model.layers.1.input_layernorm.weight': 'model-00001-of-00002.safetensors', 'model.layers.1.mlp.fc1.bias': 'model-00001-of-00002.safetensors', 'model.layers.1.mlp.fc1.weight': 'model-00001-of-00002.safetensors', 'model.layers.1.mlp.fc2.bias': 'model-00001-of-00002.safetensors', 'model.layers.1.mlp.fc2.weight': 'model-00001-of-00002.safetensors', 'model.layers.1.self_attn.dense.bias': 'model-00001-of-00002.safetensors', 'model.layers.1.self_attn.dense.weight': 'model-00001-of-00002.safetensors', 'model.layers.1.self_attn.k_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.1.self_attn.k_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.1.self_attn.q_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.1.self_attn.q_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.1.self_attn.v_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.1.self_attn.v_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.10.input_layernorm.bias': 'model-00001-of-00002.safetensors', 'model.layers.10.input_layernorm.weight': 'model-00001-of-00002.safetensors', 'model.layers.10.mlp.fc1.bias': 'model-00001-of-00002.safetensors', 'model.layers.10.mlp.fc1.weight': 'model-00001-of-00002.safetensors', 'model.layers.10.mlp.fc2.bias': 'model-00001-of-00002.safetensors', 'model.layers.10.mlp.fc2.weight': 'model-00001-of-00002.safetensors', 'model.layers.10.self_attn.dense.bias': 'model-00001-of-00002.safetensors', 'model.layers.10.self_attn.dense.weight': 'model-00001-of-00002.safetensors', 'model.layers.10.self_attn.k_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.10.self_attn.k_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.10.self_attn.q_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.10.self_attn.q_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.10.self_attn.v_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.10.self_attn.v_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.11.input_layernorm.bias': 'model-00001-of-00002.safetensors', 'model.layers.11.input_layernorm.weight': 'model-00001-of-00002.safetensors', 'model.layers.11.mlp.fc1.bias': 'model-00001-of-00002.safetensors', 'model.layers.11.mlp.fc1.weight': 'model-00001-of-00002.safetensors', 'model.layers.11.mlp.fc2.bias': 'model-00001-of-00002.safetensors', 'model.layers.11.mlp.fc2.weight': 'model-00001-of-00002.safetensors', 'model.layers.11.self_attn.dense.bias': 'model-00001-of-00002.safetensors', 'model.layers.11.self_attn.dense.weight': 'model-00001-of-00002.safetensors', 'model.layers.11.self_attn.k_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.11.self_attn.k_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.11.self_attn.q_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.11.self_attn.q_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.11.self_attn.v_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.11.self_attn.v_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.12.input_layernorm.bias': 'model-00001-of-00002.safetensors', 'model.layers.12.input_layernorm.weight': 'model-00001-of-00002.safetensors', 'model.layers.12.mlp.fc1.bias': 'model-00001-of-00002.safetensors', 'model.layers.12.mlp.fc1.weight': 'model-00001-of-00002.safetensors', 'model.layers.12.mlp.fc2.bias': 'model-00001-of-00002.safetensors', 'model.layers.12.mlp.fc2.weight': 'model-00001-of-00002.safetensors', 'model.layers.12.self_attn.dense.bias': 'model-00001-of-00002.safetensors', 'model.layers.12.self_attn.dense.weight': 'model-00001-of-00002.safetensors', 'model.layers.12.self_attn.k_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.12.self_attn.k_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.12.self_attn.q_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.12.self_attn.q_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.12.self_attn.v_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.12.self_attn.v_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.13.input_layernorm.bias': 'model-00001-of-00002.safetensors', 'model.layers.13.input_layernorm.weight': 'model-00001-of-00002.safetensors', 'model.layers.13.mlp.fc1.bias': 'model-00001-of-00002.safetensors', 'model.layers.13.mlp.fc1.weight': 'model-00001-of-00002.safetensors', 'model.layers.13.mlp.fc2.bias': 'model-00001-of-00002.safetensors', 'model.layers.13.mlp.fc2.weight': 'model-00001-of-00002.safetensors', 'model.layers.13.self_attn.dense.bias': 'model-00001-of-00002.safetensors', 'model.layers.13.self_attn.dense.weight': 'model-00001-of-00002.safetensors', 'model.layers.13.self_attn.k_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.13.self_attn.k_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.13.self_attn.q_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.13.self_attn.q_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.13.self_attn.v_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.13.self_attn.v_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.14.input_layernorm.bias': 'model-00001-of-00002.safetensors', 'model.layers.14.input_layernorm.weight': 'model-00001-of-00002.safetensors', 'model.layers.14.mlp.fc1.bias': 'model-00001-of-00002.safetensors', 'model.layers.14.mlp.fc1.weight': 'model-00001-of-00002.safetensors', 'model.layers.14.mlp.fc2.bias': 'model-00001-of-00002.safetensors', 'model.layers.14.mlp.fc2.weight': 'model-00001-of-00002.safetensors', 'model.layers.14.self_attn.dense.bias': 'model-00001-of-00002.safetensors', 'model.layers.14.self_attn.dense.weight': 'model-00001-of-00002.safetensors', 'model.layers.14.self_attn.k_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.14.self_attn.k_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.14.self_attn.q_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.14.self_attn.q_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.14.self_attn.v_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.14.self_attn.v_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.15.input_layernorm.bias': 'model-00001-of-00002.safetensors', 'model.layers.15.input_layernorm.weight': 'model-00001-of-00002.safetensors', 'model.layers.15.mlp.fc1.bias': 'model-00001-of-00002.safetensors', 'model.layers.15.mlp.fc1.weight': 'model-00001-of-00002.safetensors', 'model.layers.15.mlp.fc2.bias': 'model-00001-of-00002.safetensors', 'model.layers.15.mlp.fc2.weight': 'model-00001-of-00002.safetensors', 'model.layers.15.self_attn.dense.bias': 'model-00001-of-00002.safetensors', 'model.layers.15.self_attn.dense.weight': 'model-00001-of-00002.safetensors', 'model.layers.15.self_attn.k_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.15.self_attn.k_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.15.self_attn.q_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.15.self_attn.q_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.15.self_attn.v_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.15.self_attn.v_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.16.input_layernorm.bias': 'model-00001-of-00002.safetensors', 'model.layers.16.input_layernorm.weight': 'model-00001-of-00002.safetensors', 'model.layers.16.mlp.fc1.bias': 'model-00001-of-00002.safetensors', 'model.layers.16.mlp.fc1.weight': 'model-00001-of-00002.safetensors', 'model.layers.16.mlp.fc2.bias': 'model-00001-of-00002.safetensors', 'model.layers.16.mlp.fc2.weight': 'model-00001-of-00002.safetensors', 'model.layers.16.self_attn.dense.bias': 'model-00001-of-00002.safetensors', 'model.layers.16.self_attn.dense.weight': 'model-00001-of-00002.safetensors', 'model.layers.16.self_attn.k_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.16.self_attn.k_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.16.self_attn.q_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.16.self_attn.q_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.16.self_attn.v_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.16.self_attn.v_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.17.input_layernorm.bias': 'model-00001-of-00002.safetensors', 'model.layers.17.input_layernorm.weight': 'model-00001-of-00002.safetensors', 'model.layers.17.mlp.fc1.bias': 'model-00001-of-00002.safetensors', 'model.layers.17.mlp.fc1.weight': 'model-00001-of-00002.safetensors', 'model.layers.17.mlp.fc2.bias': 'model-00001-of-00002.safetensors', 'model.layers.17.mlp.fc2.weight': 'model-00001-of-00002.safetensors', 'model.layers.17.self_attn.dense.bias': 'model-00001-of-00002.safetensors', 'model.layers.17.self_attn.dense.weight': 'model-00001-of-00002.safetensors', 'model.layers.17.self_attn.k_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.17.self_attn.k_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.17.self_attn.q_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.17.self_attn.q_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.17.self_attn.v_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.17.self_attn.v_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.18.input_layernorm.bias': 'model-00001-of-00002.safetensors', 'model.layers.18.input_layernorm.weight': 'model-00001-of-00002.safetensors', 'model.layers.18.mlp.fc1.bias': 'model-00001-of-00002.safetensors', 'model.layers.18.mlp.fc1.weight': 'model-00001-of-00002.safetensors', 'model.layers.18.mlp.fc2.bias': 'model-00001-of-00002.safetensors', 'model.layers.18.mlp.fc2.weight': 'model-00001-of-00002.safetensors', 'model.layers.18.self_attn.dense.bias': 'model-00001-of-00002.safetensors', 'model.layers.18.self_attn.dense.weight': 'model-00001-of-00002.safetensors', 'model.layers.18.self_attn.k_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.18.self_attn.k_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.18.self_attn.q_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.18.self_attn.q_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.18.self_attn.v_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.18.self_attn.v_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.19.input_layernorm.bias': 'model-00001-of-00002.safetensors', 'model.layers.19.input_layernorm.weight': 'model-00001-of-00002.safetensors', 'model.layers.19.mlp.fc1.bias': 'model-00001-of-00002.safetensors', 'model.layers.19.mlp.fc1.weight': 'model-00001-of-00002.safetensors', 'model.layers.19.mlp.fc2.bias': 'model-00001-of-00002.safetensors', 'model.layers.19.mlp.fc2.weight': 'model-00001-of-00002.safetensors', 'model.layers.19.self_attn.dense.bias': 'model-00001-of-00002.safetensors', 'model.layers.19.self_attn.dense.weight': 'model-00001-of-00002.safetensors', 'model.layers.19.self_attn.k_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.19.self_attn.k_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.19.self_attn.q_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.19.self_attn.q_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.19.self_attn.v_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.19.self_attn.v_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.2.input_layernorm.bias': 'model-00001-of-00002.safetensors', 'model.layers.2.input_layernorm.weight': 'model-00001-of-00002.safetensors', 'model.layers.2.mlp.fc1.bias': 'model-00001-of-00002.safetensors', 'model.layers.2.mlp.fc1.weight': 'model-00001-of-00002.safetensors', 'model.layers.2.mlp.fc2.bias': 'model-00001-of-00002.safetensors', 'model.layers.2.mlp.fc2.weight': 'model-00001-of-00002.safetensors', 'model.layers.2.self_attn.dense.bias': 'model-00001-of-00002.safetensors', 'model.layers.2.self_attn.dense.weight': 'model-00001-of-00002.safetensors', 'model.layers.2.self_attn.k_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.2.self_attn.k_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.2.self_attn.q_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.2.self_attn.q_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.2.self_attn.v_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.2.self_attn.v_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.20.input_layernorm.bias': 'model-00001-of-00002.safetensors', 'model.layers.20.input_layernorm.weight': 'model-00001-of-00002.safetensors', 'model.layers.20.mlp.fc1.bias': 'model-00001-of-00002.safetensors', 'model.layers.20.mlp.fc1.weight': 'model-00001-of-00002.safetensors', 'model.layers.20.mlp.fc2.bias': 'model-00001-of-00002.safetensors', 'model.layers.20.mlp.fc2.weight': 'model-00001-of-00002.safetensors', 'model.layers.20.self_attn.dense.bias': 'model-00001-of-00002.safetensors', 'model.layers.20.self_attn.dense.weight': 'model-00001-of-00002.safetensors', 'model.layers.20.self_attn.k_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.20.self_attn.k_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.20.self_attn.q_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.20.self_attn.q_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.20.self_attn.v_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.20.self_attn.v_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.21.input_layernorm.bias': 'model-00001-of-00002.safetensors', 'model.layers.21.input_layernorm.weight': 'model-00001-of-00002.safetensors', 'model.layers.21.mlp.fc1.bias': 'model-00001-of-00002.safetensors', 'model.layers.21.mlp.fc1.weight': 'model-00001-of-00002.safetensors', 'model.layers.21.mlp.fc2.bias': 'model-00001-of-00002.safetensors', 'model.layers.21.mlp.fc2.weight': 'model-00001-of-00002.safetensors', 'model.layers.21.self_attn.dense.bias': 'model-00001-of-00002.safetensors', 'model.layers.21.self_attn.dense.weight': 'model-00001-of-00002.safetensors', 'model.layers.21.self_attn.k_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.21.self_attn.k_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.21.self_attn.q_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.21.self_attn.q_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.21.self_attn.v_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.21.self_attn.v_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.22.input_layernorm.bias': 'model-00001-of-00002.safetensors', 'model.layers.22.input_layernorm.weight': 'model-00001-of-00002.safetensors', 'model.layers.22.mlp.fc1.bias': 'model-00001-of-00002.safetensors', 'model.layers.22.mlp.fc1.weight': 'model-00001-of-00002.safetensors', 'model.layers.22.mlp.fc2.bias': 'model-00001-of-00002.safetensors', 'model.layers.22.mlp.fc2.weight': 'model-00001-of-00002.safetensors', 'model.layers.22.self_attn.dense.bias': 'model-00001-of-00002.safetensors', 'model.layers.22.self_attn.dense.weight': 'model-00001-of-00002.safetensors', 'model.layers.22.self_attn.k_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.22.self_attn.k_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.22.self_attn.q_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.22.self_attn.q_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.22.self_attn.v_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.22.self_attn.v_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.23.input_layernorm.bias': 'model-00001-of-00002.safetensors', 'model.layers.23.input_layernorm.weight': 'model-00001-of-00002.safetensors', 'model.layers.23.mlp.fc1.bias': 'model-00001-of-00002.safetensors', 'model.layers.23.mlp.fc1.weight': 'model-00001-of-00002.safetensors', 'model.layers.23.mlp.fc2.bias': 'model-00001-of-00002.safetensors', 'model.layers.23.mlp.fc2.weight': 'model-00001-of-00002.safetensors', 'model.layers.23.self_attn.dense.bias': 'model-00001-of-00002.safetensors', 'model.layers.23.self_attn.dense.weight': 'model-00001-of-00002.safetensors', 'model.layers.23.self_attn.k_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.23.self_attn.k_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.23.self_attn.q_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.23.self_attn.q_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.23.self_attn.v_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.23.self_attn.v_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.24.input_layernorm.bias': 'model-00001-of-00002.safetensors', 'model.layers.24.input_layernorm.weight': 'model-00001-of-00002.safetensors', 'model.layers.24.mlp.fc1.bias': 'model-00001-of-00002.safetensors', 'model.layers.24.mlp.fc1.weight': 'model-00001-of-00002.safetensors', 'model.layers.24.mlp.fc2.bias': 'model-00001-of-00002.safetensors', 'model.layers.24.mlp.fc2.weight': 'model-00001-of-00002.safetensors', 'model.layers.24.self_attn.dense.bias': 'model-00001-of-00002.safetensors', 'model.layers.24.self_attn.dense.weight': 'model-00001-of-00002.safetensors', 'model.layers.24.self_attn.k_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.24.self_attn.k_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.24.self_attn.q_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.24.self_attn.q_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.24.self_attn.v_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.24.self_attn.v_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.25.input_layernorm.bias': 'model-00001-of-00002.safetensors', 'model.layers.25.input_layernorm.weight': 'model-00001-of-00002.safetensors', 'model.layers.25.mlp.fc1.bias': 'model-00001-of-00002.safetensors', 'model.layers.25.mlp.fc1.weight': 'model-00001-of-00002.safetensors', 'model.layers.25.mlp.fc2.bias': 'model-00001-of-00002.safetensors', 'model.layers.25.mlp.fc2.weight': 'model-00001-of-00002.safetensors', 'model.layers.25.self_attn.dense.bias': 'model-00001-of-00002.safetensors', 'model.layers.25.self_attn.dense.weight': 'model-00001-of-00002.safetensors', 'model.layers.25.self_attn.k_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.25.self_attn.k_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.25.self_attn.q_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.25.self_attn.q_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.25.self_attn.v_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.25.self_attn.v_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.26.input_layernorm.bias': 'model-00001-of-00002.safetensors', 'model.layers.26.input_layernorm.weight': 'model-00001-of-00002.safetensors', 'model.layers.26.mlp.fc1.bias': 'model-00001-of-00002.safetensors', 'model.layers.26.mlp.fc1.weight': 'model-00001-of-00002.safetensors', 'model.layers.26.mlp.fc2.bias': 'model-00001-of-00002.safetensors', 'model.layers.26.mlp.fc2.weight': 'model-00001-of-00002.safetensors', 'model.layers.26.self_attn.dense.bias': 'model-00001-of-00002.safetensors', 'model.layers.26.self_attn.dense.weight': 'model-00001-of-00002.safetensors', 'model.layers.26.self_attn.k_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.26.self_attn.k_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.26.self_attn.q_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.26.self_attn.q_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.26.self_attn.v_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.26.self_attn.v_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.27.input_layernorm.bias': 'model-00001-of-00002.safetensors', 'model.layers.27.input_layernorm.weight': 'model-00001-of-00002.safetensors', 'model.layers.27.mlp.fc1.bias': 'model-00001-of-00002.safetensors', 'model.layers.27.mlp.fc1.weight': 'model-00001-of-00002.safetensors', 'model.layers.27.mlp.fc2.bias': 'model-00001-of-00002.safetensors', 'model.layers.27.mlp.fc2.weight': 'model-00001-of-00002.safetensors', 'model.layers.27.self_attn.dense.bias': 'model-00001-of-00002.safetensors', 'model.layers.27.self_attn.dense.weight': 'model-00001-of-00002.safetensors', 'model.layers.27.self_attn.k_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.27.self_attn.k_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.27.self_attn.q_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.27.self_attn.q_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.27.self_attn.v_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.27.self_attn.v_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.28.input_layernorm.bias': 'model-00001-of-00002.safetensors', 'model.layers.28.input_layernorm.weight': 'model-00001-of-00002.safetensors', 'model.layers.28.mlp.fc1.bias': 'model-00001-of-00002.safetensors', 'model.layers.28.mlp.fc1.weight': 'model-00001-of-00002.safetensors', 'model.layers.28.mlp.fc2.bias': 'model-00001-of-00002.safetensors', 'model.layers.28.mlp.fc2.weight': 'model-00001-of-00002.safetensors', 'model.layers.28.self_attn.dense.bias': 'model-00001-of-00002.safetensors', 'model.layers.28.self_attn.dense.weight': 'model-00001-of-00002.safetensors', 'model.layers.28.self_attn.k_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.28.self_attn.k_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.28.self_attn.q_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.28.self_attn.q_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.28.self_attn.v_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.28.self_attn.v_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.29.input_layernorm.bias': 'model-00001-of-00002.safetensors', 'model.layers.29.input_layernorm.weight': 'model-00001-of-00002.safetensors', 'model.layers.29.mlp.fc1.bias': 'model-00001-of-00002.safetensors', 'model.layers.29.mlp.fc1.weight': 'model-00001-of-00002.safetensors', 'model.layers.29.mlp.fc2.bias': 'model-00001-of-00002.safetensors', 'model.layers.29.mlp.fc2.weight': 'model-00001-of-00002.safetensors', 'model.layers.29.self_attn.dense.bias': 'model-00001-of-00002.safetensors', 'model.layers.29.self_attn.dense.weight': 'model-00001-of-00002.safetensors', 'model.layers.29.self_attn.k_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.29.self_attn.k_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.29.self_attn.q_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.29.self_attn.q_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.29.self_attn.v_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.29.self_attn.v_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.3.input_layernorm.bias': 'model-00001-of-00002.safetensors', 'model.layers.3.input_layernorm.weight': 'model-00001-of-00002.safetensors', 'model.layers.3.mlp.fc1.bias': 'model-00001-of-00002.safetensors', 'model.layers.3.mlp.fc1.weight': 'model-00001-of-00002.safetensors', 'model.layers.3.mlp.fc2.bias': 'model-00001-of-00002.safetensors', 'model.layers.3.mlp.fc2.weight': 'model-00001-of-00002.safetensors', 'model.layers.3.self_attn.dense.bias': 'model-00001-of-00002.safetensors', 'model.layers.3.self_attn.dense.weight': 'model-00001-of-00002.safetensors', 'model.layers.3.self_attn.k_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.3.self_attn.k_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.3.self_attn.q_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.3.self_attn.q_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.3.self_attn.v_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.3.self_attn.v_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.30.input_layernorm.bias': 'model-00002-of-00002.safetensors', 'model.layers.30.input_layernorm.weight': 'model-00002-of-00002.safetensors', 'model.layers.30.mlp.fc1.bias': 'model-00002-of-00002.safetensors', 'model.layers.30.mlp.fc1.weight': 'model-00002-of-00002.safetensors', 'model.layers.30.mlp.fc2.bias': 'model-00002-of-00002.safetensors', 'model.layers.30.mlp.fc2.weight': 'model-00002-of-00002.safetensors', 'model.layers.30.self_attn.dense.bias': 'model-00002-of-00002.safetensors', 'model.layers.30.self_attn.dense.weight': 'model-00002-of-00002.safetensors', 'model.layers.30.self_attn.k_proj.bias': 'model-00002-of-00002.safetensors', 'model.layers.30.self_attn.k_proj.weight': 'model-00002-of-00002.safetensors', 'model.layers.30.self_attn.q_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.30.self_attn.q_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.30.self_attn.v_proj.bias': 'model-00002-of-00002.safetensors', 'model.layers.30.self_attn.v_proj.weight': 'model-00002-of-00002.safetensors', 'model.layers.31.input_layernorm.bias': 'model-00002-of-00002.safetensors', 'model.layers.31.input_layernorm.weight': 'model-00002-of-00002.safetensors', 'model.layers.31.mlp.fc1.bias': 'model-00002-of-00002.safetensors', 'model.layers.31.mlp.fc1.weight': 'model-00002-of-00002.safetensors', 'model.layers.31.mlp.fc2.bias': 'model-00002-of-00002.safetensors', 'model.layers.31.mlp.fc2.weight': 'model-00002-of-00002.safetensors', 'model.layers.31.self_attn.dense.bias': 'model-00002-of-00002.safetensors', 'model.layers.31.self_attn.dense.weight': 'model-00002-of-00002.safetensors', 'model.layers.31.self_attn.k_proj.bias': 'model-00002-of-00002.safetensors', 'model.layers.31.self_attn.k_proj.weight': 'model-00002-of-00002.safetensors', 'model.layers.31.self_attn.q_proj.bias': 'model-00002-of-00002.safetensors', 'model.layers.31.self_attn.q_proj.weight': 'model-00002-of-00002.safetensors', 'model.layers.31.self_attn.v_proj.bias': 'model-00002-of-00002.safetensors', 'model.layers.31.self_attn.v_proj.weight': 'model-00002-of-00002.safetensors', 'model.layers.4.input_layernorm.bias': 'model-00001-of-00002.safetensors', 'model.layers.4.input_layernorm.weight': 'model-00001-of-00002.safetensors', 'model.layers.4.mlp.fc1.bias': 'model-00001-of-00002.safetensors', 'model.layers.4.mlp.fc1.weight': 'model-00001-of-00002.safetensors', 'model.layers.4.mlp.fc2.bias': 'model-00001-of-00002.safetensors', 'model.layers.4.mlp.fc2.weight': 'model-00001-of-00002.safetensors', 'model.layers.4.self_attn.dense.bias': 'model-00001-of-00002.safetensors', 'model.layers.4.self_attn.dense.weight': 'model-00001-of-00002.safetensors', 'model.layers.4.self_attn.k_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.4.self_attn.k_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.4.self_attn.q_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.4.self_attn.q_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.4.self_attn.v_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.4.self_attn.v_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.5.input_layernorm.bias': 'model-00001-of-00002.safetensors', 'model.layers.5.input_layernorm.weight': 'model-00001-of-00002.safetensors', 'model.layers.5.mlp.fc1.bias': 'model-00001-of-00002.safetensors', 'model.layers.5.mlp.fc1.weight': 'model-00001-of-00002.safetensors', 'model.layers.5.mlp.fc2.bias': 'model-00001-of-00002.safetensors', 'model.layers.5.mlp.fc2.weight': 'model-00001-of-00002.safetensors', 'model.layers.5.self_attn.dense.bias': 'model-00001-of-00002.safetensors', 'model.layers.5.self_attn.dense.weight': 'model-00001-of-00002.safetensors', 'model.layers.5.self_attn.k_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.5.self_attn.k_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.5.self_attn.q_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.5.self_attn.q_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.5.self_attn.v_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.5.self_attn.v_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.6.input_layernorm.bias': 'model-00001-of-00002.safetensors', 'model.layers.6.input_layernorm.weight': 'model-00001-of-00002.safetensors', 'model.layers.6.mlp.fc1.bias': 'model-00001-of-00002.safetensors', 'model.layers.6.mlp.fc1.weight': 'model-00001-of-00002.safetensors', 'model.layers.6.mlp.fc2.bias': 'model-00001-of-00002.safetensors', 'model.layers.6.mlp.fc2.weight': 'model-00001-of-00002.safetensors', 'model.layers.6.self_attn.dense.bias': 'model-00001-of-00002.safetensors', 'model.layers.6.self_attn.dense.weight': 'model-00001-of-00002.safetensors', 'model.layers.6.self_attn.k_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.6.self_attn.k_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.6.self_attn.q_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.6.self_attn.q_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.6.self_attn.v_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.6.self_attn.v_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.7.input_layernorm.bias': 'model-00001-of-00002.safetensors', 'model.layers.7.input_layernorm.weight': 'model-00001-of-00002.safetensors', 'model.layers.7.mlp.fc1.bias': 'model-00001-of-00002.safetensors', 'model.layers.7.mlp.fc1.weight': 'model-00001-of-00002.safetensors', 'model.layers.7.mlp.fc2.bias': 'model-00001-of-00002.safetensors', 'model.layers.7.mlp.fc2.weight': 'model-00001-of-00002.safetensors', 'model.layers.7.self_attn.dense.bias': 'model-00001-of-00002.safetensors', 'model.layers.7.self_attn.dense.weight': 'model-00001-of-00002.safetensors', 'model.layers.7.self_attn.k_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.7.self_attn.k_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.7.self_attn.q_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.7.self_attn.q_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.7.self_attn.v_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.7.self_attn.v_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.8.input_layernorm.bias': 'model-00001-of-00002.safetensors', 'model.layers.8.input_layernorm.weight': 'model-00001-of-00002.safetensors', 'model.layers.8.mlp.fc1.bias': 'model-00001-of-00002.safetensors', 'model.layers.8.mlp.fc1.weight': 'model-00001-of-00002.safetensors', 'model.layers.8.mlp.fc2.bias': 'model-00001-of-00002.safetensors', 'model.layers.8.mlp.fc2.weight': 'model-00001-of-00002.safetensors', 'model.layers.8.self_attn.dense.bias': 'model-00001-of-00002.safetensors', 'model.layers.8.self_attn.dense.weight': 'model-00001-of-00002.safetensors', 'model.layers.8.self_attn.k_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.8.self_attn.k_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.8.self_attn.q_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.8.self_attn.q_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.8.self_attn.v_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.8.self_attn.v_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.9.input_layernorm.bias': 'model-00001-of-00002.safetensors', 'model.layers.9.input_layernorm.weight': 'model-00001-of-00002.safetensors', 'model.layers.9.mlp.fc1.bias': 'model-00001-of-00002.safetensors', 'model.layers.9.mlp.fc1.weight': 'model-00001-of-00002.safetensors', 'model.layers.9.mlp.fc2.bias': 'model-00001-of-00002.safetensors', 'model.layers.9.mlp.fc2.weight': 'model-00001-of-00002.safetensors', 'model.layers.9.self_attn.dense.bias': 'model-00001-of-00002.safetensors', 'model.layers.9.self_attn.dense.weight': 'model-00001-of-00002.safetensors', 'model.layers.9.self_attn.k_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.9.self_attn.k_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.9.self_attn.q_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.9.self_attn.q_proj.weight': 'model-00001-of-00002.safetensors', 'model.layers.9.self_attn.v_proj.bias': 'model-00001-of-00002.safetensors', 'model.layers.9.self_attn.v_proj.weight': 'model-00001-of-00002.safetensors'}, files_metadata={'model-00001-of-00002.safetensors': SafetensorsFileMetadata(metadata={'format': 'pt'}, tensors={'model.embed_tokens.weight': TensorInfo(dtype='F16', shape=[51200, 2560], data_offsets=(0, 262144000), parameter_count=131072000), 'model.layers.0.input_layernorm.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(262144000, 262149120), parameter_count=2560), 'model.layers.0.input_layernorm.weight': TensorInfo(dtype='F16', shape=[2560], data_offsets=(262149120, 262154240), parameter_count=2560), 'model.layers.0.mlp.fc1.bias': TensorInfo(dtype='F16', shape=[10240], data_offsets=(262154240, 262174720), parameter_count=10240), 'model.layers.0.mlp.fc1.weight': TensorInfo(dtype='F16', shape=[10240, 2560], data_offsets=(262174720, 314603520), parameter_count=26214400), 'model.layers.0.mlp.fc2.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(314603520, 314608640), parameter_count=2560), 'model.layers.0.mlp.fc2.weight': TensorInfo(dtype='F16', shape=[2560, 10240], data_offsets=(314608640, 367037440), parameter_count=26214400), 'model.layers.0.self_attn.dense.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(367037440, 367042560), parameter_count=2560), 'model.layers.0.self_attn.dense.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(367042560, 380149760), parameter_count=6553600), 'model.layers.0.self_attn.k_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(380149760, 380154880), parameter_count=2560), 'model.layers.0.self_attn.k_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(380154880, 393262080), parameter_count=6553600), 'model.layers.0.self_attn.q_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(393262080, 393267200), parameter_count=2560), 'model.layers.0.self_attn.q_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(393267200, 406374400), parameter_count=6553600), 'model.layers.0.self_attn.v_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(406374400, 406379520), parameter_count=2560), 'model.layers.0.self_attn.v_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(406379520, 419486720), parameter_count=6553600), 'model.layers.1.input_layernorm.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(419486720, 419491840), parameter_count=2560), 'model.layers.1.input_layernorm.weight': TensorInfo(dtype='F16', shape=[2560], data_offsets=(419491840, 419496960), parameter_count=2560), 'model.layers.1.mlp.fc1.bias': TensorInfo(dtype='F16', shape=[10240], data_offsets=(419496960, 419517440), parameter_count=10240), 'model.layers.1.mlp.fc1.weight': TensorInfo(dtype='F16', shape=[10240, 2560], data_offsets=(419517440, 471946240), parameter_count=26214400), 'model.layers.1.mlp.fc2.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(471946240, 471951360), parameter_count=2560), 'model.layers.1.mlp.fc2.weight': TensorInfo(dtype='F16', shape=[2560, 10240], data_offsets=(471951360, 524380160), parameter_count=26214400), 'model.layers.1.self_attn.dense.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(524380160, 524385280), parameter_count=2560), 'model.layers.1.self_attn.dense.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(524385280, 537492480), parameter_count=6553600), 'model.layers.1.self_attn.k_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(537492480, 537497600), parameter_count=2560), 'model.layers.1.self_attn.k_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(537497600, 550604800), parameter_count=6553600), 'model.layers.1.self_attn.q_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(550604800, 550609920), parameter_count=2560), 'model.layers.1.self_attn.q_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(550609920, 563717120), parameter_count=6553600), 'model.layers.1.self_attn.v_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(563717120, 563722240), parameter_count=2560), 'model.layers.1.self_attn.v_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(563722240, 576829440), parameter_count=6553600), 'model.layers.10.input_layernorm.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(576829440, 576834560), parameter_count=2560), 'model.layers.10.input_layernorm.weight': TensorInfo(dtype='F16', shape=[2560], data_offsets=(576834560, 576839680), parameter_count=2560), 'model.layers.10.mlp.fc1.bias': TensorInfo(dtype='F16', shape=[10240], data_offsets=(576839680, 576860160), parameter_count=10240), 'model.layers.10.mlp.fc1.weight': TensorInfo(dtype='F16', shape=[10240, 2560], data_offsets=(576860160, 629288960), parameter_count=26214400), 'model.layers.10.mlp.fc2.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(629288960, 629294080), parameter_count=2560), 'model.layers.10.mlp.fc2.weight': TensorInfo(dtype='F16', shape=[2560, 10240], data_offsets=(629294080, 681722880), parameter_count=26214400), 'model.layers.10.self_attn.dense.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(681722880, 681728000), parameter_count=2560), 'model.layers.10.self_attn.dense.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(681728000, 694835200), parameter_count=6553600), 'model.layers.10.self_attn.k_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(694835200, 694840320), parameter_count=2560), 'model.layers.10.self_attn.k_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(694840320, 707947520), parameter_count=6553600), 'model.layers.10.self_attn.q_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(707947520, 707952640), parameter_count=2560), 'model.layers.10.self_attn.q_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(707952640, 721059840), parameter_count=6553600), 'model.layers.10.self_attn.v_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(721059840, 721064960), parameter_count=2560), 'model.layers.10.self_attn.v_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(721064960, 734172160), parameter_count=6553600), 'model.layers.11.input_layernorm.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(734172160, 734177280), parameter_count=2560), 'model.layers.11.input_layernorm.weight': TensorInfo(dtype='F16', shape=[2560], data_offsets=(734177280, 734182400), parameter_count=2560), 'model.layers.11.mlp.fc1.bias': TensorInfo(dtype='F16', shape=[10240], data_offsets=(734182400, 734202880), parameter_count=10240), 'model.layers.11.mlp.fc1.weight': TensorInfo(dtype='F16', shape=[10240, 2560], data_offsets=(734202880, 786631680), parameter_count=26214400), 'model.layers.11.mlp.fc2.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(786631680, 786636800), parameter_count=2560), 'model.layers.11.mlp.fc2.weight': TensorInfo(dtype='F16', shape=[2560, 10240], data_offsets=(786636800, 839065600), parameter_count=26214400), 'model.layers.11.self_attn.dense.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(839065600, 839070720), parameter_count=2560), 'model.layers.11.self_attn.dense.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(839070720, 852177920), parameter_count=6553600), 'model.layers.11.self_attn.k_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(852177920, 852183040), parameter_count=2560), 'model.layers.11.self_attn.k_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(852183040, 865290240), parameter_count=6553600), 'model.layers.11.self_attn.q_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(865290240, 865295360), parameter_count=2560), 'model.layers.11.self_attn.q_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(865295360, 878402560), parameter_count=6553600), 'model.layers.11.self_attn.v_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(878402560, 878407680), parameter_count=2560), 'model.layers.11.self_attn.v_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(878407680, 891514880), parameter_count=6553600), 'model.layers.12.input_layernorm.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(891514880, 891520000), parameter_count=2560), 'model.layers.12.input_layernorm.weight': TensorInfo(dtype='F16', shape=[2560], data_offsets=(891520000, 891525120), parameter_count=2560), 'model.layers.12.mlp.fc1.bias': TensorInfo(dtype='F16', shape=[10240], data_offsets=(891525120, 891545600), parameter_count=10240), 'model.layers.12.mlp.fc1.weight': TensorInfo(dtype='F16', shape=[10240, 2560], data_offsets=(891545600, 943974400), parameter_count=26214400), 'model.layers.12.mlp.fc2.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(943974400, 943979520), parameter_count=2560), 'model.layers.12.mlp.fc2.weight': TensorInfo(dtype='F16', shape=[2560, 10240], data_offsets=(943979520, 996408320), parameter_count=26214400), 'model.layers.12.self_attn.dense.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(996408320, 996413440), parameter_count=2560), 'model.layers.12.self_attn.dense.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(996413440, 1009520640), parameter_count=6553600), 'model.layers.12.self_attn.k_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(1009520640, 1009525760), parameter_count=2560), 'model.layers.12.self_attn.k_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(1009525760, 1022632960), parameter_count=6553600), 'model.layers.12.self_attn.q_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(1022632960, 1022638080), parameter_count=2560), 'model.layers.12.self_attn.q_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(1022638080, 1035745280), parameter_count=6553600), 'model.layers.12.self_attn.v_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(1035745280, 1035750400), parameter_count=2560), 'model.layers.12.self_attn.v_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(1035750400, 1048857600), parameter_count=6553600), 'model.layers.13.input_layernorm.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(1048857600, 1048862720), parameter_count=2560), 'model.layers.13.input_layernorm.weight': TensorInfo(dtype='F16', shape=[2560], data_offsets=(1048862720, 1048867840), parameter_count=2560), 'model.layers.13.mlp.fc1.bias': TensorInfo(dtype='F16', shape=[10240], data_offsets=(1048867840, 1048888320), parameter_count=10240), 'model.layers.13.mlp.fc1.weight': TensorInfo(dtype='F16', shape=[10240, 2560], data_offsets=(1048888320, 1101317120), parameter_count=26214400), 'model.layers.13.mlp.fc2.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(1101317120, 1101322240), parameter_count=2560), 'model.layers.13.mlp.fc2.weight': TensorInfo(dtype='F16', shape=[2560, 10240], data_offsets=(1101322240, 1153751040), parameter_count=26214400), 'model.layers.13.self_attn.dense.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(1153751040, 1153756160), parameter_count=2560), 'model.layers.13.self_attn.dense.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(1153756160, 1166863360), parameter_count=6553600), 'model.layers.13.self_attn.k_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(1166863360, 1166868480), parameter_count=2560), 'model.layers.13.self_attn.k_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(1166868480, 1179975680), parameter_count=6553600), 'model.layers.13.self_attn.q_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(1179975680, 1179980800), parameter_count=2560), 'model.layers.13.self_attn.q_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(1179980800, 1193088000), parameter_count=6553600), 'model.layers.13.self_attn.v_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(1193088000, 1193093120), parameter_count=2560), 'model.layers.13.self_attn.v_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(1193093120, 1206200320), parameter_count=6553600), 'model.layers.14.input_layernorm.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(1206200320, 1206205440), parameter_count=2560), 'model.layers.14.input_layernorm.weight': TensorInfo(dtype='F16', shape=[2560], data_offsets=(1206205440, 1206210560), parameter_count=2560), 'model.layers.14.mlp.fc1.bias': TensorInfo(dtype='F16', shape=[10240], data_offsets=(1206210560, 1206231040), parameter_count=10240), 'model.layers.14.mlp.fc1.weight': TensorInfo(dtype='F16', shape=[10240, 2560], data_offsets=(1206231040, 1258659840), parameter_count=26214400), 'model.layers.14.mlp.fc2.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(1258659840, 1258664960), parameter_count=2560), 'model.layers.14.mlp.fc2.weight': TensorInfo(dtype='F16', shape=[2560, 10240], data_offsets=(1258664960, 1311093760), parameter_count=26214400), 'model.layers.14.self_attn.dense.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(1311093760, 1311098880), parameter_count=2560), 'model.layers.14.self_attn.dense.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(1311098880, 1324206080), parameter_count=6553600), 'model.layers.14.self_attn.k_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(1324206080, 1324211200), parameter_count=2560), 'model.layers.14.self_attn.k_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(1324211200, 1337318400), parameter_count=6553600), 'model.layers.14.self_attn.q_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(1337318400, 1337323520), parameter_count=2560), 'model.layers.14.self_attn.q_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(1337323520, 1350430720), parameter_count=6553600), 'model.layers.14.self_attn.v_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(1350430720, 1350435840), parameter_count=2560), 'model.layers.14.self_attn.v_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(1350435840, 1363543040), parameter_count=6553600), 'model.layers.15.input_layernorm.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(1363543040, 1363548160), parameter_count=2560), 'model.layers.15.input_layernorm.weight': TensorInfo(dtype='F16', shape=[2560], data_offsets=(1363548160, 1363553280), parameter_count=2560), 'model.layers.15.mlp.fc1.bias': TensorInfo(dtype='F16', shape=[10240], data_offsets=(1363553280, 1363573760), parameter_count=10240), 'model.layers.15.mlp.fc1.weight': TensorInfo(dtype='F16', shape=[10240, 2560], data_offsets=(1363573760, 1416002560), parameter_count=26214400), 'model.layers.15.mlp.fc2.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(1416002560, 1416007680), parameter_count=2560), 'model.layers.15.mlp.fc2.weight': TensorInfo(dtype='F16', shape=[2560, 10240], data_offsets=(1416007680, 1468436480), parameter_count=26214400), 'model.layers.15.self_attn.dense.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(1468436480, 1468441600), parameter_count=2560), 'model.layers.15.self_attn.dense.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(1468441600, 1481548800), parameter_count=6553600), 'model.layers.15.self_attn.k_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(1481548800, 1481553920), parameter_count=2560), 'model.layers.15.self_attn.k_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(1481553920, 1494661120), parameter_count=6553600), 'model.layers.15.self_attn.q_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(1494661120, 1494666240), parameter_count=2560), 'model.layers.15.self_attn.q_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(1494666240, 1507773440), parameter_count=6553600), 'model.layers.15.self_attn.v_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(1507773440, 1507778560), parameter_count=2560), 'model.layers.15.self_attn.v_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(1507778560, 1520885760), parameter_count=6553600), 'model.layers.16.input_layernorm.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(1520885760, 1520890880), parameter_count=2560), 'model.layers.16.input_layernorm.weight': TensorInfo(dtype='F16', shape=[2560], data_offsets=(1520890880, 1520896000), parameter_count=2560), 'model.layers.16.mlp.fc1.bias': TensorInfo(dtype='F16', shape=[10240], data_offsets=(1520896000, 1520916480), parameter_count=10240), 'model.layers.16.mlp.fc1.weight': TensorInfo(dtype='F16', shape=[10240, 2560], data_offsets=(1520916480, 1573345280), parameter_count=26214400), 'model.layers.16.mlp.fc2.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(1573345280, 1573350400), parameter_count=2560), 'model.layers.16.mlp.fc2.weight': TensorInfo(dtype='F16', shape=[2560, 10240], data_offsets=(1573350400, 1625779200), parameter_count=26214400), 'model.layers.16.self_attn.dense.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(1625779200, 1625784320), parameter_count=2560), 'model.layers.16.self_attn.dense.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(1625784320, 1638891520), parameter_count=6553600), 'model.layers.16.self_attn.k_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(1638891520, 1638896640), parameter_count=2560), 'model.layers.16.self_attn.k_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(1638896640, 1652003840), parameter_count=6553600), 'model.layers.16.self_attn.q_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(1652003840, 1652008960), parameter_count=2560), 'model.layers.16.self_attn.q_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(1652008960, 1665116160), parameter_count=6553600), 'model.layers.16.self_attn.v_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(1665116160, 1665121280), parameter_count=2560), 'model.layers.16.self_attn.v_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(1665121280, 1678228480), parameter_count=6553600), 'model.layers.17.input_layernorm.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(1678228480, 1678233600), parameter_count=2560), 'model.layers.17.input_layernorm.weight': TensorInfo(dtype='F16', shape=[2560], data_offsets=(1678233600, 1678238720), parameter_count=2560), 'model.layers.17.mlp.fc1.bias': TensorInfo(dtype='F16', shape=[10240], data_offsets=(1678238720, 1678259200), parameter_count=10240), 'model.layers.17.mlp.fc1.weight': TensorInfo(dtype='F16', shape=[10240, 2560], data_offsets=(1678259200, 1730688000), parameter_count=26214400), 'model.layers.17.mlp.fc2.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(1730688000, 1730693120), parameter_count=2560), 'model.layers.17.mlp.fc2.weight': TensorInfo(dtype='F16', shape=[2560, 10240], data_offsets=(1730693120, 1783121920), parameter_count=26214400), 'model.layers.17.self_attn.dense.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(1783121920, 1783127040), parameter_count=2560), 'model.layers.17.self_attn.dense.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(1783127040, 1796234240), parameter_count=6553600), 'model.layers.17.self_attn.k_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(1796234240, 1796239360), parameter_count=2560), 'model.layers.17.self_attn.k_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(1796239360, 1809346560), parameter_count=6553600), 'model.layers.17.self_attn.q_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(1809346560, 1809351680), parameter_count=2560), 'model.layers.17.self_attn.q_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(1809351680, 1822458880), parameter_count=6553600), 'model.layers.17.self_attn.v_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(1822458880, 1822464000), parameter_count=2560), 'model.layers.17.self_attn.v_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(1822464000, 1835571200), parameter_count=6553600), 'model.layers.18.input_layernorm.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(1835571200, 1835576320), parameter_count=2560), 'model.layers.18.input_layernorm.weight': TensorInfo(dtype='F16', shape=[2560], data_offsets=(1835576320, 1835581440), parameter_count=2560), 'model.layers.18.mlp.fc1.bias': TensorInfo(dtype='F16', shape=[10240], data_offsets=(1835581440, 1835601920), parameter_count=10240), 'model.layers.18.mlp.fc1.weight': TensorInfo(dtype='F16', shape=[10240, 2560], data_offsets=(1835601920, 1888030720), parameter_count=26214400), 'model.layers.18.mlp.fc2.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(1888030720, 1888035840), parameter_count=2560), 'model.layers.18.mlp.fc2.weight': TensorInfo(dtype='F16', shape=[2560, 10240], data_offsets=(1888035840, 1940464640), parameter_count=26214400), 'model.layers.18.self_attn.dense.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(1940464640, 1940469760), parameter_count=2560), 'model.layers.18.self_attn.dense.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(1940469760, 1953576960), parameter_count=6553600), 'model.layers.18.self_attn.k_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(1953576960, 1953582080), parameter_count=2560), 'model.layers.18.self_attn.k_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(1953582080, 1966689280), parameter_count=6553600), 'model.layers.18.self_attn.q_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(1966689280, 1966694400), parameter_count=2560), 'model.layers.18.self_attn.q_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(1966694400, 1979801600), parameter_count=6553600), 'model.layers.18.self_attn.v_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(1979801600, 1979806720), parameter_count=2560), 'model.layers.18.self_attn.v_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(1979806720, 1992913920), parameter_count=6553600), 'model.layers.19.input_layernorm.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(1992913920, 1992919040), parameter_count=2560), 'model.layers.19.input_layernorm.weight': TensorInfo(dtype='F16', shape=[2560], data_offsets=(1992919040, 1992924160), parameter_count=2560), 'model.layers.19.mlp.fc1.bias': TensorInfo(dtype='F16', shape=[10240], data_offsets=(1992924160, 1992944640), parameter_count=10240), 'model.layers.19.mlp.fc1.weight': TensorInfo(dtype='F16', shape=[10240, 2560], data_offsets=(1992944640, 2045373440), parameter_count=26214400), 'model.layers.19.mlp.fc2.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(2045373440, 2045378560), parameter_count=2560), 'model.layers.19.mlp.fc2.weight': TensorInfo(dtype='F16', shape=[2560, 10240], data_offsets=(2045378560, 2097807360), parameter_count=26214400), 'model.layers.19.self_attn.dense.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(2097807360, 2097812480), parameter_count=2560), 'model.layers.19.self_attn.dense.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(2097812480, 2110919680), parameter_count=6553600), 'model.layers.19.self_attn.k_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(2110919680, 2110924800), parameter_count=2560), 'model.layers.19.self_attn.k_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(2110924800, 2124032000), parameter_count=6553600), 'model.layers.19.self_attn.q_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(2124032000, 2124037120), parameter_count=2560), 'model.layers.19.self_attn.q_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(2124037120, 2137144320), parameter_count=6553600), 'model.layers.19.self_attn.v_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(2137144320, 2137149440), parameter_count=2560), 'model.layers.19.self_attn.v_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(2137149440, 2150256640), parameter_count=6553600), 'model.layers.2.input_layernorm.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(2150256640, 2150261760), parameter_count=2560), 'model.layers.2.input_layernorm.weight': TensorInfo(dtype='F16', shape=[2560], data_offsets=(2150261760, 2150266880), parameter_count=2560), 'model.layers.2.mlp.fc1.bias': TensorInfo(dtype='F16', shape=[10240], data_offsets=(2150266880, 2150287360), parameter_count=10240), 'model.layers.2.mlp.fc1.weight': TensorInfo(dtype='F16', shape=[10240, 2560], data_offsets=(2150287360, 2202716160), parameter_count=26214400), 'model.layers.2.mlp.fc2.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(2202716160, 2202721280), parameter_count=2560), 'model.layers.2.mlp.fc2.weight': TensorInfo(dtype='F16', shape=[2560, 10240], data_offsets=(2202721280, 2255150080), parameter_count=26214400), 'model.layers.2.self_attn.dense.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(2255150080, 2255155200), parameter_count=2560), 'model.layers.2.self_attn.dense.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(2255155200, 2268262400), parameter_count=6553600), 'model.layers.2.self_attn.k_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(2268262400, 2268267520), parameter_count=2560), 'model.layers.2.self_attn.k_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(2268267520, 2281374720), parameter_count=6553600), 'model.layers.2.self_attn.q_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(2281374720, 2281379840), parameter_count=2560), 'model.layers.2.self_attn.q_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(2281379840, 2294487040), parameter_count=6553600), 'model.layers.2.self_attn.v_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(2294487040, 2294492160), parameter_count=2560), 'model.layers.2.self_attn.v_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(2294492160, 2307599360), parameter_count=6553600), 'model.layers.20.input_layernorm.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(2307599360, 2307604480), parameter_count=2560), 'model.layers.20.input_layernorm.weight': TensorInfo(dtype='F16', shape=[2560], data_offsets=(2307604480, 2307609600), parameter_count=2560), 'model.layers.20.mlp.fc1.bias': TensorInfo(dtype='F16', shape=[10240], data_offsets=(2307609600, 2307630080), parameter_count=10240), 'model.layers.20.mlp.fc1.weight': TensorInfo(dtype='F16', shape=[10240, 2560], data_offsets=(2307630080, 2360058880), parameter_count=26214400), 'model.layers.20.mlp.fc2.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(2360058880, 2360064000), parameter_count=2560), 'model.layers.20.mlp.fc2.weight': TensorInfo(dtype='F16', shape=[2560, 10240], data_offsets=(2360064000, 2412492800), parameter_count=26214400), 'model.layers.20.self_attn.dense.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(2412492800, 2412497920), parameter_count=2560), 'model.layers.20.self_attn.dense.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(2412497920, 2425605120), parameter_count=6553600), 'model.layers.20.self_attn.k_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(2425605120, 2425610240), parameter_count=2560), 'model.layers.20.self_attn.k_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(2425610240, 2438717440), parameter_count=6553600), 'model.layers.20.self_attn.q_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(2438717440, 2438722560), parameter_count=2560), 'model.layers.20.self_attn.q_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(2438722560, 2451829760), parameter_count=6553600), 'model.layers.20.self_attn.v_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(2451829760, 2451834880), parameter_count=2560), 'model.layers.20.self_attn.v_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(2451834880, 2464942080), parameter_count=6553600), 'model.layers.21.input_layernorm.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(2464942080, 2464947200), parameter_count=2560), 'model.layers.21.input_layernorm.weight': TensorInfo(dtype='F16', shape=[2560], data_offsets=(2464947200, 2464952320), parameter_count=2560), 'model.layers.21.mlp.fc1.bias': TensorInfo(dtype='F16', shape=[10240], data_offsets=(2464952320, 2464972800), parameter_count=10240), 'model.layers.21.mlp.fc1.weight': TensorInfo(dtype='F16', shape=[10240, 2560], data_offsets=(2464972800, 2517401600), parameter_count=26214400), 'model.layers.21.mlp.fc2.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(2517401600, 2517406720), parameter_count=2560), 'model.layers.21.mlp.fc2.weight': TensorInfo(dtype='F16', shape=[2560, 10240], data_offsets=(2517406720, 2569835520), parameter_count=26214400), 'model.layers.21.self_attn.dense.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(2569835520, 2569840640), parameter_count=2560), 'model.layers.21.self_attn.dense.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(2569840640, 2582947840), parameter_count=6553600), 'model.layers.21.self_attn.k_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(2582947840, 2582952960), parameter_count=2560), 'model.layers.21.self_attn.k_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(2582952960, 2596060160), parameter_count=6553600), 'model.layers.21.self_attn.q_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(2596060160, 2596065280), parameter_count=2560), 'model.layers.21.self_attn.q_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(2596065280, 2609172480), parameter_count=6553600), 'model.layers.21.self_attn.v_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(2609172480, 2609177600), parameter_count=2560), 'model.layers.21.self_attn.v_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(2609177600, 2622284800), parameter_count=6553600), 'model.layers.22.input_layernorm.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(2622284800, 2622289920), parameter_count=2560), 'model.layers.22.input_layernorm.weight': TensorInfo(dtype='F16', shape=[2560], data_offsets=(2622289920, 2622295040), parameter_count=2560), 'model.layers.22.mlp.fc1.bias': TensorInfo(dtype='F16', shape=[10240], data_offsets=(2622295040, 2622315520), parameter_count=10240), 'model.layers.22.mlp.fc1.weight': TensorInfo(dtype='F16', shape=[10240, 2560], data_offsets=(2622315520, 2674744320), parameter_count=26214400), 'model.layers.22.mlp.fc2.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(2674744320, 2674749440), parameter_count=2560), 'model.layers.22.mlp.fc2.weight': TensorInfo(dtype='F16', shape=[2560, 10240], data_offsets=(2674749440, 2727178240), parameter_count=26214400), 'model.layers.22.self_attn.dense.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(2727178240, 2727183360), parameter_count=2560), 'model.layers.22.self_attn.dense.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(2727183360, 2740290560), parameter_count=6553600), 'model.layers.22.self_attn.k_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(2740290560, 2740295680), parameter_count=2560), 'model.layers.22.self_attn.k_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(2740295680, 2753402880), parameter_count=6553600), 'model.layers.22.self_attn.q_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(2753402880, 2753408000), parameter_count=2560), 'model.layers.22.self_attn.q_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(2753408000, 2766515200), parameter_count=6553600), 'model.layers.22.self_attn.v_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(2766515200, 2766520320), parameter_count=2560), 'model.layers.22.self_attn.v_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(2766520320, 2779627520), parameter_count=6553600), 'model.layers.23.input_layernorm.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(2779627520, 2779632640), parameter_count=2560), 'model.layers.23.input_layernorm.weight': TensorInfo(dtype='F16', shape=[2560], data_offsets=(2779632640, 2779637760), parameter_count=2560), 'model.layers.23.mlp.fc1.bias': TensorInfo(dtype='F16', shape=[10240], data_offsets=(2779637760, 2779658240), parameter_count=10240), 'model.layers.23.mlp.fc1.weight': TensorInfo(dtype='F16', shape=[10240, 2560], data_offsets=(2779658240, 2832087040), parameter_count=26214400), 'model.layers.23.mlp.fc2.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(2832087040, 2832092160), parameter_count=2560), 'model.layers.23.mlp.fc2.weight': TensorInfo(dtype='F16', shape=[2560, 10240], data_offsets=(2832092160, 2884520960), parameter_count=26214400), 'model.layers.23.self_attn.dense.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(2884520960, 2884526080), parameter_count=2560), 'model.layers.23.self_attn.dense.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(2884526080, 2897633280), parameter_count=6553600), 'model.layers.23.self_attn.k_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(2897633280, 2897638400), parameter_count=2560), 'model.layers.23.self_attn.k_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(2897638400, 2910745600), parameter_count=6553600), 'model.layers.23.self_attn.q_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(2910745600, 2910750720), parameter_count=2560), 'model.layers.23.self_attn.q_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(2910750720, 2923857920), parameter_count=6553600), 'model.layers.23.self_attn.v_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(2923857920, 2923863040), parameter_count=2560), 'model.layers.23.self_attn.v_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(2923863040, 2936970240), parameter_count=6553600), 'model.layers.24.input_layernorm.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(2936970240, 2936975360), parameter_count=2560), 'model.layers.24.input_layernorm.weight': TensorInfo(dtype='F16', shape=[2560], data_offsets=(2936975360, 2936980480), parameter_count=2560), 'model.layers.24.mlp.fc1.bias': TensorInfo(dtype='F16', shape=[10240], data_offsets=(2936980480, 2937000960), parameter_count=10240), 'model.layers.24.mlp.fc1.weight': TensorInfo(dtype='F16', shape=[10240, 2560], data_offsets=(2937000960, 2989429760), parameter_count=26214400), 'model.layers.24.mlp.fc2.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(2989429760, 2989434880), parameter_count=2560), 'model.layers.24.mlp.fc2.weight': TensorInfo(dtype='F16', shape=[2560, 10240], data_offsets=(2989434880, 3041863680), parameter_count=26214400), 'model.layers.24.self_attn.dense.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(3041863680, 3041868800), parameter_count=2560), 'model.layers.24.self_attn.dense.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(3041868800, 3054976000), parameter_count=6553600), 'model.layers.24.self_attn.k_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(3054976000, 3054981120), parameter_count=2560), 'model.layers.24.self_attn.k_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(3054981120, 3068088320), parameter_count=6553600), 'model.layers.24.self_attn.q_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(3068088320, 3068093440), parameter_count=2560), 'model.layers.24.self_attn.q_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(3068093440, 3081200640), parameter_count=6553600), 'model.layers.24.self_attn.v_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(3081200640, 3081205760), parameter_count=2560), 'model.layers.24.self_attn.v_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(3081205760, 3094312960), parameter_count=6553600), 'model.layers.25.input_layernorm.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(3094312960, 3094318080), parameter_count=2560), 'model.layers.25.input_layernorm.weight': TensorInfo(dtype='F16', shape=[2560], data_offsets=(3094318080, 3094323200), parameter_count=2560), 'model.layers.25.mlp.fc1.bias': TensorInfo(dtype='F16', shape=[10240], data_offsets=(3094323200, 3094343680), parameter_count=10240), 'model.layers.25.mlp.fc1.weight': TensorInfo(dtype='F16', shape=[10240, 2560], data_offsets=(3094343680, 3146772480), parameter_count=26214400), 'model.layers.25.mlp.fc2.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(3146772480, 3146777600), parameter_count=2560), 'model.layers.25.mlp.fc2.weight': TensorInfo(dtype='F16', shape=[2560, 10240], data_offsets=(3146777600, 3199206400), parameter_count=26214400), 'model.layers.25.self_attn.dense.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(3199206400, 3199211520), parameter_count=2560), 'model.layers.25.self_attn.dense.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(3199211520, 3212318720), parameter_count=6553600), 'model.layers.25.self_attn.k_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(3212318720, 3212323840), parameter_count=2560), 'model.layers.25.self_attn.k_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(3212323840, 3225431040), parameter_count=6553600), 'model.layers.25.self_attn.q_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(3225431040, 3225436160), parameter_count=2560), 'model.layers.25.self_attn.q_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(3225436160, 3238543360), parameter_count=6553600), 'model.layers.25.self_attn.v_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(3238543360, 3238548480), parameter_count=2560), 'model.layers.25.self_attn.v_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(3238548480, 3251655680), parameter_count=6553600), 'model.layers.26.input_layernorm.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(3251655680, 3251660800), parameter_count=2560), 'model.layers.26.input_layernorm.weight': TensorInfo(dtype='F16', shape=[2560], data_offsets=(3251660800, 3251665920), parameter_count=2560), 'model.layers.26.mlp.fc1.bias': TensorInfo(dtype='F16', shape=[10240], data_offsets=(3251665920, 3251686400), parameter_count=10240), 'model.layers.26.mlp.fc1.weight': TensorInfo(dtype='F16', shape=[10240, 2560], data_offsets=(3251686400, 3304115200), parameter_count=26214400), 'model.layers.26.mlp.fc2.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(3304115200, 3304120320), parameter_count=2560), 'model.layers.26.mlp.fc2.weight': TensorInfo(dtype='F16', shape=[2560, 10240], data_offsets=(3304120320, 3356549120), parameter_count=26214400), 'model.layers.26.self_attn.dense.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(3356549120, 3356554240), parameter_count=2560), 'model.layers.26.self_attn.dense.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(3356554240, 3369661440), parameter_count=6553600), 'model.layers.26.self_attn.k_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(3369661440, 3369666560), parameter_count=2560), 'model.layers.26.self_attn.k_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(3369666560, 3382773760), parameter_count=6553600), 'model.layers.26.self_attn.q_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(3382773760, 3382778880), parameter_count=2560), 'model.layers.26.self_attn.q_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(3382778880, 3395886080), parameter_count=6553600), 'model.layers.26.self_attn.v_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(3395886080, 3395891200), parameter_count=2560), 'model.layers.26.self_attn.v_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(3395891200, 3408998400), parameter_count=6553600), 'model.layers.27.input_layernorm.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(3408998400, 3409003520), parameter_count=2560), 'model.layers.27.input_layernorm.weight': TensorInfo(dtype='F16', shape=[2560], data_offsets=(3409003520, 3409008640), parameter_count=2560), 'model.layers.27.mlp.fc1.bias': TensorInfo(dtype='F16', shape=[10240], data_offsets=(3409008640, 3409029120), parameter_count=10240), 'model.layers.27.mlp.fc1.weight': TensorInfo(dtype='F16', shape=[10240, 2560], data_offsets=(3409029120, 3461457920), parameter_count=26214400), 'model.layers.27.mlp.fc2.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(3461457920, 3461463040), parameter_count=2560), 'model.layers.27.mlp.fc2.weight': TensorInfo(dtype='F16', shape=[2560, 10240], data_offsets=(3461463040, 3513891840), parameter_count=26214400), 'model.layers.27.self_attn.dense.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(3513891840, 3513896960), parameter_count=2560), 'model.layers.27.self_attn.dense.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(3513896960, 3527004160), parameter_count=6553600), 'model.layers.27.self_attn.k_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(3527004160, 3527009280), parameter_count=2560), 'model.layers.27.self_attn.k_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(3527009280, 3540116480), parameter_count=6553600), 'model.layers.27.self_attn.q_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(3540116480, 3540121600), parameter_count=2560), 'model.layers.27.self_attn.q_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(3540121600, 3553228800), parameter_count=6553600), 'model.layers.27.self_attn.v_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(3553228800, 3553233920), parameter_count=2560), 'model.layers.27.self_attn.v_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(3553233920, 3566341120), parameter_count=6553600), 'model.layers.28.input_layernorm.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(3566341120, 3566346240), parameter_count=2560), 'model.layers.28.input_layernorm.weight': TensorInfo(dtype='F16', shape=[2560], data_offsets=(3566346240, 3566351360), parameter_count=2560), 'model.layers.28.mlp.fc1.bias': TensorInfo(dtype='F16', shape=[10240], data_offsets=(3566351360, 3566371840), parameter_count=10240), 'model.layers.28.mlp.fc1.weight': TensorInfo(dtype='F16', shape=[10240, 2560], data_offsets=(3566371840, 3618800640), parameter_count=26214400), 'model.layers.28.mlp.fc2.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(3618800640, 3618805760), parameter_count=2560), 'model.layers.28.mlp.fc2.weight': TensorInfo(dtype='F16', shape=[2560, 10240], data_offsets=(3618805760, 3671234560), parameter_count=26214400), 'model.layers.28.self_attn.dense.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(3671234560, 3671239680), parameter_count=2560), 'model.layers.28.self_attn.dense.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(3671239680, 3684346880), parameter_count=6553600), 'model.layers.28.self_attn.k_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(3684346880, 3684352000), parameter_count=2560), 'model.layers.28.self_attn.k_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(3684352000, 3697459200), parameter_count=6553600), 'model.layers.28.self_attn.q_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(3697459200, 3697464320), parameter_count=2560), 'model.layers.28.self_attn.q_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(3697464320, 3710571520), parameter_count=6553600), 'model.layers.28.self_attn.v_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(3710571520, 3710576640), parameter_count=2560), 'model.layers.28.self_attn.v_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(3710576640, 3723683840), parameter_count=6553600), 'model.layers.29.input_layernorm.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(3723683840, 3723688960), parameter_count=2560), 'model.layers.29.input_layernorm.weight': TensorInfo(dtype='F16', shape=[2560], data_offsets=(3723688960, 3723694080), parameter_count=2560), 'model.layers.29.mlp.fc1.bias': TensorInfo(dtype='F16', shape=[10240], data_offsets=(3723694080, 3723714560), parameter_count=10240), 'model.layers.29.mlp.fc1.weight': TensorInfo(dtype='F16', shape=[10240, 2560], data_offsets=(3723714560, 3776143360), parameter_count=26214400), 'model.layers.29.mlp.fc2.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(3776143360, 3776148480), parameter_count=2560), 'model.layers.29.mlp.fc2.weight': TensorInfo(dtype='F16', shape=[2560, 10240], data_offsets=(3776148480, 3828577280), parameter_count=26214400), 'model.layers.29.self_attn.dense.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(3828577280, 3828582400), parameter_count=2560), 'model.layers.29.self_attn.dense.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(3828582400, 3841689600), parameter_count=6553600), 'model.layers.29.self_attn.k_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(3841689600, 3841694720), parameter_count=2560), 'model.layers.29.self_attn.k_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(3841694720, 3854801920), parameter_count=6553600), 'model.layers.29.self_attn.q_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(3854801920, 3854807040), parameter_count=2560), 'model.layers.29.self_attn.q_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(3854807040, 3867914240), parameter_count=6553600), 'model.layers.29.self_attn.v_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(3867914240, 3867919360), parameter_count=2560), 'model.layers.29.self_attn.v_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(3867919360, 3881026560), parameter_count=6553600), 'model.layers.3.input_layernorm.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(3881026560, 3881031680), parameter_count=2560), 'model.layers.3.input_layernorm.weight': TensorInfo(dtype='F16', shape=[2560], data_offsets=(3881031680, 3881036800), parameter_count=2560), 'model.layers.3.mlp.fc1.bias': TensorInfo(dtype='F16', shape=[10240], data_offsets=(3881036800, 3881057280), parameter_count=10240), 'model.layers.3.mlp.fc1.weight': TensorInfo(dtype='F16', shape=[10240, 2560], data_offsets=(3881057280, 3933486080), parameter_count=26214400), 'model.layers.3.mlp.fc2.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(3933486080, 3933491200), parameter_count=2560), 'model.layers.3.mlp.fc2.weight': TensorInfo(dtype='F16', shape=[2560, 10240], data_offsets=(3933491200, 3985920000), parameter_count=26214400), 'model.layers.3.self_attn.dense.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(3985920000, 3985925120), parameter_count=2560), 'model.layers.3.self_attn.dense.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(3985925120, 3999032320), parameter_count=6553600), 'model.layers.3.self_attn.k_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(3999032320, 3999037440), parameter_count=2560), 'model.layers.3.self_attn.k_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(3999037440, 4012144640), parameter_count=6553600), 'model.layers.3.self_attn.q_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(4012144640, 4012149760), parameter_count=2560), 'model.layers.3.self_attn.q_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(4012149760, 4025256960), parameter_count=6553600), 'model.layers.3.self_attn.v_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(4025256960, 4025262080), parameter_count=2560), 'model.layers.3.self_attn.v_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(4025262080, 4038369280), parameter_count=6553600), 'model.layers.30.self_attn.q_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(4038369280, 4038374400), parameter_count=2560), 'model.layers.30.self_attn.q_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(4038374400, 4051481600), parameter_count=6553600), 'model.layers.4.input_layernorm.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(4051481600, 4051486720), parameter_count=2560), 'model.layers.4.input_layernorm.weight': TensorInfo(dtype='F16', shape=[2560], data_offsets=(4051486720, 4051491840), parameter_count=2560), 'model.layers.4.mlp.fc1.bias': TensorInfo(dtype='F16', shape=[10240], data_offsets=(4051491840, 4051512320), parameter_count=10240), 'model.layers.4.mlp.fc1.weight': TensorInfo(dtype='F16', shape=[10240, 2560], data_offsets=(4051512320, 4103941120), parameter_count=26214400), 'model.layers.4.mlp.fc2.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(4103941120, 4103946240), parameter_count=2560), 'model.layers.4.mlp.fc2.weight': TensorInfo(dtype='F16', shape=[2560, 10240], data_offsets=(4103946240, 4156375040), parameter_count=26214400), 'model.layers.4.self_attn.dense.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(4156375040, 4156380160), parameter_count=2560), 'model.layers.4.self_attn.dense.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(4156380160, 4169487360), parameter_count=6553600), 'model.layers.4.self_attn.k_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(4169487360, 4169492480), parameter_count=2560), 'model.layers.4.self_attn.k_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(4169492480, 4182599680), parameter_count=6553600), 'model.layers.4.self_attn.q_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(4182599680, 4182604800), parameter_count=2560), 'model.layers.4.self_attn.q_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(4182604800, 4195712000), parameter_count=6553600), 'model.layers.4.self_attn.v_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(4195712000, 4195717120), parameter_count=2560), 'model.layers.4.self_attn.v_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(4195717120, 4208824320), parameter_count=6553600), 'model.layers.5.input_layernorm.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(4208824320, 4208829440), parameter_count=2560), 'model.layers.5.input_layernorm.weight': TensorInfo(dtype='F16', shape=[2560], data_offsets=(4208829440, 4208834560), parameter_count=2560), 'model.layers.5.mlp.fc1.bias': TensorInfo(dtype='F16', shape=[10240], data_offsets=(4208834560, 4208855040), parameter_count=10240), 'model.layers.5.mlp.fc1.weight': TensorInfo(dtype='F16', shape=[10240, 2560], data_offsets=(4208855040, 4261283840), parameter_count=26214400), 'model.layers.5.mlp.fc2.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(4261283840, 4261288960), parameter_count=2560), 'model.layers.5.mlp.fc2.weight': TensorInfo(dtype='F16', shape=[2560, 10240], data_offsets=(4261288960, 4313717760), parameter_count=26214400), 'model.layers.5.self_attn.dense.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(4313717760, 4313722880), parameter_count=2560), 'model.layers.5.self_attn.dense.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(4313722880, 4326830080), parameter_count=6553600), 'model.layers.5.self_attn.k_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(4326830080, 4326835200), parameter_count=2560), 'model.layers.5.self_attn.k_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(4326835200, 4339942400), parameter_count=6553600), 'model.layers.5.self_attn.q_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(4339942400, 4339947520), parameter_count=2560), 'model.layers.5.self_attn.q_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(4339947520, 4353054720), parameter_count=6553600), 'model.layers.5.self_attn.v_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(4353054720, 4353059840), parameter_count=2560), 'model.layers.5.self_attn.v_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(4353059840, 4366167040), parameter_count=6553600), 'model.layers.6.input_layernorm.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(4366167040, 4366172160), parameter_count=2560), 'model.layers.6.input_layernorm.weight': TensorInfo(dtype='F16', shape=[2560], data_offsets=(4366172160, 4366177280), parameter_count=2560), 'model.layers.6.mlp.fc1.bias': TensorInfo(dtype='F16', shape=[10240], data_offsets=(4366177280, 4366197760), parameter_count=10240), 'model.layers.6.mlp.fc1.weight': TensorInfo(dtype='F16', shape=[10240, 2560], data_offsets=(4366197760, 4418626560), parameter_count=26214400), 'model.layers.6.mlp.fc2.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(4418626560, 4418631680), parameter_count=2560), 'model.layers.6.mlp.fc2.weight': TensorInfo(dtype='F16', shape=[2560, 10240], data_offsets=(4418631680, 4471060480), parameter_count=26214400), 'model.layers.6.self_attn.dense.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(4471060480, 4471065600), parameter_count=2560), 'model.layers.6.self_attn.dense.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(4471065600, 4484172800), parameter_count=6553600), 'model.layers.6.self_attn.k_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(4484172800, 4484177920), parameter_count=2560), 'model.layers.6.self_attn.k_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(4484177920, 4497285120), parameter_count=6553600), 'model.layers.6.self_attn.q_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(4497285120, 4497290240), parameter_count=2560), 'model.layers.6.self_attn.q_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(4497290240, 4510397440), parameter_count=6553600), 'model.layers.6.self_attn.v_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(4510397440, 4510402560), parameter_count=2560), 'model.layers.6.self_attn.v_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(4510402560, 4523509760), parameter_count=6553600), 'model.layers.7.input_layernorm.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(4523509760, 4523514880), parameter_count=2560), 'model.layers.7.input_layernorm.weight': TensorInfo(dtype='F16', shape=[2560], data_offsets=(4523514880, 4523520000), parameter_count=2560), 'model.layers.7.mlp.fc1.bias': TensorInfo(dtype='F16', shape=[10240], data_offsets=(4523520000, 4523540480), parameter_count=10240), 'model.layers.7.mlp.fc1.weight': TensorInfo(dtype='F16', shape=[10240, 2560], data_offsets=(4523540480, 4575969280), parameter_count=26214400), 'model.layers.7.mlp.fc2.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(4575969280, 4575974400), parameter_count=2560), 'model.layers.7.mlp.fc2.weight': TensorInfo(dtype='F16', shape=[2560, 10240], data_offsets=(4575974400, 4628403200), parameter_count=26214400), 'model.layers.7.self_attn.dense.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(4628403200, 4628408320), parameter_count=2560), 'model.layers.7.self_attn.dense.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(4628408320, 4641515520), parameter_count=6553600), 'model.layers.7.self_attn.k_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(4641515520, 4641520640), parameter_count=2560), 'model.layers.7.self_attn.k_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(4641520640, 4654627840), parameter_count=6553600), 'model.layers.7.self_attn.q_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(4654627840, 4654632960), parameter_count=2560), 'model.layers.7.self_attn.q_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(4654632960, 4667740160), parameter_count=6553600), 'model.layers.7.self_attn.v_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(4667740160, 4667745280), parameter_count=2560), 'model.layers.7.self_attn.v_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(4667745280, 4680852480), parameter_count=6553600), 'model.layers.8.input_layernorm.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(4680852480, 4680857600), parameter_count=2560), 'model.layers.8.input_layernorm.weight': TensorInfo(dtype='F16', shape=[2560], data_offsets=(4680857600, 4680862720), parameter_count=2560), 'model.layers.8.mlp.fc1.bias': TensorInfo(dtype='F16', shape=[10240], data_offsets=(4680862720, 4680883200), parameter_count=10240), 'model.layers.8.mlp.fc1.weight': TensorInfo(dtype='F16', shape=[10240, 2560], data_offsets=(4680883200, 4733312000), parameter_count=26214400), 'model.layers.8.mlp.fc2.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(4733312000, 4733317120), parameter_count=2560), 'model.layers.8.mlp.fc2.weight': TensorInfo(dtype='F16', shape=[2560, 10240], data_offsets=(4733317120, 4785745920), parameter_count=26214400), 'model.layers.8.self_attn.dense.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(4785745920, 4785751040), parameter_count=2560), 'model.layers.8.self_attn.dense.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(4785751040, 4798858240), parameter_count=6553600), 'model.layers.8.self_attn.k_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(4798858240, 4798863360), parameter_count=2560), 'model.layers.8.self_attn.k_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(4798863360, 4811970560), parameter_count=6553600), 'model.layers.8.self_attn.q_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(4811970560, 4811975680), parameter_count=2560), 'model.layers.8.self_attn.q_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(4811975680, 4825082880), parameter_count=6553600), 'model.layers.8.self_attn.v_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(4825082880, 4825088000), parameter_count=2560), 'model.layers.8.self_attn.v_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(4825088000, 4838195200), parameter_count=6553600), 'model.layers.9.input_layernorm.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(4838195200, 4838200320), parameter_count=2560), 'model.layers.9.input_layernorm.weight': TensorInfo(dtype='F16', shape=[2560], data_offsets=(4838200320, 4838205440), parameter_count=2560), 'model.layers.9.mlp.fc1.bias': TensorInfo(dtype='F16', shape=[10240], data_offsets=(4838205440, 4838225920), parameter_count=10240), 'model.layers.9.mlp.fc1.weight': TensorInfo(dtype='F16', shape=[10240, 2560], data_offsets=(4838225920, 4890654720), parameter_count=26214400), 'model.layers.9.mlp.fc2.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(4890654720, 4890659840), parameter_count=2560), 'model.layers.9.mlp.fc2.weight': TensorInfo(dtype='F16', shape=[2560, 10240], data_offsets=(4890659840, 4943088640), parameter_count=26214400), 'model.layers.9.self_attn.dense.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(4943088640, 4943093760), parameter_count=2560), 'model.layers.9.self_attn.dense.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(4943093760, 4956200960), parameter_count=6553600), 'model.layers.9.self_attn.k_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(4956200960, 4956206080), parameter_count=2560), 'model.layers.9.self_attn.k_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(4956206080, 4969313280), parameter_count=6553600), 'model.layers.9.self_attn.q_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(4969313280, 4969318400), parameter_count=2560), 'model.layers.9.self_attn.q_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(4969318400, 4982425600), parameter_count=6553600), 'model.layers.9.self_attn.v_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(4982425600, 4982430720), parameter_count=2560), 'model.layers.9.self_attn.v_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(4982430720, 4995537920), parameter_count=6553600)}, parameter_count={'F16': 2497768960}), 'model-00002-of-00002.safetensors': SafetensorsFileMetadata(metadata={'format': 'pt'}, tensors={'lm_head.bias': TensorInfo(dtype='F16', shape=[51200], data_offsets=(0, 102400), parameter_count=51200), 'lm_head.weight': TensorInfo(dtype='F16', shape=[51200, 2560], data_offsets=(102400, 262246400), parameter_count=131072000), 'model.final_layernorm.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(262246400, 262251520), parameter_count=2560), 'model.final_layernorm.weight': TensorInfo(dtype='F16', shape=[2560], data_offsets=(262251520, 262256640), parameter_count=2560), 'model.layers.30.input_layernorm.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(262256640, 262261760), parameter_count=2560), 'model.layers.30.input_layernorm.weight': TensorInfo(dtype='F16', shape=[2560], data_offsets=(262261760, 262266880), parameter_count=2560), 'model.layers.30.mlp.fc1.bias': TensorInfo(dtype='F16', shape=[10240], data_offsets=(262266880, 262287360), parameter_count=10240), 'model.layers.30.mlp.fc1.weight': TensorInfo(dtype='F16', shape=[10240, 2560], data_offsets=(262287360, 314716160), parameter_count=26214400), 'model.layers.30.mlp.fc2.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(314716160, 314721280), parameter_count=2560), 'model.layers.30.mlp.fc2.weight': TensorInfo(dtype='F16', shape=[2560, 10240], data_offsets=(314721280, 367150080), parameter_count=26214400), 'model.layers.30.self_attn.dense.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(367150080, 367155200), parameter_count=2560), 'model.layers.30.self_attn.dense.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(367155200, 380262400), parameter_count=6553600), 'model.layers.30.self_attn.k_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(380262400, 380267520), parameter_count=2560), 'model.layers.30.self_attn.k_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(380267520, 393374720), parameter_count=6553600), 'model.layers.30.self_attn.v_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(393374720, 393379840), parameter_count=2560), 'model.layers.30.self_attn.v_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(393379840, 406487040), parameter_count=6553600), 'model.layers.31.input_layernorm.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(406487040, 406492160), parameter_count=2560), 'model.layers.31.input_layernorm.weight': TensorInfo(dtype='F16', shape=[2560], data_offsets=(406492160, 406497280), parameter_count=2560), 'model.layers.31.mlp.fc1.bias': TensorInfo(dtype='F16', shape=[10240], data_offsets=(406497280, 406517760), parameter_count=10240), 'model.layers.31.mlp.fc1.weight': TensorInfo(dtype='F16', shape=[10240, 2560], data_offsets=(406517760, 458946560), parameter_count=26214400), 'model.layers.31.mlp.fc2.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(458946560, 458951680), parameter_count=2560), 'model.layers.31.mlp.fc2.weight': TensorInfo(dtype='F16', shape=[2560, 10240], data_offsets=(458951680, 511380480), parameter_count=26214400), 'model.layers.31.self_attn.dense.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(511380480, 511385600), parameter_count=2560), 'model.layers.31.self_attn.dense.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(511385600, 524492800), parameter_count=6553600), 'model.layers.31.self_attn.k_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(524492800, 524497920), parameter_count=2560), 'model.layers.31.self_attn.k_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(524497920, 537605120), parameter_count=6553600), 'model.layers.31.self_attn.q_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(537605120, 537610240), parameter_count=2560), 'model.layers.31.self_attn.q_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(537610240, 550717440), parameter_count=6553600), 'model.layers.31.self_attn.v_proj.bias': TensorInfo(dtype='F16', shape=[2560], data_offsets=(550717440, 550722560), parameter_count=2560), 'model.layers.31.self_attn.v_proj.weight': TensorInfo(dtype='F16', shape=[2560, 2560], data_offsets=(550722560, 563829760), parameter_count=6553600)}, parameter_count={'F16': 281914880})}, parameter_count={'F16': 2779683840})"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prefill Overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def calculate_mlp_prefill_overhead(\n",
    "    seq_len: int,\n",
    "    batch_size: int,\n",
    "    hidden_size: int,\n",
    "    intermediate_size: int,\n",
    "    dtype: str = \"float16\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate the prefill overhead for single MLP block/layer.\n",
    "\n",
    "    Accounts for following activations:\n",
    "        - Input to the first Linear layer\n",
    "        - Input to the activation function\n",
    "        - Input to the second Linear layer\n",
    "        - Dropout mask\n",
    "    \"\"\"\n",
    "\n",
    "    bytes_per_unit = bytes_per_dtype[dtype]\n",
    "\n",
    "    mlp_input = batch_size * seq_len * hidden_size * bytes_per_unit\n",
    "    act_input = batch_size * seq_len * intermediate_size * bytes_per_unit\n",
    "    down_proj_input = batch_size * seq_len * intermediate_size * bytes_per_unit\n",
    "    dropout_mask = (\n",
    "        batch_size * seq_len * hidden_size * bytes_per_dtype[\"int8\"]\n",
    "    )  # binary mask\n",
    "\n",
    "    mlp_block_bytes = mlp_input + act_input + down_proj_input + dropout_mask\n",
    "\n",
    "    return mlp_block_bytes\n",
    "\n",
    "\n",
    "def calculate_attention_prefill_overhead(\n",
    "    seq_len: int,\n",
    "    batch_size: int,\n",
    "    hidden_size: int,\n",
    "    head_dim: int,\n",
    "    num_attention_heads: int,\n",
    "    num_key_value_heads: int,\n",
    "    dtype: str = \"float16\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate the prefill overhead for single attention block/layer.\n",
    "    \"\"\"\n",
    "    bytes_per_unit = bytes_per_dtype[dtype]\n",
    "\n",
    "    attention_input = batch_size * seq_len * hidden_size * bytes_per_unit\n",
    "    q = batch_size * seq_len * head_dim * num_attention_heads * bytes_per_unit\n",
    "    k = batch_size * seq_len * head_dim * num_key_value_heads * bytes_per_unit\n",
    "    softmax_output = batch_size * num_attention_heads * seq_len**2 * bytes_per_unit\n",
    "    softmax_dropout_mask = (\n",
    "        batch_size * num_attention_heads * seq_len**2 * bytes_per_dtype[\"int8\"]\n",
    "    )\n",
    "    dropout_output = batch_size * num_attention_heads * seq_len**2 * bytes_per_unit\n",
    "    v = batch_size * seq_len * head_dim * num_key_value_heads * bytes_per_unit\n",
    "    out_proj_input = (\n",
    "        batch_size * seq_len * num_attention_heads * head_dim * bytes_per_unit\n",
    "    )\n",
    "    attention_dropout_mask = (\n",
    "        batch_size * seq_len * hidden_size * bytes_per_dtype[\"int8\"]\n",
    "    )\n",
    "\n",
    "    attention_block_bytes = (\n",
    "        attention_input\n",
    "        + q\n",
    "        + k\n",
    "        + softmax_output\n",
    "        + softmax_dropout_mask\n",
    "        + dropout_output\n",
    "        + v\n",
    "        + out_proj_input\n",
    "        + attention_dropout_mask\n",
    "    )\n",
    "\n",
    "    return attention_block_bytes\n",
    "\n",
    "\n",
    "def calculate_layernorm_prefill_overhead(\n",
    "    seq_len: int,\n",
    "    batch_size: int,\n",
    "    hidden_size: int,\n",
    "    dtype: str = \"float16\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate the prefill overhead for single layer norm layer.\n",
    "    \"\"\"\n",
    "    bytes_per_unit = bytes_per_dtype[dtype]\n",
    "    layernorm_bytes = batch_size * seq_len * hidden_size * bytes_per_unit\n",
    "\n",
    "    return layernorm_bytes\n",
    "\n",
    "\n",
    "def get_prefill_overhead(\n",
    "    config: AutoConfig,\n",
    "    max_batch_prefill_tokens: int,\n",
    "    max_input_tokens: int,\n",
    "    dtype: str = \"float16\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate the prefill overhead for a given model and TGI config.\n",
    "\n",
    "    Size of a biggest tensor within forward pass.\n",
    "    It is estimated as the sum of all intermediate tensors within computation of a single layer.\n",
    "    Activations size have quadratic dependence on Sequence Length.\n",
    "\n",
    "    Reference:\n",
    "        - https://arxiv.org/pdf/2205.05198\n",
    "        - https://asmirnov.xyz/vram#fn1\n",
    "    \"\"\"\n",
    "    batch_size = math.ceil(max_batch_prefill_tokens / max_input_tokens)\n",
    "    head_dim = (\n",
    "        config.head_dim\n",
    "        if hasattr(config, \"head_dim\")\n",
    "        else config.hidden_size // config.num_key_value_heads\n",
    "    )\n",
    "\n",
    "    mlp_overhead = calculate_mlp_prefill_overhead(\n",
    "        seq_len=max_input_tokens,\n",
    "        batch_size=batch_size,\n",
    "        hidden_size=config.hidden_size,\n",
    "        intermediate_size=config.intermediate_size,\n",
    "        dtype=dtype,\n",
    "    )\n",
    "\n",
    "    attention_overhead = calculate_attention_prefill_overhead(\n",
    "        seq_len=max_input_tokens,\n",
    "        batch_size=batch_size,\n",
    "        hidden_size=config.hidden_size,\n",
    "        head_dim=head_dim,\n",
    "        num_attention_heads=config.num_attention_heads,\n",
    "        num_key_value_heads=config.num_key_value_heads,\n",
    "        dtype=dtype,\n",
    "    )\n",
    "\n",
    "    layernorm_overhead = calculate_layernorm_prefill_overhead(\n",
    "        seq_len=max_input_tokens,\n",
    "        batch_size=batch_size,\n",
    "        hidden_size=config.hidden_size,\n",
    "        dtype=dtype,\n",
    "    )\n",
    "\n",
    "    return bytes_to_gb(mlp_overhead + attention_overhead + layernorm_overhead * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5788"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prefill_overhead(\n",
    "    config=AutoConfig.from_pretrained(\"mistralai/Mistral-7B-v0.1\"),\n",
    "    max_batch_prefill_tokens=2048,\n",
    "    max_input_tokens=1024,\n",
    "    dtype=\"float16\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KV Cache Overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caclulate_kv_cache_memory_per_token(config: AutoConfig, dtype: str):\n",
    "    \"\"\"Calculates the memory required for the key-value cache per token in a Large Language Model (LLM).\"\"\"\n",
    "    dtype_bytes = bytes_per_dtype[dtype]\n",
    "    head_dim = (\n",
    "        config.head_dim\n",
    "        if hasattr(config, \"head_dim\")\n",
    "        else config.hidden_size // config.num_key_value_heads\n",
    "    )\n",
    "\n",
    "    bytes_per_token = (\n",
    "        2  # k & v\n",
    "        * config.num_hidden_layers\n",
    "        * config.num_key_value_heads\n",
    "        * head_dim\n",
    "        * dtype_bytes\n",
    "    )\n",
    "    return bytes_per_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131072"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caclulate_kv_cache_memory_per_token(\n",
    "    config=AutoConfig.from_pretrained(\"mistralai/Mistral-7B-v0.1\"),\n",
    "    dtype=\"float16\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate VRAM usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_vram_overhead(\n",
    "    model_id: str,\n",
    "    num_gpus: int,\n",
    "    vram_per_gpu: int,  # in GB\n",
    "    max_input_tokens: int,\n",
    "    max_total_tokens: int,\n",
    "    max_batch_prefill_tokens: int,\n",
    "    cuda_memory_fraction: float = 1.0,\n",
    "    dtype: str = \"float16\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Validate the VRAM overhead for a given model and TGI config.\n",
    "    \"\"\"\n",
    "    config = AutoConfig.from_pretrained(model_id)\n",
    "\n",
    "    # determine overhead\n",
    "    cuda_overhead = get_cuda_overhead()\n",
    "    model_overhead = get_model_overhead(model_id, dtype)\n",
    "    prefill_overhead = get_prefill_overhead(\n",
    "        config, max_batch_prefill_tokens, max_input_tokens, dtype\n",
    "    )\n",
    "    kv_cache_overhead_per_token = caclulate_kv_cache_memory_per_token(config, dtype)\n",
    "\n",
    "    # calculate free vram for kv cache\n",
    "    free_vram = vram_per_gpu * num_gpus\n",
    "    free_vram = (\n",
    "        free_vram - (cuda_overhead * num_gpus) - model_overhead - prefill_overhead\n",
    "    )\n",
    "    free_vram = (\n",
    "        free_vram * cuda_memory_fraction * 0.95\n",
    "    )  # by default, TGI allocates 95% of free VRAM to kv-cache\n",
    "\n",
    "    print(f\"Model Overhead: {model_overhead} GB\")\n",
    "    print(f\"CUDA Overhead: {cuda_overhead} GB\")\n",
    "    print(f\"Prefill Overhead: {prefill_overhead} GB\")\n",
    "    print(f\"Free VRAM: {free_vram} GB\")\n",
    "\n",
    "    # calculate token budget for full kv cache (num_batches * max_total_tokens)\n",
    "    kv_cache_token_budget = int((free_vram * 10**9) // kv_cache_overhead_per_token)\n",
    "\n",
    "    # check if we have enough vram for full kv cache at max_total_tokens\n",
    "    batch_size = math.ceil(max_batch_prefill_tokens / max_input_tokens)\n",
    "    kv_cache_tokens_needed = batch_size * max_total_tokens\n",
    "    is_enough_vram = kv_cache_token_budget >= kv_cache_tokens_needed\n",
    "\n",
    "    result = {\n",
    "        \"cuda_overhead\": cuda_overhead,\n",
    "        \"model_overhead\": model_overhead,\n",
    "        \"prefill_overhead\": prefill_overhead,\n",
    "        \"kv_cache_overhead_per_token\": bytes_to_gb(kv_cache_overhead_per_token),\n",
    "        \"kv_cache_token_budget\": kv_cache_token_budget,\n",
    "        \"kv_cache_tokens_needed\": kv_cache_tokens_needed,\n",
    "        \"is_enough_vram\": is_enough_vram,\n",
    "    }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parse safetensors files: 100%|██████████| 4/4 [00:00<00:00, 24.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Overhead: 16.06052 GB\n",
      "Prefill Overhead: 27.65392 GB\n",
      "Free VRAM: -19.678718 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cuda_overhead': 1,\n",
       " 'model_overhead': 16.06052,\n",
       " 'prefill_overhead': 27.65392,\n",
       " 'kv_cache_overhead_per_token': 0.00052,\n",
       " 'kv_cache_token_budget': -37535,\n",
       " 'kv_cache_tokens_needed': 17564,\n",
       " 'is_enough_vram': False}"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_input_tokens = 8750\n",
    "max_total_tokens = 8782\n",
    "max_batch_prefill_tokens = 8782\n",
    "\n",
    "validate_vram_overhead(\n",
    "    model_id=model_id,\n",
    "    num_gpus=1,\n",
    "    vram_per_gpu=24,\n",
    "    max_input_tokens=max_input_tokens,\n",
    "    max_total_tokens=max_total_tokens,\n",
    "    max_batch_prefill_tokens=max_batch_prefill_tokens,\n",
    "    dtype=\"float16\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
