{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc6e5f2b",
   "metadata": {},
   "source": [
    "# Deploying a Hugging Face Model via TorchServe for Bulk Embedding Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6980e028-d777-4096-b153-52acaccd2bb2",
   "metadata": {},
   "source": [
    "In this notebook, we'll deploy a `sentence_transformers` model with [TorchServe](https://pytorch.org/serve/). To accelerate inference we will also use features from the `optimum` [library](https://github.com/huggingface/optimum) to apply graph optimization and/or quantization to the model. Finally, we'll run some benchmarks to understand performance tradeoffs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41229c4d",
   "metadata": {},
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e0237e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install --upgrade transformers, datasets, optimum[onnxruntime-gpu]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827daccf",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d25c6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import base64\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa5e61db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook runtime: GPU\n",
      "PyTorch version : 2.0.1+cu118\n",
      "Transformers version : 4.33.2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Notebook runtime: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(f\"PyTorch version : {torch.__version__}\")\n",
    "print(f\"Transformers version : {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed45919e",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841c6e37",
   "metadata": {},
   "source": [
    "### Overview   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e05d7bb",
   "metadata": {},
   "source": [
    "We'll deploy a container running [PyTorch's TorchServe](https://pytorch.org/serve/) tool in order to serve predictions from a fine-tuned sentence transformer model `bge-base-en` available in [Hugging Face Transformers](https://huggingface.co/BAAI/bge-base-en). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035c60c5",
   "metadata": {},
   "source": [
    "### Save model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3372ad06-ee72-46fd-ae1b-e3195ab505c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_NAME = \"test_bge_embedder\"\n",
    "MODEL_DIR = \"./\"+APP_NAME+\"_predictor\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0825e6ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_id = \"BAAI/bge-base-en\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModel.from_pretrained(model_id)\n",
    "\n",
    "# save base model\n",
    "pt_save_directory = os.path.join(MODEL_DIR, \"model\")\n",
    "tokenizer.save_pretrained(pt_save_directory)\n",
    "model.save_pretrained(pt_save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc88bab-43f9-4a67-b2a1-1d47e1d13e12",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Save ORT model locally\n",
    "\n",
    "The ONNX model can be directly optimized during the ONNX export using [Optimum CLI](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/optimization), by passing the argument `--optimize {O1,O2,O3,O4}` in the CLI. Here, we'll apply O4 graph optimizations and save the ORT model locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da43e509-d094-4bfc-a9cf-6d0eb1dbccf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_save_directory = os.path.join(MODEL_DIR, \"ort_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6794684-5e67-4d5d-a734-5b94cf9feb1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!optimum-cli export onnx \\\n",
    "  --model $model_id \\\n",
    "  --task feature-extraction \\\n",
    "  --optimize O4 \\\n",
    "  --device cuda \\\n",
    "  $ort_save_directory # output folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d1b219",
   "metadata": {},
   "source": [
    "### Create a custom model handler "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2eb813",
   "metadata": {},
   "source": [
    "Please refer to the [TorchServe documentation](https://pytorch.org/serve/custom_service.html) for defining a custom handler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b1be603-2c2c-4832-94f9-17fa99f14348",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_ORT = True # True if using ORT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9de29252",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s $APP_NAME $USE_ORT\n",
    "\n",
    "APP_NAME=$1\n",
    "USE_ORT=$2\n",
    "\n",
    "cat << EOF > ./${APP_NAME}_predictor/custom_handler.py\n",
    "\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, pipeline\n",
    "from optimum.onnxruntime import ORTModelForFeatureExtraction\n",
    "\n",
    "from ts.torch_handler.base_handler import BaseHandler\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class SentenceTransformersHandler(BaseHandler):\n",
    "    \"\"\"\n",
    "    The handler takes an input string and returns the embedding \n",
    "    based on the serialized transformers checkpoint.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(SentenceTransformersHandler, self).__init__()\n",
    "        self.initialized = False\n",
    "        self.use_ort = $USE_ORT\n",
    "\n",
    "    def initialize(self, ctx):\n",
    "        \"\"\" Loads the model.onnx file and initialized the model object.\n",
    "        Instantiates Tokenizer for preprocessor to use and a feature extraction pipeline\n",
    "        \"\"\"\n",
    "        self.manifest = ctx.manifest\n",
    "\n",
    "        properties = ctx.system_properties\n",
    "        model_dir = properties.get(\"model_dir\")\n",
    "        self.device = torch.device(\"cuda:\" + str(properties.get(\"gpu_id\")) if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Read model serialize/pt file\n",
    "        serialized_file = self.manifest[\"model\"][\"serializedFile\"]\n",
    "        model_pt_path = os.path.join(model_dir, serialized_file)\n",
    "        \n",
    "        logger.info(f\"model_pt_path: {model_pt_path}\")\n",
    "        logger.info(f\"model_dir: {model_dir}\")\n",
    "        \n",
    "        if not os.path.isfile(model_pt_path):\n",
    "            raise RuntimeError(\"Missing the model.onnx or pytorch_model.bin file\")\n",
    "        \n",
    "        # Load model\n",
    "        if self.use_ort:\n",
    "            logger.info(f\"----Loading ORT model----\")\n",
    "            self.model = ORTModelForFeatureExtraction.from_pretrained(model_dir, provider=\"CUDAExecutionProvider\")\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_dir, model_max_length=512)\n",
    "            \n",
    "        else:\n",
    "            logger.info(f\"----Loading PyTorch model----\")\n",
    "            self.model = AutoModel.from_pretrained(model_dir)\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_dir, model_max_length=512)\n",
    "        \n",
    "        self.model.to(self.device)\n",
    "        self.initialized = True\n",
    "\n",
    "    def preprocess(self, requests):\n",
    "        \"\"\" Preprocessing input request by tokenizing\n",
    "            Extend with your own preprocessing steps as needed\n",
    "            \n",
    "            [{'data': b'I am creating an endpoint using TorchServe and HF transformers\\n'}]\n",
    "        \"\"\"\n",
    "        input_texts = []\n",
    "        for idx, request in enumerate(requests):\n",
    "            text = request.get(\"data\")\n",
    "            if text is None:\n",
    "                text = request.get(\"body\")\n",
    "                \n",
    "            text = text.decode('utf-8')\n",
    "            input_texts.append(text)\n",
    "        logger.info(\"Received text: '%s'\", input_texts)\n",
    "        \n",
    "        return input_texts\n",
    "\n",
    "    def inference(self, input_texts):\n",
    "        \"\"\" Predict the class of a text using a trained transformer model.\n",
    "        \"\"\"\n",
    "        encoded_input = self.tokenizer(input_texts, padding=True, truncation=True, return_tensors='pt').to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model_output = self.model(**encoded_input)\n",
    "            sentence_embeddings = model_output[0][:, 0] # Perform pooling. In this case, cls pooling.\n",
    "        \n",
    "        sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1).cpu().tolist()\n",
    "        \n",
    "        logger.info(f\"Model embedded: {len(sentence_embeddings)}\")\n",
    "        logger.info(f\"Output type: {type(sentence_embeddings)}\")\n",
    "        return sentence_embeddings\n",
    "\n",
    "    def postprocess(self, inference_output):\n",
    "        return inference_output\n",
    "    \n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f44f516",
   "metadata": {},
   "source": [
    "### Create custom container image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bfd18e-00c0-4631-906d-430681bb0eb1",
   "metadata": {},
   "source": [
    "#### 1. Build TorchServe image compatible with ONNX\n",
    "\n",
    "To make use of ONNX, we need to use an NVIDIA CUDA runtime as the base image for TorchServe as specified [here](https://github.com/pytorch/serve/blob/master/docker/README.md). This requires building a custom image, which the following commands will do for you. The output will be a new Docker image called `pytorch/torchserve:latest-gpu` with the CUDA libraries necessary for running ONNX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d78c56f2-b190-41a6-985c-b65ebea1d54f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'serve'...\n",
      "remote: Enumerating objects: 46147, done.\u001b[K\n",
      "remote: Counting objects: 100% (7773/7773), done.\u001b[K\n",
      "remote: Compressing objects: 100% (1167/1167), done.\u001b[K\n",
      "remote: Total 46147 (delta 6711), reused 7137 (delta 6434), pack-reused 38374\u001b[K\n",
      "Receiving objects: 100% (46147/46147), 72.29 MiB | 46.56 MiB/s, done.\n",
      "Resolving deltas: 100% (28602/28602), done.\n",
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Building 0.0s (0/1)                                                         \n",
      "\u001b[?25h\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.1s (1/2)                                                         \n",
      " => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[34m => => transferring dockerfile: 6.63kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.1s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.3s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.2s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 6.63kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.1s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => resolve image config for docker.io/docker/dockerfile:experimental      0.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.4s (4/4)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.2s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 6.63kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.1s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => resolve image config for docker.io/docker/dockerfile:experimental      0.2s\n",
      "\u001b[0m\u001b[34m => CACHED docker-image://docker.io/docker/dockerfile:experimental@sha256  0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.6s (4/4)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.2s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 6.63kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.1s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => resolve image config for docker.io/docker/dockerfile:experimental      0.2s\n",
      "\u001b[0m\u001b[34m => CACHED docker-image://docker.io/docker/dockerfile:experimental@sha256  0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.8s (4/4)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.2s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 6.63kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.1s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => resolve image config for docker.io/docker/dockerfile:experimental      0.2s\n",
      "\u001b[0m\u001b[34m => CACHED docker-image://docker.io/docker/dockerfile:experimental@sha256  0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.9s (4/4)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.2s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 6.63kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.1s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => resolve image config for docker.io/docker/dockerfile:experimental      0.2s\n",
      "\u001b[0m\u001b[34m => CACHED docker-image://docker.io/docker/dockerfile:experimental@sha256  0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.1s (4/4)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.2s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 6.63kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.1s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => resolve image config for docker.io/docker/dockerfile:experimental      0.2s\n",
      "\u001b[0m\u001b[34m => CACHED docker-image://docker.io/docker/dockerfile:experimental@sha256  0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.2s (4/4)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.2s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 6.63kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.1s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => resolve image config for docker.io/docker/dockerfile:experimental      0.2s\n",
      "\u001b[0m\u001b[34m => CACHED docker-image://docker.io/docker/dockerfile:experimental@sha256  0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.4s (4/4)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.2s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 6.63kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.1s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => resolve image config for docker.io/docker/dockerfile:experimental      0.2s\n",
      "\u001b[0m\u001b[34m => CACHED docker-image://docker.io/docker/dockerfile:experimental@sha256  0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.5s (4/5)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.2s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 6.63kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.1s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => resolve image config for docker.io/docker/dockerfile:experimental      0.2s\n",
      "\u001b[0m\u001b[34m => CACHED docker-image://docker.io/docker/dockerfile:experimental@sha256  0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/nvidia/cuda:11.8.0-cudnn8-runt  0.0s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.6s (5/5)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.2s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 6.63kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.1s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => resolve image config for docker.io/docker/dockerfile:experimental      0.2s\n",
      "\u001b[0m\u001b[34m => CACHED docker-image://docker.io/docker/dockerfile:experimental@sha256  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/nvidia/cuda:11.8.0-cudnn8-runt  0.1s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.7s (5/23)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.2s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 6.63kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.1s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => resolve image config for docker.io/docker/dockerfile:experimental      0.2s\n",
      "\u001b[0m\u001b[34m => CACHED docker-image://docker.io/docker/dockerfile:experimental@sha256  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/nvidia/cuda:11.8.0-cudnn8-runt  0.1s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.8s (24/24) FINISHED                                              \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.2s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 6.63kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.1s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => resolve image config for docker.io/docker/dockerfile:experimental      0.2s\n",
      "\u001b[0m\u001b[34m => CACHED docker-image://docker.io/docker/dockerfile:experimental@sha256  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/nvidia/cuda:11.8.0-cudnn8-runt  0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 541B                                          0.0s\n",
      "\u001b[0m\u001b[34m => [compile-image 1/9] FROM docker.io/nvidia/cuda:11.8.0-cudnn8-runtime-  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [production-image 2/9] RUN --mount=type=cache,target=/var/cach  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [production-image 3/9] RUN useradd -m model-server     && mkdi  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [compile-image 2/9] RUN --mount=type=cache,id=apt-dev,target=/  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [compile-image 3/9] RUN python3.10 -m venv /home/venv           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [compile-image 4/9] RUN python -m pip install -U pip setuptool  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [compile-image 5/9] RUN export USE_CUDA=1                       0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [compile-image 6/9] RUN git clone --depth 1 https://github.com  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [compile-image 7/9] WORKDIR serve                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [compile-image 8/9] RUN     if echo \"nvidia/cuda:11.8.0-cudnn8  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [compile-image 9/9] RUN     if echo \"false\" | grep -q \"false\";  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [production-image 4/9] COPY --chown=model-server --from=compil  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [production-image 5/9] COPY dockerd-entrypoint.sh /usr/local/b  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [production-image 6/9] RUN chmod +x /usr/local/bin/dockerd-ent  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [production-image 7/9] COPY config.properties /home/model-serv  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [production-image 8/9] RUN mkdir /home/model-server/model-stor  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [production-image 9/9] WORKDIR /home/model-server               0.0s\n",
      "\u001b[0m\u001b[34m => exporting to image                                                     0.0s\n",
      "\u001b[0m\u001b[34m => => exporting layers                                                    0.0s\n",
      "\u001b[0m\u001b[34m => => writing image sha256:c2fd7df76dc95eb65bd433291d7b81c832046d170b19c  0.0s\n",
      "\u001b[0m\u001b[34m => => naming to docker.io/pytorch/torchserve:latest-gpu                   0.0s\n",
      "\u001b[0m\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/pytorch/serve.git && \\\n",
    "    cd serve/docker && \\\n",
    "    DOCKER_BUILDKIT=1 docker build --file Dockerfile \\\n",
    "                                --build-arg BASE_IMAGE=\"nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu20.04\" \\\n",
    "                                --build-arg CUDA_VERSION=\"cu118\" \\\n",
    "                                --build-arg PYTHON_VERSION=\"3.10\" \\\n",
    "                                --build-arg BUILD_NIGHTLY=\"false\" \\\n",
    "                                -t \"pytorch/torchserve:latest-gpu\" \\\n",
    "                                --target production-image ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fb4a9a",
   "metadata": {},
   "source": [
    "#### 2. Create a Dockerfile with the new TorchServe image as base image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37c65820-44de-4ca4-a17d-048e38aab696",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"./ort_model/\" if USE_ORT else \"./model/\"\n",
    "MODEL_FILE = \"model.onnx\" if USE_ORT else \"pytorch_model.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca79194d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./test_bge_embedder_predictor/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%bash -s $APP_NAME $MODEL_FILE $MODEL_PATH\n",
    "\n",
    "APP_NAME=$1\n",
    "MODEL_FILE=$2\n",
    "MODEL_PATH=$3\n",
    "\n",
    "cat << EOF > ./${APP_NAME}_predictor/Dockerfile\n",
    "\n",
    "FROM pytorch/torchserve:latest-gpu\n",
    "\n",
    "# install dependencies\n",
    "RUN python3 -m pip install --upgrade pip\n",
    "RUN pip3 install transformers optimum[onnxruntime-gpu]\n",
    "\n",
    "USER model-server\n",
    "\n",
    "# copy model artifacts, custom handler and other dependencies\n",
    "COPY custom_handler.py /home/model-server/\n",
    "COPY $MODEL_PATH /home/model-server/\n",
    "\n",
    "# create torchserve configuration file\n",
    "USER root\n",
    "RUN printf \"\\nservice_envelope=json\" >> /home/model-server/config.properties\n",
    "RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home/model-server/config.properties\n",
    "RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /home/model-server/config.properties\n",
    "RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /home/model-server/config.properties\n",
    "RUN printf \"\\njob_queue_size=1000\" >> /home/model-server/config.properties\n",
    "\n",
    "    \n",
    "# expose health and prediction listener ports from the image\n",
    "EXPOSE 7080\n",
    "EXPOSE 7081\n",
    "\n",
    "# create model archive file packaging model artifacts and dependencies\n",
    "RUN torch-model-archiver -f \\\n",
    "  --model-name=$APP_NAME \\\n",
    "  --version=1.0 \\\n",
    "  --serialized-file=/home/model-server/$MODEL_FILE \\\n",
    "  --handler=/home/model-server/custom_handler.py \\\n",
    "  --extra-files \"/home/model-server/config.json,/home/model-server/tokenizer.json,/home/model-server/tokenizer_config.json,/home/model-server/special_tokens_map.json,/home/model-server/vocab.txt\" \\\n",
    "  --export-path=/home/model-server/model-store\n",
    "\n",
    "# run Torchserve HTTP serve to respond to prediction requests\n",
    "# note that we don't register a model upon startup...\n",
    "# we'll use the Management API for this later on\n",
    "CMD [\"torchserve\", \\\n",
    "     \"--start\", \\\n",
    "     \"--ts-config=/home/model-server/config.properties\", \\\n",
    "     \"--model-store\", \\\n",
    "     \"/home/model-server/model-store\"]\n",
    "\n",
    "EOF\n",
    "\n",
    "echo \"Writing ./${APP_NAME}_predictor/Dockerfile\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc89ea12-0e5e-4308-863c-5aae33058f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_DIR = APP_NAME+\"_predictor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98567898-71ce-4d2b-9f86-d9565973c1b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('test_bge_embedder', 'test_bge_embedder_predictor')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "APP_NAME, APP_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca594d5b",
   "metadata": {},
   "source": [
    "#### 3. Build the TorchServe container with our custom handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2b160e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUSTOM_PREDICTOR_IMAGE_URI = pytorch_predict_test_bge_embedder_true\n"
     ]
    }
   ],
   "source": [
    "CUSTOM_PREDICTOR_IMAGE_URI = f\"pytorch_predict_{APP_NAME}_{str(USE_ORT).lower()}\"\n",
    "print(f\"CUSTOM_PREDICTOR_IMAGE_URI = {CUSTOM_PREDICTOR_IMAGE_URI}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d09ce47",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  668.4MB\n",
      "Step 1/16 : FROM pytorch/torchserve:latest-gpu\n",
      " ---> c2fd7df76dc9\n",
      "Step 2/16 : RUN python3 -m pip install --upgrade pip\n",
      " ---> Using cache\n",
      " ---> a7407e5de4f1\n",
      "Step 3/16 : RUN pip3 install transformers optimum[onnxruntime-gpu]\n",
      " ---> Using cache\n",
      " ---> a3cb1a6a27ff\n",
      "Step 4/16 : USER model-server\n",
      " ---> Using cache\n",
      " ---> 136b1aca1136\n",
      "Step 5/16 : COPY custom_handler.py /home/model-server/\n",
      " ---> 867b117a724c\n",
      "Step 6/16 : COPY ./ort_model /home/model-server/\n",
      " ---> 1faf117c6315\n",
      "Step 7/16 : USER root\n",
      " ---> Running in a5b06607c29c\n",
      "Removing intermediate container a5b06607c29c\n",
      " ---> b50496f7ea0a\n",
      "Step 8/16 : RUN printf \"\\nservice_envelope=json\" >> /home/model-server/config.properties\n",
      " ---> Running in 25f9508a5974\n",
      "Removing intermediate container 25f9508a5974\n",
      " ---> ac23e18e2c9b\n",
      "Step 9/16 : RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home/model-server/config.properties\n",
      " ---> Running in c35c8b7809f4\n",
      "Removing intermediate container c35c8b7809f4\n",
      " ---> e70e6854e459\n",
      "Step 10/16 : RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /home/model-server/config.properties\n",
      " ---> Running in 77b8712cd912\n",
      "Removing intermediate container 77b8712cd912\n",
      " ---> 7f559aa2ddce\n",
      "Step 11/16 : RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /home/model-server/config.properties\n",
      " ---> Running in 56c4b104264b\n",
      "Removing intermediate container 56c4b104264b\n",
      " ---> e7858e529e35\n",
      "Step 12/16 : RUN printf \"\\njob_queue_size=1000\" >> /home/model-server/config.properties\n",
      " ---> Running in a14af017e06c\n",
      "Removing intermediate container a14af017e06c\n",
      " ---> f0c65ec72d6f\n",
      "Step 13/16 : EXPOSE 7080\n",
      " ---> Running in bddedc7b5abf\n",
      "Removing intermediate container bddedc7b5abf\n",
      " ---> 454f47fe9585\n",
      "Step 14/16 : EXPOSE 7081\n",
      " ---> Running in de162783e425\n",
      "Removing intermediate container de162783e425\n",
      " ---> b31cc16553f2\n",
      "Step 15/16 : RUN torch-model-archiver -f   --model-name=test_bge_embedder   --version=1.0   --serialized-file=/home/model-server/model.onnx   --handler=/home/model-server/custom_handler.py   --extra-files \"/home/model-server/config.json,/home/model-server/tokenizer.json,/home/model-server/tokenizer_config.json,/home/model-server/special_tokens_map.json,/home/model-server/vocab.txt\"   --export-path=/home/model-server/model-store\n",
      " ---> Running in 50abf4ee3be8\n",
      "Removing intermediate container 50abf4ee3be8\n",
      " ---> 94a3a8b8a070\n",
      "Step 16/16 : CMD [\"torchserve\",      \"--start\",      \"--ts-config=/home/model-server/config.properties\",      \"--model-store\",      \"/home/model-server/model-store\"]\n",
      " ---> Running in 39d05fe68dfc\n",
      "Removing intermediate container 39d05fe68dfc\n",
      " ---> db62f8590f95\n",
      "Successfully built db62f8590f95\n",
      "Successfully tagged pytorch_predict_test_bge_embedder_true:latest\n"
     ]
    }
   ],
   "source": [
    "!docker build \\\n",
    "  --tag=$CUSTOM_PREDICTOR_IMAGE_URI \\\n",
    "  ./$APP_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97df6271",
   "metadata": {},
   "source": [
    "### Test the TorchServe API locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bafe00e-7063-4d8b-9d67-1c279f76764e",
   "metadata": {},
   "source": [
    "#### 1. Run our Docker container and map the necessary ports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4387c796-1069-4e69-b379-e9c0a29c0092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40ab1b7a9dd71369ea24088b43ce35b700f58bb88eb0d6c67cfe5f492dfcaa61\n"
     ]
    }
   ],
   "source": [
    "!docker run -td --rm -p 7080:7080 -p 7081:7081 -p 8082:8082 --gpus all --name=$APP_NAME $CUSTOM_PREDICTOR_IMAGE_URI\n",
    "!sleep 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43497c8-8ed4-4dde-aa03-001e92e5c9e3",
   "metadata": {},
   "source": [
    "#### 2. Use TorchServe's Management API to register our model(s)\n",
    "\n",
    "Here we can optionally choose to deploy the model a.) with multiple replicas and b.) for [batched inference](https://pytorch.org/serve/batch_inference_with_ts.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1be3531-df07-4374-b3e9-5cfa23112564",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "N_WORKERS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ccfe1a5-2123-4008-bbfc-c3f1eb7e7b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"status\": \"Model \\\"test_bge_embedder\\\" Version: 1.0 registered with 2 initial workers\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Without batched inference\n",
    "# !curl -X POST \"localhost:7081/models?url=test_bge_embedder.mar&initial_workers=$N_WORKERS&model_name=test_bge_embedder\"\n",
    "\n",
    "# With batched inference\n",
    "!curl -X POST \"localhost:7081/models?url=test_bge_embedder.mar&batch_size=$BATCH_SIZE&max_batch_delay=1000&initial_workers=$N_WORKERS&model_name=test_bge_embedder\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4816b243",
   "metadata": {},
   "source": [
    "##### Run a health check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f66cc11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"status\": \"Healthy\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!curl http://localhost:7080/ping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd38c546-a827-4896-b87b-3df0f719adff",
   "metadata": {},
   "source": [
    "##### Confirm our deployment's model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bee4a8ce-abae-4395-a075-da519a6fbef9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"modelName\": \"test_bge_embedder\",\n",
      "    \"modelVersion\": \"1.0\",\n",
      "    \"modelUrl\": \"test_bge_embedder.mar\",\n",
      "    \"runtime\": \"python\",\n",
      "    \"minWorkers\": 2,\n",
      "    \"maxWorkers\": 2,\n",
      "    \"batchSize\": 8,\n",
      "    \"maxBatchDelay\": 1000,\n",
      "    \"loadedAtStartup\": false,\n",
      "    \"workers\": [\n",
      "      {\n",
      "        \"id\": \"9000\",\n",
      "        \"startTime\": \"2023-09-29T19:36:48.967Z\",\n",
      "        \"status\": \"READY\",\n",
      "        \"memoryUsage\": 3799924736,\n",
      "        \"pid\": 63,\n",
      "        \"gpu\": true,\n",
      "        \"gpuUsage\": \"gpuId::1 utilization.gpu [%]::0 % utilization.memory [%]::0 % memory.used [MiB]::1148 MiB\"\n",
      "      },\n",
      "      {\n",
      "        \"id\": \"9001\",\n",
      "        \"startTime\": \"2023-09-29T19:36:48.968Z\",\n",
      "        \"status\": \"READY\",\n",
      "        \"memoryUsage\": 3206426624,\n",
      "        \"pid\": 64,\n",
      "        \"gpu\": true,\n",
      "        \"gpuUsage\": \"gpuId::0 utilization.gpu [%]::0 % utilization.memory [%]::0 % memory.used [MiB]::2010 MiB\"\n",
      "      }\n",
      "    ],\n",
      "    \"jobQueueStatus\": {\n",
      "      \"remainingCapacity\": 1000,\n",
      "      \"pendingRequests\": 0\n",
      "    }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "!curl http://localhost:7081/models/test_bge_embedder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd925c3f-d3a4-44f4-b52e-62a06de2f571",
   "metadata": {},
   "source": [
    "##### Send test request\n",
    "\n",
    "Here we'll use a long input to simulate long documents being embedded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "87129b45-fa88-475e-98bd-c99d73c40efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_INPUT = \"hello \" * 510"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2dcbddc7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"predictions\": [[0.0027409277390688658, -0.0005287091480568051, 0.014051479287445545, 0.009391368366777897, 0.04596078768372536, 0.027604931965470314, 0.049034327268600464, 0.019978001713752747, -0.014058594591915607, -0.049546584486961365, -0.015737656503915787, 0.009661726653575897, -0.05654742196202278, 0.015054648742079735, -0.002073927316814661, 0.03420734778046608, 0.05885257571935654, 0.014727373607456684, 0.010892564430832863, 0.004596078768372536, 0.006040357518941164, 0.03010929748415947, 0.019124241545796394, 0.025541676208376884, 0.02521440200507641, -0.010102835483849049, 0.023122688755393028, 0.05202249065041542, -0.10159753262996674, 0.023023081943392754, 0.007890172302722931, 0.005545887630432844, 0.0013073212467133999, -0.01532500609755516, -0.003941528964787722, -0.058909494429826736, -0.024104513227939606, -0.004322163760662079, 0.0014531719498336315, -0.03417889028787613, -0.02850138023495674, -0.019294993951916695, -0.025086337700486183, 0.020746387541294098, -0.03050771914422512, -0.01294870488345623, -0.032243698835372925, 0.053502343595027924, -0.012279925867915154, 0.010543945245444775, -0.023279210552573204, 0.05168098583817482, 0.03528877720236778, -0.017117904499173164, 0.009163699112832546, 0.019124241545796394, 0.003242512233555317, -0.03901686891913414, -0.032016027718782425, -0.05114027112722397, 0.04664379730820656, 0.009377139620482922, 0.04829440265893936, -0.020404882729053497, 0.04874974116683006, 0.01885388419032097, 0.020063377916812897, 0.013589025475084782, -0.045846953988075256, 0.007961319759488106, -0.036540962755680084, -0.012066485360264778, -0.00436129467561841, -0.0033616831060498953, 0.03432118520140648, -0.05520986393094063, 0.023962220177054405, 0.007007953245192766, 0.02595432847738266, 0.041179727762937546, 0.012237237766385078, -0.004923353902995586, 0.0034541739150881767, 0.042061947286129, -0.03514648601412773, -0.045363157987594604, -0.025043649598956108, 0.005933637265115976, -0.04869282245635986, 0.07000838220119476, 0.0020525832660496235, -0.06317830085754395, 0.03540261462330818, 0.06585341691970825, 0.004845092538744211, -0.018512379378080368, 0.0211732666939497, -0.0008662114851176739, 0.024446016177535057, 0.05885257571935654, -0.014613538980484009, -0.045505449175834656, 0.0008573181112296879, -0.013389814645051956, -0.07814756780862808, -0.024588311091065407, -0.01681908778846264, 0.030934598296880722, 1.8509266737964936e-05, 0.026736941188573837, -0.006887003779411316, -0.045334696769714355, 0.0016639442183077335, 0.0032976509537547827, -0.06027550995349884, 0.09197849780321121, 0.03395121917128563, -0.01716059260070324, -0.02615353837609291, -0.021215954795479774, 0.03776468709111214, 0.03722396865487099, 0.010785844177007675, 0.09010022133588791, 0.02377723716199398, 0.025598594918847084, 0.010003230534493923, 0.07740764319896698, -0.037337806075811386, -0.05976325646042824, -0.0003701853274833411, 0.05481144040822983, -0.00887911207973957, -0.013553452678024769, -0.009227731265127659, -0.025911640375852585, 0.03830540180206299, -0.016008013859391212, 0.04661533981561661, -0.008295709267258644, 0.025427842512726784, 0.010970826260745525, 0.040667470544576645, -0.035886410623788834, 0.032499827444553375, -0.036597877740859985, -0.0003770776675082743, -0.03853306919336319, -0.024588311091065407, 0.05270550027489662, 0.0057024103589355946, 0.008039580658078194, 0.02547053061425686, -0.04650150239467621, 0.012742379680275917, 0.049461208283901215, -0.026680024340748787, 0.013994562439620495, -0.04633074998855591, 0.03471960499882698, 0.010387422516942024, -0.026053933426737785, 0.01865467242896557, 0.0017528776079416275, -0.023037312552332878, 0.06989455223083496, 0.02626737393438816, 0.01204514130949974, -0.021614376455545425, 0.0001864266669144854, -0.027135362848639488, -0.01716059260070324, 0.029141701757907867, 0.0107716154307127, -0.010252243839204311, 0.03466268628835678, 0.0850345715880394, -0.007189377211034298, 0.026964612305164337, 0.0031891523394733667, -0.07535862177610397, 0.013774007558822632, 0.029227077960968018, 0.017345573753118515, 0.008558952249586582, -0.03810618817806244, 0.031930651515722275, -0.008587410673499107, -0.019807249307632446, -0.024929814040660858, -0.058966413140296936, -0.049888089299201965, 0.02564128302037716, 0.011049087159335613, 0.06175536289811134, -0.02887134440243244, -0.027121134102344513, 0.018626214936375618, -0.006652219220995903, 0.06545498967170715, 0.0006852319347672164, 0.020148754119873047, 0.04860744625329971, -0.024787520989775658, -0.06835778057575226, 0.04670071601867676, 0.02360648475587368, 0.005243514198809862, 0.01188861858099699, 0.018128186464309692, -0.018925029784440994, -0.020148754119873047, 0.07484636455774307, -0.016349518671631813, 0.015125795267522335, -0.013055425137281418, 0.02555590681731701, -0.02951166406273842, -0.01223012339323759, -0.015282317996025085, 0.01427914947271347, -0.017089445143938065, -0.03836231678724289, 0.02615353837609291, -1.1290375368844252e-05, 0.07888749986886978, 0.07137440145015717, -0.06892695277929306, 0.0012023798190057278, -0.004514260217547417, 0.03400813788175583, -0.027064217254519463, 0.02326498180627823, 0.04877819865942001, -0.025057878345251083, 0.008743933402001858, 0.02962549962103367, -0.05757193639874458, 0.046074625104665756, -0.06517040729522705, -0.02323652245104313, 0.05452685430645943, 0.002198433969169855, 0.057628851383924484, -0.011618261225521564, -0.03921607881784439, 0.000980046228505671, -0.022966165095567703, -0.05688892677426338, 0.019181158393621445, -0.013823810033500195, -0.04877819865942001, 0.018370086327195168, -0.020504487678408623, 0.010003230534493923, -0.04587541148066521, -0.03144685551524162, 0.014001676812767982, 0.05034342780709267, 0.06090160086750984, -0.019835708662867546, -0.03434964269399643, 0.02194165252149105, -0.006296485662460327, -0.058169569820165634, -0.08161953091621399, -0.009967656806111336, 0.026879236102104187, 0.022268926724791527, -0.015367694199085236, 0.03503265231847763, 0.002815631916746497, -0.006755382288247347, 0.012486251071095467, -0.00013317778939381242, -0.02174244076013565, 0.0876527726650238, -0.011724981479346752, -0.015737656503915787, -0.07256966829299927, 0.00696170749142766, 0.05111181363463402, -0.031475313007831573, -0.05162406712770462, 0.009903624653816223, -0.058454155921936035, -0.001430938602425158, -0.01693292148411274, -0.031987570226192474, -0.0037138594780117273, 0.014414328150451183, 0.05612054094672203, -0.016008013859391212, 0.02635275013744831, 0.026366978883743286, -0.0008546501048840582, -0.003639155300334096, 0.03528877720236778, 0.014101282693445683, -0.018725819885730743, 0.044879358261823654, 0.05062801390886307, -0.010487028397619724, -0.01778668351471424, -0.0077834525145590305, -0.020034920424222946, 0.027149593457579613, -0.06494273990392685, -0.2167413979768753, 0.0022019913885742426, 0.028842885047197342, -0.041265103965997696, 0.05700276046991348, -0.004201214294880629, 0.015879951417446136, -0.03932991251349449, -0.03545952960848808, 0.01959381066262722, -0.045647744089365005, 0.0009569235262461007, -0.022610431537032127, 0.03127610310912132, 0.04758293554186821, -0.01263565942645073, -0.012287040241062641, -0.03548799082636833, 0.03045080043375492, 0.01690446399152279, -0.01506887748837471, -0.05333159118890762, -0.011041972786188126, 0.005254186224192381, -0.026765400543808937, 0.05142485722899437, -0.04587541148066521, -0.01636374741792679, -0.05088414251804352, -0.02982470951974392, 0.009562120772898197, -0.05492527782917023, 0.006406763102859259, -0.013517878949642181, 0.010351849719882011, -0.04724143072962761, 0.02434641122817993, -0.02999546192586422, -0.03383738547563553, -0.012920246459543705, -0.014208002015948296, -0.04826594144105911, -0.03039388358592987, -0.007826140150427818, 0.05714505538344383, -0.02934091165661812, 0.020604094490408897, -0.03130456060171127, -0.011255413293838501, 0.025769345462322235, -0.017943205311894417, 0.008281479589641094, 4.821858510695165e-06, 0.013496534898877144, 0.005631263833492994, 0.032784413546323776, -0.0011152250226587057, 0.01491235475987196, -0.019465746358036995, -0.024830209091305733, 0.009675955399870872, -0.03395121917128563, -0.031162269413471222, -0.03950066491961479, 0.015566905029118061, -0.07274042069911957, -0.023535339161753654, -0.0435987189412117, 0.05512448772788048, 0.03557336702942848, -0.0015358800301328301, -0.020319506525993347, -0.014784290455281734, -0.06004784256219864, -0.013197719119489193, -0.03056463599205017, -0.025598594918847084, -0.06915462017059326, -0.005378692876547575, 0.03141839802265167, -0.07689538598060608, 0.0110846608877182, 0.02352111041545868, 0.032784413546323776, 0.001342005212791264, -0.028145646676421165, 0.021301330998539925, -0.020988285541534424, -0.05273395776748657, -0.022667348384857178, 0.05310392007231712, -0.03486189991235733, -0.0028049598913639784, -0.025598594918847084, 0.06813011318445206, 0.036996301263570786, -0.000697682611644268, -0.004179870709776878, 0.026736941188573837, -0.021244414150714874, 0.06386130303144455, -0.049518126994371414, 0.016605647280812263, 0.012621430680155754, 0.04476552456617355, -0.012173205614089966, -0.03873227909207344, 0.0018142416374757886, 0.023449962958693504, 0.016249913722276688, 0.057372722774744034, -0.032442908734083176, -0.003913070075213909, -0.06295062601566315, 0.054128434509038925, -0.07052063941955566, 0.03958604112267494, 0.09032788872718811, 0.012194549664855003, 0.03315437585115433, -0.015595363453030586, 0.02773299627006054, -0.06824394315481186, -0.011611146852374077, -0.10091452300548553, -0.014549506828188896, -0.003118005581200123, 0.0106008630245924, -0.03531723842024803, -0.02787528932094574, -0.027178050950169563, -0.003637376707047224, 0.01419377326965332, -0.054014597088098526, 0.045932330191135406, 0.027434179559350014, -0.05606362596154213, -0.050400346517562866, -0.008004006929695606, -0.03141839802265167, 0.032357532531023026, -0.020177213475108147, 0.03039388358592987, 0.018298938870429993, 0.015125795267522335, 0.06010475754737854, -0.01572342775762081, -0.04186273738741875, -0.008950258605182171, 0.019095782190561295, -0.021984340623021126, -0.03412197157740593, -0.0007870607078075409, -0.06750401854515076, -0.011390591971576214, -0.027007298544049263, 0.058283403515815735, 0.02114480920135975, -0.012664117850363255, -0.021671295166015625, -0.00482019130140543, -0.06528424471616745, -0.027192281559109688, -0.021301330998539925, 0.019579580053687096, 0.0868559330701828, -0.036654796451330185, -0.03358125686645508, -0.013660172931849957, 0.03161760792136192, 0.02510056644678116, -0.03554490581154823, -0.053786929696798325, 0.022866560146212578, -0.022994624450802803, 0.01286332868039608, -0.0019387484062463045, 0.022425450384616852, 0.041037436574697495, 0.029113242402672768, 0.02750532701611519, -0.02309422940015793, -0.020604094490408897, 0.01738826185464859, 0.07052063941955566, -0.0437694676220417, 0.03161760792136192, -0.04379792883992195, 8.69879931997275e-06, 0.01891080103814602, -0.0012255024630576372, 0.04726988822221756, 0.016107620671391487, -0.010060148313641548, -0.07592779397964478, -0.06778860837221146, -0.009263304062187672, 0.03460577130317688, 0.006744710262864828, 0.015538446605205536, 0.0218562763184309, -0.03705321624875069, -0.022823872044682503, 0.02142939530313015, 0.0425172857940197, -0.09106781333684921, 0.03748009726405144, -0.021073661744594574, -0.032755956053733826, 0.007719420362263918, 0.0033332244493067265, -0.022710036486387253, -0.017317114397883415, -0.009377139620482922, -0.005086991470307112, -0.06078776717185974, -0.04661533981561661, -0.03893149271607399, 0.02959704026579857, -0.01950843445956707, -0.018697360530495644, -0.0035573365166783333, 0.012657003477215767, -0.05307546257972717, -0.026993069797754288, -0.03059309534728527, -0.040582094341516495, 0.023506879806518555, 0.04692838340997696, -0.03346742317080498, 0.05461223050951958, -0.02383415587246418, -0.00047846176312305033, 0.03554490581154823, -0.011241183616220951, 0.01868313178420067, 0.004414654802531004, -0.01755901426076889, 0.022724267095327377, 0.06482890248298645, 0.009199272841215134, 0.002481242176145315, -0.036284834146499634, -0.05495373532176018, -0.004759716335684061, 0.04012675583362579, -0.010501258075237274, 0.014556621201336384, -0.032841332256793976, 0.044850900769233704, -0.026566188782453537, 0.016050701960921288, -0.012201664969325066, -0.020959828048944473, 0.05316083878278732, -0.03543107211589813, -0.04689992591738701, 0.007434833329170942, -0.03272749483585358, -0.01787205971777439, 0.00033083229209296405, 0.00781902577728033, -0.03144685551524162, 0.03369509056210518, 0.05256320536136627, 0.03010929748415947, -0.02589740976691246, 0.016918692737817764, 0.06943920999765396, -0.01501196064054966, 0.014407212845981121, -0.07052063941955566, -0.002886778675019741, 0.0067304810509085655, 0.018782736733555794, -0.004556948319077492, -0.036910925060510635, 0.04689992591738701, 0.01525385957211256, -0.02228315733373165, 0.004315049387514591, 0.040752846747636795, -0.032869789749383926, -0.005474741104990244, 0.05691738426685333, -0.06016167625784874, 0.012913132086396217, -0.029013637453317642, -0.053388506174087524, 0.032784413546323776, 0.007477521430701017, 0.04692838340997696, -0.002559503773227334, 0.05668971687555313, 0.0010760943405330181, 0.02666579559445381, 0.050172675400972366, 0.05341696739196777, -0.036626338958740234, 0.03443501889705658, -0.03304054215550423, 0.07962742447853088, 0.03497573360800743, -0.02391953207552433, -0.036854006350040436, 0.010750271379947662, 0.008701245300471783, -0.05614900216460228, 0.005962096154689789, 0.0214009378105402, 0.024189889430999756, -0.058966413140296936, 0.05079876631498337, 0.024132970720529556, -0.05993400514125824, -0.006869216915220022, -0.001174810458905995, -0.05708813667297363, 0.005069204606115818, 0.006264469586312771, 0.008381085470318794, -0.036996301263570786, 0.05657587945461273, 0.04339950531721115, 0.00369251542724669, 0.028757508844137192, -0.03503265231847763, 0.009185043163597584, 0.04226115718483925, 0.045448534190654755, 0.04872128367424011, 0.040439803153276443, 0.0032620776910334826, 0.0850914940237999, -0.058226484805345535, -0.05626283586025238, 0.022823872044682503, 0.0041265105828642845, -0.04300108551979065, 0.0077407644130289555, -0.002694682450965047, 0.03827694058418274, -0.019522663205862045, 0.05498219281435013, 0.0015607813838869333, -0.020333735272288322, -0.0016621655086055398, 0.023962220177054405, 0.05555136874318123, 0.05133948102593422, 0.00790440198034048, 0.06602416932582855, -0.006139962933957577, -0.049119703471660614, 0.03975679352879524, -0.011376362293958664, -0.01784360036253929, -0.008758163079619408, -0.04362717643380165, 0.011283871717751026, -0.012379531748592854, 0.013297324068844318, 0.08253020793199539, 0.04311491921544075, -0.005485413130372763, -0.001862265751697123, 0.01704675704240799, 0.03036542609333992, 0.010330505669116974, 0.0059229652397334576, 0.012856214307248592, 0.024901356548070908, -0.032528284937143326, 0.011938421986997128, -0.017487866804003716, 0.020575635135173798, 0.00011972661013714969, -0.01724596694111824, -0.008089383132755756, 0.04806673154234886, 0.03517494350671768, -0.036797091364860535, -0.041208188980817795, -0.06972379982471466, -0.019067324697971344, -0.027832601219415665, -0.010209555737674236, 0.02547053061425686, -0.0206894688308239, -0.06067393347620964, -0.006353402975946665, -0.04900586977601051, 0.00443244120106101, -0.00028214126359671354, -0.036284834146499634, -0.05262012407183647, 0.037252429872751236, 0.0442248098552227, 0.0037921208422631025, 0.036939382553100586, 0.11309484392404556, 0.002872549230232835, -0.03042234294116497, -0.01214474719017744, -0.0016176988137885928, 0.09192157536745071, -0.020532947033643723, 0.0003163806104566902, -0.07900132983922958, 0.036114081740379333, 0.03984216973185539, 0.04032596945762634, -0.0834408849477768, 0.011639605276286602, -0.0445663146674633, -0.02298039384186268, 0.040980517864227295, 0.02741995081305504, 0.037195511162281036, -0.025157485157251358, -0.015538446605205536, -0.0420050323009491, 0.005410708952695131, 0.05148177593946457, -0.0431433767080307, 0.041834279894828796, -0.010956596583127975, 0.01716059260070324, -0.0429726243019104, 0.03978525102138519, 0.004816633649170399, 0.020561406388878822, -0.037110134959220886, 0.02569819986820221, -0.019864168018102646, -0.10250820964574814, -0.015481528826057911, 0.03338204696774483, 0.031190726906061172, -0.044793982058763504, 0.017004068940877914, -0.03457731008529663, 0.03480498120188713, 0.0062004378996789455, -0.01498350128531456, -0.00656684348359704, -0.02838754653930664, -0.02968241646885872, -0.06408897787332535, -0.02804604172706604, -0.001414041267707944, -0.05310392007231712, 0.017743995413184166, -0.023706091567873955, 0.017516326159238815, -0.02649504318833351, -0.007136017084121704, 0.004973156377673149, 0.04892049357295036, 0.016093390062451363]]}"
     ]
    }
   ],
   "source": [
    "%%bash -s $APP_NAME\n",
    "\n",
    "APP_NAME=$1\n",
    "\n",
    "cat > ./${APP_NAME}_predictor/instances.json << END\n",
    "{ \n",
    "   \"instances\": [\n",
    "     { \n",
    "       \"data\": {\n",
    "         \"b64\": \"$(echo 'hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello  ' | base64 --wrap=0)\"\n",
    "       }\n",
    "     }\n",
    "   ]\n",
    "}\n",
    "END\n",
    "\n",
    "curl -s -X POST \\\n",
    "  -H \"Content-Type: application/json; charset=utf-8\" \\\n",
    "  -d @./${APP_NAME}_predictor/instances.json \\\n",
    "  http://localhost:7080/predictions/$APP_NAME/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a89149-812f-4ca7-bce9-9ac8501e7408",
   "metadata": {},
   "source": [
    "##### Run a load test to benchmark performance with Apache Bench\n",
    "\n",
    "First, we need to install [Apache Bench](https://httpd.apache.org/docs/2.4/programs/ab.html). Then we can run a test and save out results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "da0b5775-63b4-4b53-9e05-da6fa6002c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get update\n",
    "!apt-get install apache2-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2eb0ad30-0021-4206-9901-41b75d4012b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 200 requests\n",
      "Completed 400 requests\n",
      "Completed 600 requests\n",
      "Completed 800 requests\n",
      "Completed 1000 requests\n",
      "Completed 1200 requests\n",
      "Completed 1400 requests\n",
      "Completed 1600 requests\n",
      "Completed 1800 requests\n",
      "Completed 2000 requests\n",
      "Finished 2000 requests\n"
     ]
    }
   ],
   "source": [
    "REQUESTS = 2000\n",
    "CONCURRENCY = 1000\n",
    "\n",
    "!ab -l -c $CONCURRENCY -n $REQUESTS -k -p ./$APP_DIR/instances.json -T \"application/json\" http://localhost:7080/predictions/$APP_NAME/ > ./$APP_DIR/result.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1129538d-12d3-4b5c-b071-9d271e0c3630",
   "metadata": {},
   "source": [
    "**Benchmarking Results**\n",
    "- 1 GPU, without ORT: 25.75 RPS \n",
    "- 1 GPU, with ORT: 86.47 RPS\n",
    "- 2 GPU, without ORT: 52.31 RPS\n",
    "- 2 GPU, with ORT: 153.33 RPS\n",
    "- 2 GPU, with ORT, batch size 4: 171.52 RPS\n",
    "- 2 GPU, with ORT, batch size 8: 183.93 RPS\n",
    "- 2 GPU, with ORT, batch size 16: 187.43 RPS\n",
    "- 2 GPU, with ORT, batch size 32: 185.46 RPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726bd349",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker stop $CUSTOM_PREDICTOR_IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222da492-d787-42dd-bb80-f59c3dc97bf7",
   "metadata": {},
   "source": [
    "**Resources**:\n",
    "   - On batching:\n",
    "        - https://huggingface.co/docs/transformers/main_classes/pipelines#pipeline-batching\n",
    "   - On TorchServe:\n",
    "        - https://github.com/pytorch/serve/tree/master/examples/Huggingface_Transformers\n",
    "   - On Locust:\n",
    "        - https://medium.com/@tferreiraw/performing-load-tests-with-python-locust-io-62de7d91eebd\n",
    "        - https://medium.com/@ashmi_banerjee/3-step-tutorial-to-performance-test-ml-serving-apis-using-locust-and-fastapi-40e6cc580adc"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.2-0.m110",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.2-0:m110"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
